{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llamabot import StructuredBot, prompt\n",
    "from llamabot.experiments import Experiment, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "@prompt(\"system\")\n",
    "def jdbot_sysprompt(type_of_manager):\n",
    "    \"\"\"You are an {{ type_of_manager }}.\"\"\"\n",
    "\n",
    "\n",
    "@prompt(\"user\")\n",
    "def jdbot_user_message(job_description):\n",
    "    \"\"\"Give me a name for an job that follows this description: {{ job_description }}.\"\"\"\n",
    "\n",
    "\n",
    "class JobDescription(BaseModel):\n",
    "    name: str = Field(..., description=\"A job name.\")\n",
    "    description: str = Field(..., description=\"A job description.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "jdbot_user_message._prompt_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@metric  # <-- this decorator validates that the eval function returns a scalar-type thing\n",
    "def name_length(response):\n",
    "    return len(response.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt(\"system\")\n",
    "def judgebot_sysprompt():\n",
    "    \"\"\"You are a judge of how cool a name is.\"\"\"\n",
    "\n",
    "\n",
    "@prompt(\"user\")\n",
    "def judgebot_userprompt(namebot_response):\n",
    "    \"\"\"Return for me your coolness score: 1-10 for this job name: {{ namebot_response.name }}.\"\"\"\n",
    "\n",
    "\n",
    "class JobNameCoolness(BaseModel):\n",
    "    score: int = Field(\n",
    "        ..., description=\"How cool the job name is. 1 = not cool, 10 = amazeballer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "@metric\n",
    "def llm_judge(namebot_response):\n",
    "    judgebot = StructuredBot(\n",
    "        judgebot_sysprompt(), model_name=\"gpt-4o\", pydantic_model=JobNameCoolness\n",
    "    )\n",
    "    coolness = judgebot(judgebot_userprompt(namebot_response))\n",
    "    return coolness.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution. Each execution of this experiment gets us one new run.\n",
    "with Experiment(\"experiment_name\") as expt:\n",
    "    # Run your program\n",
    "    bot = StructuredBot(\n",
    "        jdbot_sysprompt(\"data science manager\"),\n",
    "        model_name=\"gpt-4o\",\n",
    "        pydantic_model=JobDescription,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    response = bot(jdbot_user_message(\"someone who builds full stack AI apps\"))\n",
    "\n",
    "    # Evals\n",
    "    name_length(response)\n",
    "    llm_judge(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "llamabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
