{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamabot as lmb\n",
    "from typing import Any, List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ToolsToCall(BaseModel):\n",
    "    tool_names: List[str] = Field(..., description=\"The order in which to call tools.\")\n",
    "    tool_arguments: Dict[str, Any | None] = Field(\n",
    "        ...,\n",
    "        description=\"Arguments to pass to each tool. Use None for arguments that are not known ahead of time.\",\n",
    "    )\n",
    "\n",
    "\n",
    "decision_bot = lmb.StructuredBot(\n",
    "    pydantic_model=ToolsToCall,\n",
    "    system_prompt=lmb.system(\n",
    "        \"Given the following message and available tools, \"\n",
    "        \"pick the tool(s) that you need to call on \"\n",
    "        \"and the arguments that you need for those. \"\n",
    "        \"Any arguments that you do not know ahead of time, just leave blank. \"\n",
    "        \"Not all tools need to be called; only call the tools that are relevant to the user's message.\"\n",
    "    ),\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def calculate_total_with_tip(bill_amount: float, tip_rate: float) -> float:\n",
    "    \"\"\"Calculate the total amount to pay at a restaurant including tip.\n",
    "\n",
    "    tip_rate should be a decimal between 0 and 1 (e.g., 0.15, which represents 15% tip rate).\n",
    "\n",
    "    :param bill_amount: The original bill amount before tip.\n",
    "    :param tip_rate: The tip rate as a decimal (e.g., 0.15).\n",
    "    :returns: The total amount including tip.\n",
    "    \"\"\"\n",
    "    if tip_rate < 0 or tip_rate > 1.0:\n",
    "        raise ValueError(\"Tip rate must be between 0 and 1.0\")\n",
    "    return bill_amount * (1 + tip_rate)\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def split_bill(total_amount: float, num_people: int) -> float:\n",
    "    \"\"\"Calculate how much each person needs to pay when splitting a bill evenly.\n",
    "\n",
    "    :param total_amount: The total bill amount including tip.\n",
    "    :param num_people: Number of people splitting the bill.\n",
    "    :returns: The amount each person should pay.\n",
    "    \"\"\"\n",
    "    return total_amount / num_people\n",
    "\n",
    "\n",
    "funcs = [calculate_total_with_tip, split_bill]\n",
    "tools = {func.__name__: func for func in funcs}\n",
    "\n",
    "split_bill_only_prompt = (\n",
    "    \"My dinner was $2300 in total. Split the bill between 4 people.\"\n",
    ")\n",
    "calculate_total_only_prompt = (\n",
    "    \"My dinner was $2300 without tips. Calculate my total with an 18% tip.\"\n",
    ")\n",
    "split_and_calculate_prompt = \"My dinner was $2300 without tips. Calculate my total with an 18% tip and split the bill between 4 people.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.bot.agentbot import AgentBot\n",
    "\n",
    "bot = AgentBot(\n",
    "    system_prompt=lmb.system(\"You are my assistant with respect to restaurant bills.\"),\n",
    "    functions=[calculate_total_with_tip, split_bill],\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "bot(split_bill_only_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(split_and_calculate_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(calculate_total_only_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of using the AgentBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some tools for web scraping and data analysis\n",
    "import numpy as np\n",
    "import httpx\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def scrape_stock_prices(symbol: str) -> List[float]:\n",
    "    \"\"\"Scrape historical stock prices from Yahoo Finance API.\n",
    "\n",
    "    :param symbol: Stock ticker symbol\n",
    "    :returns: List of closing prices\n",
    "    \"\"\"\n",
    "    # Yahoo Finance API endpoint for historical data\n",
    "    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}\"\n",
    "    params = {\n",
    "        \"range\": \"100d\",  # Get 100 days of data\n",
    "        \"interval\": \"1d\",  # Daily intervals\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # Extract closing prices from the response\n",
    "            prices = data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"close\"]\n",
    "            # Filter out None values and convert to float\n",
    "            prices = [float(price) for price in prices if price is not None]\n",
    "            return prices[-100:]  # Return last 100 prices\n",
    "\n",
    "    except httpx.HTTPError as e:\n",
    "        logger.error(f\"HTTP error occurred: {e}\")\n",
    "        raise Exception(f\"Failed to fetch data for {symbol}: {e}\")\n",
    "    except (KeyError, IndexError) as e:\n",
    "        logger.error(f\"Error parsing response: {e}\")\n",
    "        raise Exception(f\"Failed to parse data for {symbol}: {e}\")\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def calculate_percentile(data: List[float], percentile: float) -> float:\n",
    "    \"\"\"Calculate the percentile value from a list of numbers.\n",
    "\n",
    "    :param data: List of numerical values\n",
    "    :param percentile: The percentile to calculate (between 0 and 100)\n",
    "    :returns: The percentile value\n",
    "    \"\"\"\n",
    "    return float(np.percentile(data, percentile))\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def detect_outliers(data: List[float], threshold: float = 1.5) -> List[float]:\n",
    "    \"\"\"Detect outliers in data using IQR method.\n",
    "\n",
    "    :param data: List of numerical values\n",
    "    :param threshold: The IQR multiplier threshold for outlier detection\n",
    "    :returns: List of outlier values\n",
    "    \"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - threshold * iqr\n",
    "    upper_bound = q3 + threshold * iqr\n",
    "    return [x for x in data if x < lower_bound or x > upper_bound]\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def summarize_statistics(data: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate basic statistical measures for a dataset.\n",
    "\n",
    "    :param data: List of numerical values\n",
    "    :returns: Dictionary containing mean, median, std, min, and max\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mean\": float(np.mean(data)),\n",
    "        \"median\": float(np.median(data)),\n",
    "        \"std\": float(np.std(data)),\n",
    "        \"min\": float(np.min(data)),\n",
    "        \"max\": float(np.max(data)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an AgentBot for stock analysis\n",
    "stats_bot = AgentBot(\n",
    "    system_prompt=lmb.system(\n",
    "        \"\"\"You are a stock market analysis assistant. Help analyze stock price data\n",
    "        by providing insights about their distribution, outliers, and basic statistics.\n",
    "        Always explain your findings in plain English.\"\"\"\n",
    "    ),\n",
    "    functions=[\n",
    "        scrape_stock_prices,\n",
    "        calculate_percentile,\n",
    "        detect_outliers,\n",
    "        summarize_statistics,\n",
    "    ],\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "# Ask the bot to analyze stock data\n",
    "response = stats_bot(\n",
    "    \"\"\"Please analyze the last 100 days of MRNA and AAPL stock prices:\n",
    "    1. Scrape the price data\n",
    "    2. Calculate the 90th percentile price\n",
    "    3. Detect any price outliers\n",
    "    4. Provide basic statistical summary\n",
    "\n",
    "    Please analyze this data and explain what you find in terms a retail investor would understand.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "Entrez.email = \"your_email@example.com\"  # Replace with your email\n",
    "\n",
    "\n",
    "@lmb.tool\n",
    "def query_pubmed(query: str, retmax: int = 5) -> List[str]:\n",
    "    \"\"\"Query PubMed for articles matching a given search term.\n",
    "\n",
    "    :param query: The search query\n",
    "    :param retmax: The maximum number of results to return\n",
    "    :returns: List of PubMed IDs\n",
    "    \"\"\"\n",
    "    handle = Entrez.esearch(db=\"pmc\", term=query, retmax=retmax)\n",
    "    result = Entrez.read(handle)\n",
    "    return result[\"IdList\"]\n",
    "\n",
    "\n",
    "result = query_pubmed(\"antibody engineering review article\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "\n",
    "def get_full_text_from_pmcid(pmcid):\n",
    "    Entrez.email = \"your_email@example.com\"  # Replace with your email\n",
    "    handle = Entrez.efetch(db=\"pmc\", id=pmcid, retmode=\"xml\")\n",
    "    record = handle.read()\n",
    "    handle.close()\n",
    "    return record\n",
    "\n",
    "\n",
    "pmcid = result[1]  # Replace with the actual PMC ID\n",
    "full_text_xml = get_full_text_from_pmcid(pmcid)\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_text_from_xml(xml_string: str) -> str:\n",
    "    \"\"\"Extract plain text from PMC XML.\n",
    "\n",
    "    :param xml_string: XML string from PMC\n",
    "    :returns: Plain text content of the article\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(xml_string, \"xml\")\n",
    "\n",
    "    # Find all paragraphs\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "    # Extract text from each paragraph\n",
    "    text_content = []\n",
    "    for p in paragraphs:\n",
    "        text_content.append(p.get_text().strip())\n",
    "\n",
    "    # Join paragraphs with newlines\n",
    "    full_text = \"\\n\\n\".join(text_content)\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "full_text = extract_text_from_xml(full_text_xml)\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SDPMChunker\n",
    "\n",
    "\n",
    "chunker = SDPMChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunker(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks[8].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement graph construction now\n",
    "import networkx as nx\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class TextSpan(BaseModel):\n",
    "    \"\"\"A span of text from a source document.\n",
    "\n",
    "    :param text: The verbatim text content\n",
    "    :param start_char: Character offset where this span starts in the source\n",
    "    :param end_char: Character offset where this span ends in the source\n",
    "    :param source_id: Identifier for the source document\n",
    "    \"\"\"\n",
    "\n",
    "    text: str = Field(..., description=\"The verbatim text content.\")\n",
    "    start_char: int = Field(\n",
    "        ..., description=\"Character offset where this span starts in the source.\"\n",
    "    )\n",
    "    end_char: int = Field(\n",
    "        ..., description=\"Character offset where this span ends in the source.\"\n",
    "    )\n",
    "    source_id: str = Field(..., description=\"Identifier for the source document.\")\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    \"\"\"An entity node in the knowledge graph.\n",
    "\n",
    "    :param id: Unique identifier for this entity\n",
    "    :param name: Display name of the entity\n",
    "    :param type: The type/category of entity (e.g. protein, gene, disease)\n",
    "    :param mentions: Text spans where this entity is mentioned\n",
    "    :param metadata: Optional additional attributes\n",
    "    \"\"\"\n",
    "\n",
    "    id: str = Field(..., description=\"The unique identifier for this entity.\")\n",
    "    name: str = Field(..., description=\"The display name of the entity.\")\n",
    "    type: str = Field(..., description=\"The type/category of entity.\")\n",
    "    mentions: List[TextSpan] = Field(\n",
    "        ..., description=\"The text spans where this entity is mentioned.\"\n",
    "    )\n",
    "    metadata: Optional[dict] = Field(\n",
    "        None, description=\"Optional additional attributes.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Relation(BaseModel):\n",
    "    \"\"\"A relationship between entities in the knowledge graph.\n",
    "\n",
    "    :param source: Entity ID for the source/subject\n",
    "    :param target: Entity ID for the target/object\n",
    "    :param relation_type: Type of relationship\n",
    "    :param evidence: Text spans supporting this relationship\n",
    "    :param confidence: Optional confidence score\n",
    "    :param metadata: Optional additional attributes\n",
    "    \"\"\"\n",
    "\n",
    "    source: str = Field(..., description=\"The source entity ID.\")\n",
    "    target: str = Field(..., description=\"The target entity ID.\")\n",
    "    relation_type: str = Field(..., description=\"The type of relationship.\")\n",
    "    evidence: List[TextSpan] = Field(\n",
    "        ..., description=\"The evidence for the relationship.\"\n",
    "    )\n",
    "    confidence: Optional[float] = Field(\n",
    "        None, description=\"The confidence score for the relationship.\"\n",
    "    )\n",
    "    metadata: Optional[dict] = Field(\n",
    "        None, description=\"Optional additional attributes.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"A knowledge graph constructed from scientific papers.\n",
    "\n",
    "    :param entities: Collection of entities in the graph\n",
    "    :param relations: Collection of relationships between entities\n",
    "    :param metadata: Optional metadata about the graph\n",
    "    \"\"\"\n",
    "\n",
    "    entities: List[Entity] = Field(..., description=\"The entities in the graph.\")\n",
    "    relations: List[Relation] = Field(\n",
    "        ..., description=\"The relationships in the graph.\"\n",
    "    )\n",
    "    metadata: Optional[dict] = Field(\n",
    "        None, description=\"Optional metadata about the graph.\"\n",
    "    )\n",
    "\n",
    "    def to_networkx(self) -> nx.MultiDiGraph:\n",
    "        \"\"\"Convert the knowledge graph to a NetworkX MultiDiGraph.\n",
    "\n",
    "        :returns: A NetworkX MultiDiGraph representation of the knowledge graph.\n",
    "            Nodes are entities with their attributes stored as node properties.\n",
    "            Edges are relations with their attributes stored as edge properties.\n",
    "        \"\"\"\n",
    "        G = nx.MultiDiGraph()\n",
    "\n",
    "        # Add entities as nodes\n",
    "        for entity in self.entities:\n",
    "            G.add_node(\n",
    "                entity.id,\n",
    "                name=entity.name,\n",
    "                type=entity.type,\n",
    "                mentions=entity.mentions,\n",
    "                metadata=entity.metadata,\n",
    "            )\n",
    "\n",
    "        # Add relations as edges\n",
    "        for relation in self.relations:\n",
    "            G.add_edge(\n",
    "                relation.source,\n",
    "                relation.target,\n",
    "                relation_type=relation.relation_type,\n",
    "                evidence=relation.evidence,\n",
    "                confidence=relation.confidence,\n",
    "                metadata=relation.metadata,\n",
    "            )\n",
    "\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a mini-knowledge graph from the individual chunks\n",
    "from tqdm import tqdm\n",
    "\n",
    "kgs = []\n",
    "\n",
    "kgbot = lmb.StructuredBot(\n",
    "    pydantic_model=KnowledgeGraph,\n",
    "    system_prompt=lmb.system(\"You are a biomedical knowledge graph builder.\"),\n",
    "    model_name=\"gpt-4o-mini\",\n",
    ")\n",
    "for chunk in tqdm(chunks):\n",
    "    kg = kgbot(chunk.text)\n",
    "    kgs.append(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined NetworkX MultiDiGraph\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# Add nodes and edges from each knowledge graph\n",
    "for kg in kgs:\n",
    "    # Add entities as nodes\n",
    "    for entity in kg.entities:\n",
    "        G.add_node(\n",
    "            entity.name,  # Use entity name as node identifier\n",
    "            type=entity.type,\n",
    "            mentions=entity.mentions,\n",
    "            metadata=entity.metadata,\n",
    "        )\n",
    "\n",
    "    # Add relations as edges\n",
    "    for relation in kg.relations:\n",
    "        # Find source and target entity names\n",
    "        source_name = next(e.name for e in kg.entities if e.id == relation.source)\n",
    "        target_name = next(e.name for e in kg.entities if e.id == relation.target)\n",
    "\n",
    "        G.add_edge(\n",
    "            source_name,\n",
    "            target_name,\n",
    "            relation_type=relation.relation_type,\n",
    "            evidence=relation.evidence,\n",
    "            confidence=relation.confidence,\n",
    "            metadata=relation.metadata,\n",
    "        )\n",
    "\n",
    "G  # Return the populated graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.networkx as hvnx\n",
    "\n",
    "hvnx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store node relationships\n",
    "node_relations = {}\n",
    "\n",
    "# Iterate through each node in the graph\n",
    "for node in G.nodes():\n",
    "    relations = []\n",
    "\n",
    "    # Get all outgoing edges\n",
    "    for _, target, data in G.out_edges(node, data=True):\n",
    "        relations.append((target, data[\"relation_type\"], data[\"evidence\"]))\n",
    "\n",
    "    # Get all incoming edges\n",
    "    for source, _, data in G.in_edges(node, data=True):\n",
    "        relations.append((source, data[\"relation_type\"], data[\"evidence\"]))\n",
    "\n",
    "    node_relations[node] = relations\n",
    "\n",
    "from llamabot import StructuredBot\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class NodeSummary(BaseModel):\n",
    "    \"\"\"Summary of a node's relationships and context in the knowledge graph.\"\"\"\n",
    "\n",
    "    node_name: str\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# Create a StructuredBot with appropriate system prompt\n",
    "summarizer = StructuredBot(\n",
    "    \"\"\"You are an expert at summarizing relationships between entities in a knowledge graph.\n",
    "    Given information about a node and its relationships, generate a clear and concise summary\n",
    "    that captures the key connections and their nature.\"\"\",\n",
    "    NodeSummary,\n",
    ")\n",
    "\n",
    "# Store summaries for each node\n",
    "node_summaries = {}\n",
    "\n",
    "for node, relations in node_relations.items():\n",
    "    # Format relationships into a readable string\n",
    "    relations_text = \"\\n\".join(\n",
    "        [f\"- {rel[0]} ({rel[1]}): {rel[2]}\" for rel in relations]\n",
    "    )\n",
    "\n",
    "    # Create prompt describing the node and its relationships\n",
    "    prompt = f\"\"\"Node: {node}\n",
    "\n",
    "Relationships:\n",
    "{relations_text}\n",
    "\n",
    "Generate a concise summary of this node's role and relationships in the knowledge graph.\"\"\"\n",
    "\n",
    "    # Get summary from StructuredBot\n",
    "    summary = summarizer(prompt)\n",
    "    if summary:  # Check if we got a valid response\n",
    "        node_summaries[node] = summary.summary\n",
    "\n",
    "# Print summaries\n",
    "for node, summary in node_summaries.items():\n",
    "    print(f\"\\n{node}:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform community detection using networkx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "# Find communities using Louvain method (similar to Leiden)\n",
    "communities = community.louvain_communities(G)\n",
    "\n",
    "# Create mapping of community ID to node names\n",
    "community_mapping = {}\n",
    "for community_id, nodes in enumerate(communities):\n",
    "    community_mapping[community_id] = list(nodes)\n",
    "\n",
    "# Print communities\n",
    "for community_id, nodes in community_mapping.items():\n",
    "    print(f\"\\nCommunity {community_id}:\")\n",
    "    print(\"\\n\".join(f\"- {node}\" for node in nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create community summaries by combining node summaries\n",
    "community_summaries = {}\n",
    "\n",
    "from llamabot import StructuredBot\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class CommunitySummary(BaseModel):\n",
    "    \"\"\"Summary of a community of nodes in the knowledge graph.\"\"\"\n",
    "\n",
    "    summary: str = Field(..., description=\"A concise summary of the community.\")\n",
    "    community_name: str = Field(\n",
    "        ..., description=\"A name for the community that captures the key themes.\"\n",
    "    )\n",
    "\n",
    "\n",
    "community_summarizer = StructuredBot(\n",
    "    \"\"\"You are an expert at analyzing and summarizing groups of related concepts.\n",
    "    Given a set of nodes and their summaries from a knowledge graph, generate a concise\n",
    "    summary that captures the key themes and relationships that unite this community.\"\"\",\n",
    "    CommunitySummary,\n",
    ")\n",
    "\n",
    "for community_id, nodes in community_mapping.items():\n",
    "    # Gather summaries for nodes in this community\n",
    "    community_node_summaries = {\n",
    "        node: node_summaries.get(node, \"No summary available\") for node in nodes\n",
    "    }\n",
    "\n",
    "    # Format the summaries into a prompt\n",
    "    prompt = f\"\"\"Community {community_id} contains the following nodes and their summaries:\n",
    "\n",
    "{chr(10).join(f'Node: {node}{chr(10)}Summary: {summary}{chr(10)}' for node, summary in community_node_summaries.items())}\n",
    "\n",
    "Generate a concise summary that captures the key themes and relationships that unite this community of nodes.\"\"\"\n",
    "\n",
    "    # Get summary from StructuredBot\n",
    "    summary = community_summarizer(prompt)\n",
    "    if summary:  # Check if we got a valid response\n",
    "        community_summaries[community_id] = summary.summary\n",
    "\n",
    "# Print community summaries\n",
    "print(\"\\nCommunity Summaries:\")\n",
    "for community_id, summary in community_summaries.items():\n",
    "    print(f\"\\nCommunity {community_id}:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamabot as lmb\n",
    "\n",
    "query_bot = lmb.QueryBot(\n",
    "    system_prompt=lmb.system(\"You are a biomedical knowledge graph explorer.\"),\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    collection_name=\"biomedical-knowledge-graph\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bot.docstore.extend([c.text for c in chunks])\n",
    "query_bot.docstore.extend(list(node_summaries.values()))\n",
    "query_bot.docstore.extend(list(community_summaries.values()))\n",
    "\n",
    "# query_bot(\"Tell me about the major communities in the knowledge graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bot.docstore.table.create_fts_index(field_names=[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bot(\n",
    "    \"Tell me about NADPH and FAD and what the paper says about their role in fibrosis. Return in tabular format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bot(\"How do we use ImageJ and flow cytometry together to measure NADPH?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bot(\n",
    "    \"What stain should I be using to stain NADPH, and what fluorophore is it usually coupled to?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
