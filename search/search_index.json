{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LlamaBot: A Pythonic bot interface to LLMs","text":"<p>LlamaBot implements a Pythonic interface to LLMs, making it much easier to experiment with LLMs in a Jupyter notebook and build Python apps that utilize LLMs. All models supported by LiteLLM are supported by LlamaBot.</p>"},{"location":"#install-llamabot","title":"Install LlamaBot","text":"<p>To install LlamaBot:</p> <pre><code>pip install llamabot==0.17.11\n</code></pre> <p>This will give you the minimum set of dependencies for running LlamaBot.</p> <p>To install all of the optional dependencies, run:</p> <pre><code>pip install \"llamabot[all]\"\n</code></pre>"},{"location":"#get-access-to-llms","title":"Get access to LLMs","text":""},{"location":"#option-1-using-local-models-with-ollama","title":"Option 1: Using local models with Ollama","text":"<p>LlamaBot supports using local models through Ollama. To do so, head over to the Ollama website and install Ollama. Then follow the instructions below.</p>"},{"location":"#option-2-use-an-api-provider","title":"Option 2: Use an API provider","text":""},{"location":"#openai","title":"OpenAI","text":"<p>If you have an OpenAI API key, then configure LlamaBot to use the API key by running:</p> <pre><code>export OPENAI_API_KEY=\"sk-your1api2key3goes4here\"\n</code></pre>"},{"location":"#mistral","title":"Mistral","text":"<p>If you have a Mistral API key, then configure LlamaBot to use the API key by running:</p> <pre><code>export MISTRAL_API_KEY=\"your-api-key-goes-here\"\n</code></pre>"},{"location":"#other-api-providers","title":"Other API providers","text":"<p>Other API providers will usually specify an environment variable to set. If you have an API key, then set the environment variable accordingly.</p>"},{"location":"#option-3-using-local-models-with-lmstudio","title":"Option 3: Using local models with LMStudio","text":"<p>LlamaBot supports using local models through LMStudio via LiteLLM. To use LMStudio with LlamaBot:</p> <ol> <li>Install and set up LMStudio</li> <li>Load your desired model in LMStudio</li> <li>Start the local server in LMStudio (usually runs on <code>http://localhost:1234</code>)</li> <li>Set the environment variable for LMStudio's API base:</li> </ol> <pre><code>export LM_STUDIO_API_BASE=\"http://localhost:1234\"\n</code></pre> <ol> <li>Use the model with LlamaBot using the <code>lm_studio/</code> prefix:</li> </ol> <pre><code>import llamabot as lmb\n\nsystem_prompt = \"You are a helpful assistant.\"\nbot = lmb.SimpleBot(\n    system_prompt,\n    model_name=\"lm_studio/your-model-name\"  # Use lm_studio/ prefix\n)\n</code></pre> <p>Replace <code>your-model-name</code> with the actual name of the model you've loaded in LMStudio. LlamaBot can use any model provider that LiteLLM supports, and LMStudio is one of the many supported providers.</p>"},{"location":"#how-to-use","title":"How to use","text":"<p>Not sure which bot to use?</p> <p>Check out the Which Bot Should I Use? guide to help you choose the right bot for your needs.</p>"},{"location":"#simplebot","title":"SimpleBot","text":"<p>The simplest use case of LlamaBot is to create a <code>SimpleBot</code> that keeps no record of chat history. This is effectively the same as a stateless function that you program with natural language instructions rather than code. This is useful for prompt experimentation, or for creating simple bots that are preconditioned on an instruction to handle texts and are then called upon repeatedly with different texts.</p>"},{"location":"#using-simplebot-with-an-api-provider","title":"Using <code>SimpleBot</code> with an API provider","text":"<p>For example, to create a Bot that explains a given chunk of text like Richard Feynman would:</p> <pre><code>import llamabot as lmb\n\nsystem_prompt = \"You are Richard Feynman. You will be given a difficult concept, and your task is to explain it back.\"\nfeynman = lmb.SimpleBot(\n  system_prompt,\n  model_name=\"gpt-4.1-mini\"\n)\n</code></pre> <p>For using GPT, you need to have the <code>OPENAI_API_KEY</code> environment variable configured. If you want to use <code>SimpleBot</code> with a local Ollama model, check out this example</p> <p>Now, <code>feynman</code> is callable on any arbitrary chunk of text and will return a rephrasing of that text in Richard Feynman's style (or more accurately, according to the style prescribed by the <code>system_prompt</code>). For example:</p> <pre><code>prompt = \"\"\"\nEnzyme function annotation is a fundamental challenge, and numerous computational tools have been developed.\nHowever, most of these tools cannot accurately predict functional annotations,\nsuch as enzyme commission (EC) number,\nfor less-studied proteins or those with previously uncharacterized functions or multiple activities.\nWe present a machine learning algorithm named CLEAN (contrastive learning\u2013enabled enzyme annotation)\nto assign EC numbers to enzymes with better accuracy, reliability,\nand sensitivity compared with the state-of-the-art tool BLASTp.\nThe contrastive learning framework empowers CLEAN to confidently (i) annotate understudied enzymes,\n(ii) correct mislabeled enzymes, and (iii) identify promiscuous enzymes with two or more EC numbers\u2014functions\nthat we demonstrate by systematic in silico and in vitro experiments.\nWe anticipate that this tool will be widely used for predicting the functions of uncharacterized enzymes,\nthereby advancing many fields, such as genomics, synthetic biology, and biocatalysis.\n\"\"\"\nfeynman(prompt)\n</code></pre> <p>This will return something that looks like:</p> <pre><code>Alright, let's break this down.\n\nEnzymes are like little biological machines that help speed up chemical reactions in our\nbodies. Each enzyme has a specific job, or function, and we use something called an\nEnzyme Commission (EC) number to categorize these functions.\n\nNow, the problem is that we don't always know what function an enzyme has, especially if\nit's a less-studied or new enzyme. This is where computational tools come in. They try\nto predict the function of these enzymes, but they often struggle to do so accurately.\n\nSo, the folks here have developed a new tool called CLEAN, which stands for contrastive\nlearning\u2013enabled enzyme annotation. This tool uses a machine learning algorithm, which\nis a type of artificial intelligence that learns from data to make predictions or\ndecisions.\n\nCLEAN uses a method called contrastive learning. Imagine you have a bunch of pictures of\ncats and dogs, and you want to teach a machine to tell the difference. You'd show it\npairs of pictures, some of the same animal (two cats or two dogs) and some of different\nanimals (a cat and a dog). The machine would learn to tell the difference by contrasting\nthe features of the two pictures. That's the basic idea behind contrastive learning.\n\nCLEAN uses this method to predict the EC numbers of enzymes more accurately than\nprevious tools. It can confidently annotate understudied enzymes, correct mislabeled\nenzymes, and even identify enzymes that have more than one function.\n\nThe creators of CLEAN have tested it with both computer simulations and lab experiments,\nand they believe it will be a valuable tool for predicting the functions of unknown\nenzymes. This could have big implications for fields like genomics, synthetic biology,\nand biocatalysis, which all rely on understanding how enzymes work.\n</code></pre>"},{"location":"#using-simplebot-with-a-local-ollama-model","title":"Using <code>SimpleBot</code> with a Local Ollama Model","text":"<p>If you want to use an Ollama model hosted locally, then you would use the following syntax:</p> <pre><code>import llamabot as lmb\n\nsystem_prompt = \"You are Richard Feynman. You will be given a difficult concept, and your task is to explain it back.\"\nbot = lmb.SimpleBot(\n    system_prompt,\n    model_name=\"ollama_chat/llama2:13b\"\n)\n</code></pre> <p>Simply specify the <code>model_name</code> keyword argument following the <code>&lt;provider&gt;/&lt;model name&gt;</code> format. For example:</p> <ul> <li><code>ollama_chat/</code> as the prefix, and</li> <li>a model name from the Ollama library of models</li> </ul> <p>All you need to do is make sure Ollama is running locally; see the Ollama documentation for more details. (The same can be done for the <code>QueryBot</code> class below!)</p> <p>The <code>model_name</code> argument is optional. If you don't provide it, Llamabot will try to use the default model. You can configure that in the <code>DEFAULT_LANGUAGE_MODEL</code> environment variable.</p>"},{"location":"#simplebot-with-memory-for-chat-functionality","title":"SimpleBot with memory for chat functionality","text":"<p>If you want chat functionality with memory, you can use SimpleBot with ChatMemory. This allows the bot to remember previous conversations:</p> <pre><code>import llamabot as lmb\n\n# Create a bot with memory\nsystem_prompt = \"You are Richard Feynman. You will be given a difficult concept, and your task is to explain it back.\"\n\n# For simple linear memory (fast, no LLM calls)\nmemory = lmb.ChatMemory()\n\n# For intelligent threading (uses LLM for smart connections)\n# memory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n\nfeynman = lmb.SimpleBot(\n    system_prompt,\n    memory=memory,\n    model_name=\"gpt-4.1-mini\"\n)\n\n# Have a conversation\nresponse1 = feynman(\"Can you explain quantum mechanics?\")\nprint(response1)\n\n# The bot remembers the previous conversation\nresponse2 = feynman(\"Can you give me a simpler explanation?\")\nprint(response2)\n</code></pre> <p>The ChatMemory system provides intelligent conversation memory that can maintain context across multiple interactions. It supports both linear memory (fast, no LLM calls) and graph-based memory with intelligent threading (uses LLM to connect related conversation topics).</p> <p>Note: For RAG (Retrieval-Augmented Generation) with document stores, use <code>QueryBot</code> with a document store instead of SimpleBot with memory. SimpleBot's memory parameter is specifically for conversational memory, while QueryBot is designed for document retrieval and question answering.</p> <p>For more details on chat memory, see the Chat Memory component documentation.</p>"},{"location":"#toolbot","title":"ToolBot","text":"<p>ToolBot is a specialized bot designed for single-turn tool execution and function calling. It analyzes user requests and selects the most appropriate tool to execute, making it perfect for automation tasks and data analysis workflows.</p> <pre><code>import llamabot as lmb\nfrom llamabot.components.tools import write_and_execute_code\n\n# Create a ToolBot with code execution capabilities\nbot = lmb.ToolBot(\n    system_prompt=\"You are a data analysis assistant.\",\n    model_name=\"gpt-4.1\",\n    tools=[write_and_execute_code(globals_dict=globals())],\n    memory=lmb.ChatMemory(),\n)\n\n# Create some data\nimport pandas as pd\nimport numpy as np\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\n# Use the bot to analyze the data\nresponse = bot(\"Calculate the correlation between x and y in the data DataFrame\")\nprint(response)\n</code></pre> <p>ToolBot is ideal for: * Data analysis workflows where you need to execute custom code * Automation tasks that require specific function calls * API integrations that need to call external services * Single-turn function calling scenarios</p>"},{"location":"#querybot","title":"QueryBot","text":"<p>QueryBot lets you query a collection of documents. QueryBot now works with a docstore that you create first, making it more modular.</p> <p>Here's how to use QueryBot with a docstore:</p> <pre><code>import llamabot as lmb\nfrom pathlib import Path\n\n# First, create a docstore and add your documents\ndocstore = lmb.LanceDBDocStore(table_name=\"eric_ma_blog\")\ndocstore.add_documents([\n    Path(\"/path/to/blog/post1.txt\"),\n    Path(\"/path/to/blog/post2.txt\"),\n    # ... more documents\n])\n\n# Then, create a QueryBot with the docstore\nbot = lmb.QueryBot(\n  system_prompt=\"You are an expert on Eric Ma's blog.\",\n  docstore=docstore,\n  # Optional:\n  # model_name=\"gpt-4.1-mini\"\n  # or\n  # model_name=\"ollama_chat/mistral\"\n)\n\nresult = bot(\"Do you have any advice for me on career development?\")\n</code></pre> <p>You can also use an existing docstore:</p> <pre><code>import llamabot as lmb\n\n# Load an existing docstore\ndocstore = lmb.LanceDBDocStore(table_name=\"eric_ma_blog\")\n\n# Create QueryBot with the existing docstore\nbot = lmb.QueryBot(\n  system_prompt=\"You are an expert on Eric Ma's blog\",\n  docstore=docstore,\n  # Optional:\n  # model_name=\"gpt-4.1-mini\"\n  # or\n  # model_name=\"ollama_chat/mistral\"\n)\n\nresult = bot(\"Do you have any advice for me on career development?\")\n</code></pre> <p>For more explanation about the <code>model_name</code>, see the examples with <code>SimpleBot</code>.</p>"},{"location":"#structuredbot","title":"StructuredBot","text":"<p>StructuredBot is designed for getting structured, validated outputs from LLMs. Unlike SimpleBot, StructuredBot enforces Pydantic schema validation and provides automatic retry logic when the LLM doesn't produce valid output.</p> <pre><code>import llamabot as lmb\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    hobbies: List[str]\n\n# Create a StructuredBot with your Pydantic model\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information from text.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n\n# The bot will return a validated Person object\nperson = bot(\"John is 25 years old and enjoys hiking and photography.\")\nprint(person.name)  # \"John\"\nprint(person.age)   # 25\nprint(person.hobbies)  # [\"hiking\", \"photography\"]\n</code></pre> <p>StructuredBot is perfect for: * Data extraction from unstructured text * API responses that need to match specific schemas * Form processing with validation * Structured outputs for downstream processing</p>"},{"location":"#imagebot","title":"ImageBot","text":"<p>With the release of the OpenAI API updates, as long as you have an OpenAI API key, you can generate images with LlamaBot:</p> <pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot()\n# Within a Jupyter/Marimo notebook:\nurl = bot(\"A painting of a dog.\")\n\n# Or within a Python script\nfilepath = bot(\"A painting of a dog.\")\n\n# Now, you can do whatever you need with the url or file path.\n</code></pre> <p>If you're in a Jupyter/Marimo notebook, you'll see the image show up magically as part of the output cell as well.</p>"},{"location":"#working-with-images-and-user-input","title":"Working with images and user input","text":"<p>You can easily pass images to your bots using <code>lmb.user()</code> with image file paths. This is particularly useful for vision models that can analyze, describe, or answer questions about images:</p> <pre><code>import llamabot as lmb\n\n# Create a bot that can analyze images\nvision_bot = lmb.SimpleBot(\n    \"You are an expert image analyst. Describe what you see in detail.\",\n    model_name=\"gpt-4o\"  # Use a vision-capable model\n)\n\n# Pass an image file path using lmb.user()\nresponse = vision_bot(lmb.user(\"/path/to/your/image.jpg\"))\nprint(response)\n\n# You can also combine text and images\nresponse = vision_bot(lmb.user(\n    \"What colors are prominent in this image?\",\n    \"/path/to/your/image.jpg\"\n))\nprint(response)\n</code></pre> <p>The <code>lmb.user()</code> function automatically detects image files (PNG, JPG, JPEG, GIF, WebP) and converts them to the appropriate format for the model. You can use local file paths or even image URLs.</p>"},{"location":"#developer-messages-with-lmbdev","title":"Developer Messages with <code>lmb.dev()</code>","text":"<p>For development and debugging scenarios, you can use <code>lmb.dev()</code> to create developer messages that provide context about code changes, debugging instructions, or development tasks:</p> <pre><code>import llamabot as lmb\n\n# Create a bot for code development\ndev_bot = lmb.SimpleBot(\n    \"You are a helpful coding assistant. Help with development tasks.\",\n    model_name=\"gpt-4o-mini\"\n)\n\n# Use dev() for development context\nresponse = dev_bot(lmb.dev(\"Add error handling to this function\"))\nprint(response)\n\n# Combine multiple development instructions\nresponse = dev_bot(lmb.dev(\n    \"Refactor this code to be more modular\",\n    \"Add comprehensive docstrings\",\n    \"Follow PEP8 style guidelines\"\n))\nprint(response)\n</code></pre> <p>When to use <code>lmb.dev()</code>: * Development tasks: Code refactoring, debugging, testing * Code review: Providing feedback on code quality * Documentation: Adding docstrings, comments, or README updates * Debugging: Describing issues or requesting fixes</p> <p>Message Type Hierarchy: * <code>lmb.system()</code> - Bot behavior and instructions * <code>lmb.user()</code> - User input and questions * <code>lmb.dev()</code> - Development context and tasks</p>"},{"location":"#experimentation","title":"Experimentation","text":"<p>Automagically record your prompt experimentation locally on your system by using llamabot's <code>Experiment</code> context manager:</p> <pre><code>import llamabot as lmb\n\n@lmb.prompt(\"system\")\ndef sysprompt():\n    \"\"\"You are a funny llama.\"\"\"\n\n@lmb.prompt(\"user\")\ndef joke_about(topic):\n    \"\"\"Tell me a joke about {{ topic }}.\"\"\"\n\n@lmb.metric\ndef response_length(response) -&gt; int:\n    return len(response.content)\n\nwith lmb.Experiment(name=\"llama_jokes\") as exp:\n    # You would have written this outside of the context manager anyways!\n    bot = lmb.SimpleBot(sysprompt(), model_name=\"gpt-4o\")\n    response = bot(joke_about(\"cars\"))\n    _ = response_length(response)\n</code></pre> <p>And now they will be viewable in the locally-stored message logs:</p> <p></p>"},{"location":"#cli-demos","title":"CLI Demos","text":"<p>Llamabot comes with CLI demos of what can be built with it and a bit of supporting code.</p> <p>And here is one where I use <code>llamabot</code>'s <code>SimpleBot</code> to create a bot that automatically writes commit messages for me.</p> <p></p>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#new-features","title":"New features","text":"<p>New features are welcome! These are early and exciting days for users of large language models. Our development goals are to keep the project as simple as possible. Features requests that come with a pull request will be prioritized; the simpler the implementation of a feature (in terms of maintenance burden), the more likely it will be approved.</p>"},{"location":"#bug-reports","title":"Bug reports","text":"<p>Please submit a bug report using the issue tracker.</p>"},{"location":"#questionsdiscussions","title":"Questions/Discussions","text":"<p>Please use the issue tracker on GitHub.</p>"},{"location":"#contributors","title":"Contributors","text":"<sub>Rena Lu</sub>\ud83d\udcbb <sub>andrew giessel</sub>\ud83e\udd14 \ud83c\udfa8 \ud83d\udcbb <sub>Aidan Brewis</sub>\ud83d\udcbb <sub>Eric Ma</sub>\ud83e\udd14 \ud83c\udfa8 \ud83d\udcbb <sub>Mark Harrison</sub>\ud83e\udd14 <sub>reka</sub>\ud83d\udcd6 \ud83d\udcbb <sub>anujsinha3</sub>\ufffd\ufffd \ud83d\udcd6 <sub>Elliot Salisbury</sub>\ud83d\udcd6 <sub>Ethan Fricker, PhD</sub>\ud83d\udcd6 <sub>Ikko Eltociear Ashimine</sub>\ud83d\udcd6 <sub>Amir Molavi</sub>\ud83d\ude87 \ud83d\udcd6"},{"location":"cli/blog/","title":"Blog Assistant CLI Tutorial","text":"<p>The Blog Assistant CLI is a powerful tool that helps you streamline your blogging workflow. It can generate blog summaries, apply semantic line breaks (SEMBR), and even create social media posts for LinkedIn, Patreon, and Twitter. This tutorial will guide you through the usage of this tool.</p>"},{"location":"cli/blog/#summarize-command","title":"Summarize Command","text":"<p>The <code>summarize</code> command is used to generate a blog summary, title, and tags. Here's how to use it:</p> <ol> <li>Run the command <code>summarize</code> in your terminal: <code>llamabot blog summarize</code></li> <li>You will be prompted to paste your blog post.</li> <li>The tool will then generate a blog title, apply SEMBR to your summary, and provide you with relevant tags.</li> </ol> <p>The output will look something like this:</p> <pre><code>Here is your blog title:\n[Generated Blog Title]\n\nApplying SEMBR to your summary...\n\nHere is your blog summary:\n[Generated Blog Summary with SEMBR]\n\nHere are your blog tags:\n[Generated Blog Tags]\n</code></pre>"},{"location":"cli/blog/#social-media-command","title":"Social Media Command","text":"<p>The <code>social_media</code> command is used to generate social media posts. Here's how to use it:</p> <ol> <li>Run the command <code>social_media [platform]</code> in your terminal, where <code>[platform]</code> is either <code>linkedin</code>, <code>patreon</code>, or <code>twitter</code>: <code>llamabot blog social-media linkedin</code>.</li> <li>You will be prompted to paste your blog post.</li> <li>The tool will then generate a social media post for the specified platform.</li> </ol> <p>For LinkedIn and Twitter, the generated post will be copied to your clipboard. For Patreon, the tool will display the post in the terminal.</p>"},{"location":"cli/blog/#sembr-command","title":"SEMBR Command","text":"<p>The <code>sembr</code> command is used to apply semantic line breaks to a blog post. Here's how to use it:</p> <ol> <li>Run the command <code>sembr</code> in your terminal: <code>llamabot blog sembr</code></li> <li>You will be prompted to paste your blog post.</li> <li>The tool will then apply semantic line breaks to your post and copy the result to your clipboard.</li> </ol> <p>With these commands, you can streamline your blogging workflow and ensure your content is optimized for readability and engagement. Happy blogging!</p>"},{"location":"cli/docs/","title":"CLI Documentation for <code>llamabot docs write</code>","text":""},{"location":"cli/docs/#overview","title":"Overview","text":"<p>The <code>llamabot docs write</code> command is a powerful tool designed to help you create and maintain Markdown documentation for your project. This command leverages the capabilities of LLMs (Large Language Models) to generate and update documentation based on the content and intents specified in your Markdown source files.</p>"},{"location":"cli/docs/#usage","title":"Usage","text":"<p>To use the <code>llamabot docs write</code> command, open your terminal and navigate to the root directory of your project. Then, run the following command:</p> <pre><code>llamabot docs write &lt;path_to_markdown_file&gt;\n</code></pre> <p>Replace <code>&lt;path_to_markdown_file&gt;</code> with the path to the Markdown file you want to generate or update documentation for.</p>"},{"location":"cli/docs/#-from-scratch-flag","title":"<code>--from-scratch</code> Flag","text":"<p>The <code>--from-scratch</code> flag is an optional parameter that you can use with the <code>llamabot docs write</code> command. When this flag is set to <code>True</code>, the command will start with a blank documentation, ignoring any existing content in the Markdown file. This is useful when you want to completely regenerate the documentation from scratch.</p> <p>To use the <code>--from-scratch</code> flag, run the following command:</p> <pre><code>llamabot docs write &lt;path_to_markdown_file&gt; --from-scratch\n</code></pre>"},{"location":"cli/docs/#frontmatter-key-value-pairs","title":"Frontmatter Key-Value Pairs","text":"<p>For the <code>llamabot docs write</code> command to work correctly, your Markdown source file must contain specific frontmatter key-value pairs. The frontmatter should be written in YAML format and placed at the top of the Markdown file. Here is an example of the required frontmatter:</p> <pre><code>---\nintents:\n- Point 1 that the documentation should cover.\n- Point 2 that the documentation should cover.\n- ...\nlinked_files:\n- path/to/relevant_file1.py\n- path/to/relevant_file2.toml\n- ...\n---\n</code></pre>"},{"location":"cli/docs/#intents","title":"Intents","text":"<p>The <code>intents</code> key is a list of points that the documentation should cover. These points guide the LLM in generating the content of the documentation.</p>"},{"location":"cli/docs/#linked-files","title":"Linked Files","text":"<p>The <code>linked_files</code> key is a list of paths to relevant source files that the documentation should reference. These paths must be relative to the root of the repository.</p>"},{"location":"cli/docs/#example","title":"Example","text":"<p>Here is an example of a complete Markdown source file with the required frontmatter and some initial content:</p> <pre><code>---\nintents:\n- Provide an overview of the `llamabot docs write` command.\n- Explain the `--from-scratch` flag.\n- Describe the frontmatter key-value pairs needed to make it work.\nlinked_files:\n- llamabot/cli/docs.py\n- pyproject.toml\n---\n\n# CLI Documentation for `llamabot docs write`\n\n&lt;The documentation content will be generated here.&gt;\n</code></pre> <p>By following these guidelines, you can effectively use the <code>llamabot docs write</code> command to generate and maintain high-quality documentation for your project.</p>"},{"location":"cli/docs/#how-linked-files-are-referenced","title":"How Linked Files are Referenced","text":"<p>The <code>llamabot docs write</code> command references linked files specified in the <code>linked_files</code> key of the frontmatter. These files are read and their content is used to inform the generated documentation. The paths to these files must be relative to the root of the repository. For example, if you have a file <code>llamabot/cli/docs.py</code> that you want to reference, you would include it in the <code>linked_files</code> list as shown in the example above.</p> <p>By understanding and utilizing these features, you can ensure that your documentation is comprehensive, up-to-date, and aligned with the source code and project intents.</p>"},{"location":"cli/git/","title":"LlamaBot Git CLI Documentation","text":"<p>Welcome to the LlamaBot Git CLI documentation. This guide provides a comprehensive tutorial on how to use the various commands available in the LlamaBot Git CLI, designed to enhance your Git experience with automated commit messages, release notes, and activity reports.</p>"},{"location":"cli/git/#getting-started","title":"Getting Started","text":"<p>Before you begin, ensure that you have the LlamaBot CLI installed on your system. You will also need to have Git installed and be within a Git repository to use most of the commands.</p>"},{"location":"cli/git/#commands-overview","title":"Commands Overview","text":"<p>The LlamaBot Git CLI includes several commands, each tailored for specific Git-related tasks:</p>"},{"location":"cli/git/#1-hooks","title":"1. <code>hooks</code>","text":"<p>Purpose: Installs a commit message hook that automatically generates commit messages using a structured bot.</p> <p>Usage:</p> <pre><code>llamabot git hooks\n</code></pre> <p>This command sets up a Git hook in your repository that triggers the LlamaBot to compose commit messages if none are provided during commits.</p>"},{"location":"cli/git/#2-compose","title":"2. <code>compose</code>","text":"<p>Purpose: Automatically generates a commit message based on the current Git diff.</p> <p>Usage:</p> <pre><code>llamabot git compose\n</code></pre> <p>Use this command to autogenerate a commit message which you can then review and edit as needed. This is particularly useful for ensuring commit messages are consistent and informative.</p>"},{"location":"cli/git/#3-write_release_notes","title":"3. <code>write_release_notes</code>","text":"<p>Purpose: Generates release notes for the latest tags in your repository.</p> <p>Usage:</p> <pre><code>llamabot git write_release_notes\n</code></pre> <p>This command will create a markdown file in the specified directory containing release notes based on the commits between the last two tags.</p>"},{"location":"cli/git/#4-report","title":"4. <code>report</code>","text":"<p>Purpose: Generates a report based on Git commit logs for a specified time frame.</p> <p>Usage:</p> <pre><code>llamabot git report --hours 24\nllamabot git report --start-date 2023-01-01 --end-date 2023-01-02\n</code></pre> <p>This command can be used to generate a detailed report of activities, highlighting key changes and features implemented within the specified period.</p>"},{"location":"cli/git/#conclusion","title":"Conclusion","text":"<p>The LlamaBot Git CLI is a powerful tool for automating and enhancing your Git workflow. By understanding and utilizing these commands, you can significantly improve the efficiency and consistency of your version control practices.</p>"},{"location":"cli/llamabot/","title":"LlamaBot Configuration Tutorial","text":"<p>In this tutorial, we will walk through the configuration process for LlamaBot, a Python-based bot that uses the OpenAI API. The configuration process involves setting up the API key and selecting the default model for the bot.</p>"},{"location":"cli/llamabot/#setting-up-the-api-key","title":"Setting up the API Key","text":"<p>The first step in configuring LlamaBot is to set up the API key. This is done by invoking:</p> <pre><code>llamabot configure api-key\n</code></pre> <p>The user will be prompted to enter their OpenAI API key. The key will be hidden as you type it, and you will be asked to confirm it. Once confirmed, the key will be stored as an environment variable, <code>OPENAI_API_KEY</code>.</p>"},{"location":"cli/llamabot/#configuring-the-default-model","title":"Configuring the Default Model","text":"<p>The next step in the configuration process is to select the default model for LlamaBot. This is done by invoking:</p> <pre><code>llamabot configure default-model\n</code></pre> <p>LlamaBot will first load the environment variables from the <code>.env</code> file located at <code>llamabotrc_path</code>. It then retrieves a list of available models from the OpenAI API, filtering for those that include 'gpt' in their ID. For this reason, it is important to set your OpenAI API key before configuring the default model.</p> <p>The function then displays the list of available models and prompts you to select one. As you type, the function will suggest completions based on the available models. The last model in the list is provided as the default option.</p> <p>Once you have entered a valid model ID, the function stores it as an environment variable, <code>DEFAULT_LANGUAGE_MODEL</code>.</p>"},{"location":"cli/llamabot/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can easily configure LlamaBot to use your OpenAI API key and your chosen default model. Remember to keep your API key secure, and to choose a model that best suits your needs. Happy coding!</p>"},{"location":"cli/log-viewer/","title":"LlamaBot Log Viewer Tutorial","text":"<p>Welcome to the LlamaBot Log Viewer tutorial. This guide will walk you through launching and using the LlamaBot log viewer, with an emphasis on its local nature and providing insights into the data storage and interaction capabilities.</p>"},{"location":"cli/log-viewer/#overview","title":"Overview","text":"<p>The LlamaBot log viewer is a local tool designed to help you visualize and interact with your prompt and message logs. It ensures that no data is sent to the cloud, maintaining your privacy and data security. The data is stored locally in two potential locations:</p> <ol> <li>The root directory of your git repository as <code>message_log.db</code>.</li> <li>The home directory under <code>.llamabot/message_log.db</code> if not working in a git repository.</li> </ol> <p>Note that <code>message_log.db</code> stored within a repo are automatically added to <code>.gitignore</code> so that it does not get committed to source control. This is to ensure that your logs remain private and secure.</p>"},{"location":"cli/log-viewer/#launching-the-log-viewer","title":"Launching the Log Viewer","text":"<p>To launch the LlamaBot log viewer, ensure you have the necessary environment set up and run the following command:</p> <pre><code>llamabot log-viewer launch\n</code></pre> <p>This will start a local FastAPI server, allowing you to access the log viewer through your web browser. The default host and port is <code>0.0.0.0:8000</code>.</p>"},{"location":"cli/log-viewer/#data-tables","title":"Data Tables","text":"<p>The log viewer interacts with three main tables:</p> <ul> <li>Experiments</li> <li>Message Log</li> <li>Prompts</li> </ul> <p>These tables are defined in the <code>ExperimentRun</code>, <code>MessageLog</code>, and <code>Runs</code> classes, respectively.</p>"},{"location":"cli/log-viewer/#viewing-and-interacting-with-logs","title":"Viewing and Interacting with Logs","text":""},{"location":"cli/log-viewer/#message-logs","title":"Message Logs","text":"<p>The <code>message_log</code> table stores detailed records of message exchanges. You can view and interact with these logs through the log viewer interface.</p> <p></p> <ul> <li>Viewing Logs: Access the logs by navigating to the logs section. You can filter logs by function name or text content.</li> <li>Interacting with Logs: Expand or collapse log details, and rate logs as helpful or not.</li> </ul>"},{"location":"cli/log-viewer/#prompt-versions","title":"Prompt Versions","text":"<p>The <code>prompts</code> table maintains a version-controlled record of prompt templates. This allows you to track changes and view different versions of prompts.</p> <p></p> <ul> <li>Viewing Prompt Versions: Navigate to the prompt versions section to see a list of all stored prompts and their versions.</li> </ul>"},{"location":"cli/log-viewer/#experiments","title":"Experiments","text":"<p>The <code>experiments</code> table records experiment runs, including metadata and metrics. This helps in tracking and comparing different experiment outcomes, particularly useful when you are tweaking your prompts.</p> <p></p> <ul> <li>Viewing Experiments: Access the experiments section to view details of each experiment, including metrics and associated message logs.</li> </ul>"},{"location":"cli/log-viewer/#conclusion","title":"Conclusion","text":"<p>The LlamaBot log viewer is a powerful tool for managing and analyzing your prompt and message logs locally. By following this tutorial, you should be able to effectively launch and use the log viewer to gain insights into your data.</p>"},{"location":"cli/mcp/","title":"MCP Server","text":"<p>The LlamaBot MCP (Model Context Protocol) server provides AI coding agents with access to LlamaBot's comprehensive documentation and source code through semantic search.</p>"},{"location":"cli/mcp/#features","title":"Features","text":"<ul> <li>Semantic Search: Find relevant documentation using natural language   queries</li> <li>Comprehensive Coverage: Includes all documentation, tutorials, and   source code docstrings</li> <li>Fast Performance: Pre-built database for instant startup</li> <li>MCP Compatible: Works with Cursor, VSCode, and other MCP-enabled tools</li> </ul>"},{"location":"cli/mcp/#installation","title":"Installation","text":"<p>The MCP server comes with a pre-built documentation database, so no additional setup is required.</p>"},{"location":"cli/mcp/#configure-your-coding-tool","title":"Configure Your Coding Tool","text":""},{"location":"cli/mcp/#cursor","title":"Cursor","text":"<p>Add to your MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"llamabot-docs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--with\", \"llamabot[all]\", \"llamabot\", \"mcp\", \"launch\"]\n    }\n  }\n}\n</code></pre>"},{"location":"cli/mcp/#vscode","title":"VSCode","text":"<p>Add to your MCP configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"llamabot-docs\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--with\", \"llamabot[all]\", \"llamabot\", \"mcp\", \"launch\"]\n    }\n  }\n}\n</code></pre>"},{"location":"cli/mcp/#other-tools","title":"Other Tools","text":"<p>Use <code>uvx --with llamabot[all] llamabot mcp launch</code> as your MCP server command.</p>"},{"location":"cli/mcp/#usage","title":"Usage","text":"<p>Once configured, you can use the <code>docs_search</code> tool in your AI coding assistant:</p> <ul> <li>Query: Natural language search (e.g., \"How do I create a SimpleBot?\")</li> <li>Results: Returns relevant documentation snippets with metadata</li> <li>Limit: Control number of results (default: 5)</li> </ul>"},{"location":"cli/mcp/#commands","title":"Commands","text":""},{"location":"cli/mcp/#llamabot-mcp-build","title":"<code>llamabot mcp build</code>","text":"<p>Build a custom documentation database from GitHub and local source code.</p> <p>When to use:</p> <ul> <li>You need the latest documentation (not the packaged version)</li> <li>You're developing locally and want fresh docs</li> <li>You want to customize the documentation sources</li> </ul> <p>Options:</p> <ul> <li>Fetches latest docs from GitHub</li> <li>Extracts Python docstrings</li> <li>Creates semantic search index</li> <li>Stores in <code>~/.llamabot/mcp_docs/</code></li> </ul>"},{"location":"cli/mcp/#llamabot-mcp-launch","title":"<code>llamabot mcp launch</code>","text":"<p>Launch the MCP server for AI coding tools.</p> <p>Options:</p> <ul> <li><code>--host</code>: Host to bind to (default: 0.0.0.0)</li> <li><code>--port</code>: Port to run on (default: 8765)</li> </ul>"},{"location":"cli/mcp/#database-management","title":"Database Management","text":"<p>The MCP server uses a two-tier approach:</p> <ol> <li>User Database (<code>~/.llamabot/mcp_docs/</code>): Built with <code>llamabot mcp build</code></li> <li>Packaged Database: Pre-built and included with the wheel</li> </ol> <p>The server automatically uses the user database if it exists, otherwise falls back to the packaged database. Most users will use the packaged database.</p>"},{"location":"cli/mcp/#cicd-and-packaging","title":"CI/CD and Packaging","text":"<p>The MCP documentation database follows a two-phase build process during CI/CD to ensure the database is included in the final package.</p>"},{"location":"cli/mcp/#build-process","title":"Build Process","text":"<ol> <li>Database Build: The MCP database is built using <code>scripts/build_mcp_docs.py</code></li> <li>Database Copy: The built database is copied to <code>llamabot/data/mcp_docs/</code></li> <li>Package Build: The Python package is built once with the database included</li> <li>Artifacts Inclusion: Hatchling includes the database files via <code>artifacts</code> configuration</li> </ol>"},{"location":"cli/mcp/#configuration","title":"Configuration","text":"<p>The database directory is in <code>.gitignore</code> but still gets packaged through the <code>artifacts</code> configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.hatch.build.targets.wheel]\nartifacts = [\n    \"llamabot/data/mcp_docs/**/*\",\n]\n</code></pre> <p>This pattern allows build artifacts to be included in the package even when they're not tracked in git.</p>"},{"location":"cli/mcp/#cicd-workflow","title":"CI/CD Workflow","text":"<p>The process is implemented in <code>.github/workflows/release-python-package.yaml</code>:</p> <ol> <li>Build MCP database with <code>pixi run python scripts/build_mcp_docs.py</code></li> <li>Verify database files exist in <code>llamabot/data/mcp_docs/</code></li> <li>Build package with <code>uv build --sdist --wheel</code></li> <li>Publish to PyPI</li> </ol> <p>This ensures every release includes a fresh, up-to-date documentation database.</p>"},{"location":"cli/mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/mcp/#database-not-found","title":"Database Not Found","text":"<p>If you see \"LlamaBot documentation database not found\":</p> <pre><code>llamabot mcp build\n</code></pre>"},{"location":"cli/mcp/#slow-startup","title":"Slow Startup","text":"<p>The first launch may be slow due to model loading. Subsequent launches are much faster.</p>"},{"location":"cli/mcp/#outdated-documentation","title":"Outdated Documentation","text":"<p>To get the latest documentation:</p> <pre><code>llamabot mcp build\n</code></pre> <p>This rebuilds the database with the most recent content from GitHub.</p>"},{"location":"cli/mcp/#examples","title":"Examples","text":""},{"location":"cli/mcp/#search-for-bot-creation","title":"Search for Bot Creation","text":"<p>Query: \"How do I create a SimpleBot?\"</p> <p>Returns: Documentation about SimpleBot initialization, examples, and usage patterns.</p>"},{"location":"cli/mcp/#find-api-documentation","title":"Find API Documentation","text":"<p>Query: \"What are the parameters for StructuredBot?\"</p> <p>Returns: API documentation, parameter descriptions, and usage examples.</p>"},{"location":"cli/mcp/#get-tutorial-information","title":"Get Tutorial Information","text":"<p>Query: \"Show me the tutorial for QueryBot\"</p> <p>Returns: Tutorial content, step-by-step instructions, and code examples.</p>"},{"location":"cli/notebook/","title":"LlamaBot Notebook CLI Tutorial","text":"<p>Welcome to the LlamaBot Notebook CLI tutorial. This guide will walk you through how to use the LlamaBot CLI to explain Jupyter notebook code cells in a simple and understandable way for non-technical audiences.</p>"},{"location":"cli/notebook/#benefits-of-using-llamabot-notebook-cli","title":"Benefits of Using LlamaBot Notebook CLI","text":"<p>The key selling point is that it lets you, the data scientist, work in flow mode without worrying too hard about documenting your code. Once you're ready and satisfied with your notebook, you can then use the <code>llamabot notebook explain</code> command to generate explanations for your notebook.</p> <p>Using the LlamaBot Notebook CLI provides several benefits:</p> <ul> <li>Simplification of Complex Code: Converts complex Jupyter notebook code cells into easy-to-understand markdown explanations.</li> <li>Educational Tool: Enhances the learning experience for students and non-technical stakeholders by providing clear explanations of what the code does.</li> <li>Documentation Efficiency: Saves time and effort in documentation by automatically generating explanations, which can be particularly useful for data scientists and educators.</li> <li>Customizable Explanations: Offers flexibility with the <code>overwrite</code> option to either replace the original notebook or create a new explained version, catering to different documentation needs.</li> <li>Code-First Approach: Emphasizes the importance of code quality and documentation from the start, ensuring that the code is both functional and well-documented.</li> </ul>"},{"location":"cli/notebook/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have <code>llamabot</code> installed:</p> <pre><code>pip install llamabot\n</code></pre>"},{"location":"cli/notebook/#usage","title":"Usage","text":"<p>To use the LlamaBot notebook CLI, follow these steps:</p> <ol> <li>Prepare Your Notebook: Ensure your Jupyter notebook is ready -- just code cells, no need for markdown cells that explain the code.</li> <li>Run the CLI: Use the following command to run the CLI:</li> </ol> <pre><code>llamabot notebook explain /path/to/notebook.ipynb [--overwrite]\n</code></pre>"},{"location":"cli/notebook/#parameters","title":"Parameters","text":"<ul> <li><code>notebook_path</code>: The path to the Jupyter notebook file you want to explain.</li> <li><code>overwrite</code>: Optional. If set to True, the original notebook will be overwritten with the explanations. If False, a new file with the '_explained' suffix will be created.</li> </ul>"},{"location":"cli/notebook/#how-it-works","title":"How It Works","text":"<p>When you run the command, the CLI does the following:</p> <ul> <li>Reads the Jupyter notebook from the specified path.</li> <li>Iterates through each code cell in the notebook.</li> <li>Uses the built-in <code>SimpleBot</code> to generate a markdown explanation for each code cell.</li> <li>Appends a new markdown cell with the explanation before the original code cell in the notebook.</li> </ul> <p>If the <code>overwrite</code> parameter is False, the explanations are saved in a new notebook file with the '_explained' suffix, preserving the original notebook.</p>"},{"location":"cli/notebook/#example","title":"Example","text":"<p>Here's an example command to explain a notebook without overwriting the original file:</p> <pre><code>llamabot notebook explain /path/to/notebook.ipynb\n</code></pre> <p>This will create a new file at <code>/path/to/notebook_explained.ipynb</code> in the same directory as the original.</p>"},{"location":"cli/notebook/#recommended-usage","title":"Recommended Usage","text":"<p>We recommend that your notebook comprise code cells exclusively. That said, leave comments within your code cells as they'll be picked up by the LLM and can be factored into the explanation.</p> <p>Any existing markdown cells will be skipped in the explanation process.</p> <p>After all, the goal here is to free you up to focus on coding and thinking about the logic of your model, not to have to worry about documenting it!</p>"},{"location":"cli/notebook/#conclusion","title":"Conclusion","text":"<p>Using the LlamaBot Notebook CLI, you can easily provide clear explanations for Jupyter notebook code cells, making them accessible to non-technical audiences. This tool is particularly useful for educators, data scientists, or anyone needing to demystify code logic in a simple and engaging way.</p>"},{"location":"cli/python/","title":"Llamabot Python CLI Tutorial","text":"<p>Welcome to the Llamabot Python CLI tutorial! In this tutorial, we will explore the various commands available in the Llamabot Python CLI and learn how to use them effectively. The Llamabot Python CLI is a powerful tool for generating module-level and function docstrings, as well as generating code based on a given description.</p>"},{"location":"cli/python/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the Llamabot Python CLI installed on your system. You can install it using pip:</p> <pre><code>pip install -U llamabot\n</code></pre> <p>Once installed, you can access the CLI using the <code>llamabot python</code> command.</p>"},{"location":"cli/python/#commands","title":"Commands","text":"<p>The Llamabot Python CLI provides the following commands:</p> <ol> <li><code>module-docstrings</code>: Generate module-level docstrings for a given module file.</li> <li><code>generate-docstrings</code>: Generate function docstrings for a specific function in a module file.</li> <li><code>code-generator</code>: Generate code based on a given description.</li> <li><code>test-writer</code>: Write tests for a given object.</li> </ol> <p>Let's dive into each command and see how they can be used.</p>"},{"location":"cli/python/#1-module-docstrings","title":"1. module-docstrings","text":"<p>The <code>module-docstrings</code> command generates module-level docstrings for a given module file. It takes the following arguments:</p> <ul> <li><code>module_fpath</code>: Path to the module to generate docstrings for.</li> <li><code>dirtree_context_path</code>: (Optional) Path to the directory to use as the context for the directory tree. Defaults to the parent directory of the module file.</li> </ul> <p>Example usage:</p> <pre><code>llamabot python module-docstrings /path/to/your/module.py\n</code></pre> <p>To specify a custom directory tree context path, use the following command:</p> <pre><code>llamabot python module-docstrings /path/to/your/module.py /path/to/your/directory\n</code></pre>"},{"location":"cli/python/#2-generate-docstrings","title":"2. generate-docstrings","text":"<p>The <code>generate-docstrings</code> command generates function docstrings for a specific function in a module file. It takes the following arguments:</p> <ul> <li><code>module_fpath</code>: Path to the module to generate docstrings for.</li> <li><code>object_name</code>: Name of the object to generate docstrings for.</li> <li><code>style</code>: (Optional) Style of docstring to generate. Defaults to \"sphinx\".</li> </ul> <p>Example usage:</p> <pre><code>llamabot python generate-docstrings /path/to/your/module.py function_name\n</code></pre> <p>To specify a custom docstring style, use the following command:</p> <pre><code>llamabot python generate-docstrings /path/to/your/module.py function_name google\n</code></pre>"},{"location":"cli/python/#3-code-generator","title":"3. code-generator","text":"<p>The <code>code-generator</code> command generates code based on a given description. It takes the following argument:</p> <ul> <li><code>request</code>: A description of what the code should do.</li> </ul> <p>Example usage:</p> <pre><code>llamabot python code-generator \"Create a function that adds two numbers\"\n</code></pre>"},{"location":"cli/python/#4-test-writer","title":"4. test-writer","text":"<p>The <code>test-writer</code> command writes tests for a given object. It takes the following arguments:</p> <ul> <li><code>module_fpath</code>: Path to the module to generate tests for.</li> <li><code>object_name</code>: Name of the object to generate tests for.</li> </ul> <p>Example usage:</p> <pre><code>llamabot python test-writer /path/to/your/module.py function_name\n</code></pre>"},{"location":"cli/python/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we have covered the various commands available in the Llamabot Python CLI and learned how to use them effectively. With these commands, you can easily generate module-level and function docstrings, generate code based on a given description, and write tests for your code. Happy coding!</p>"},{"location":"cli/repo/","title":"Chatting with a Code Repository: Llamabot CLI Guide","text":"<p>Welcome to the guide on using the Llamabot CLI for interacting with code repositories. This innovative tool leverages AI to facilitate engaging and insightful conversations with your codebase. Discover how to effectively use this tool to read and understand documentation within a repository.</p>"},{"location":"cli/repo/#getting-started","title":"Getting started","text":"<p>Before you can start chatting with your code repository, ensure that Llamabot CLI is installed on your system. You can install it via pip with the following command:</p> <pre><code>pip install -U llamabot\n</code></pre> <p>Once installed, you can access the CLI using the <code>llamabot repo chat</code> command.</p>"},{"location":"cli/repo/#key-commands","title":"Key commands","text":"<p>Llamabot CLI introduces the <code>chat</code> command, allowing for dynamic interactions with your code repository.</p>"},{"location":"cli/repo/#chat-with-your-repository","title":"Chat with your repository","text":"<p>The <code>chat</code> command allows you to interact with your code repository in a conversational manner.</p>"},{"location":"cli/repo/#usage","title":"Usage","text":"<p>Run the command below to start a conversation with your repository:</p> <pre><code>llamabot repo chat --repo-url https://github.com/yourusername/yourrepo --checkout=\"branch_or_tag\" --source-file-extensions py --source-file-extensions md --model-name=\"gpt-4-0125-preview\"\n</code></pre> <p>Here are the key parameters to understand:</p> <ul> <li><code>--repo-url</code>: URL of the git repository you want to interact with.</li> <li><code>--checkout</code>: Specify the branch or tag you wish to use. The default is \"main\".</li> <li><code>--source-file-extensions</code>: Define the types of source files to include in the conversation. Supports a variety of file extensions.</li> <li><code>--model-name</code>: AI model to be used for generating responses.</li> <li><code>--initial-message</code> (optional): Initial message to start the conversation.</li> <li><code>--panel</code> (optional): Set to <code>true</code> to launch a Panel web app to chat.</li> </ul> <p>After executing the command, Llamabot clones the repository into a temporary directory, processes the files as specified, and starts the chat interface. If <code>--panel</code> is true, the chat interface will be served in a browser.</p>"},{"location":"cli/repo/#conclusion","title":"Conclusion","text":"<p>This guide covers the essential aspects of the Llamabot CLI, a tool designed to enhance your coding experience through AI-powered conversations about a code repository. Embrace these capabilities to make your coding more efficient and insightful. Happy coding!</p>"},{"location":"contributing/setup/","title":"Development Setup","text":"<p>Welcome! This guide will help you set up your development environment for LlamaBot using pixi, our dependency management tool.</p>"},{"location":"contributing/setup/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>pixi installed (installation instructions)</li> </ul> <p>Python Installation</p> <p>You don't need to install Python separately. Pixi will manage the Python version for you (this project requires Python 3.10-3.13).</p>"},{"location":"contributing/setup/#initial-setup","title":"Initial Setup","text":""},{"location":"contributing/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/ericmjl/llamabot.git\ncd llamabot\n</code></pre>"},{"location":"contributing/setup/#2-install-dependencies-with-pixi","title":"2. Install Dependencies with Pixi","text":"<p>This project uses pixi for dependency management. Install all dependencies:</p> <pre><code>pixi install\n</code></pre> <p>This will create a pixi environment with all the necessary dependencies, including:</p> <ul> <li>Core dependencies (defined in <code>[project.dependencies]</code>)</li> <li>Optional dependencies for notebooks, RAG, agents, and CLI tools</li> <li>Development tools (pytest, pre-commit, etc.)</li> <li>Documentation tools (mkdocs, etc.)</li> </ul>"},{"location":"contributing/setup/#3-activate-the-development-environment","title":"3. Activate the Development Environment","text":"<p>You have two options:</p>"},{"location":"contributing/setup/#option-a-use-pixi-shell-recommended","title":"Option A: Use pixi shell (recommended)","text":"<pre><code>pixi shell\n</code></pre> <p>This activates the pixi environment in your current shell.</p>"},{"location":"contributing/setup/#option-b-prefix-commands-with-pixi-run","title":"Option B: Prefix commands with <code>pixi run</code>","text":"<pre><code>pixi run python -c \"import llamabot\"\n</code></pre> <p>Always use pixi</p> <p>All commands must be run with the <code>pixi run</code> prefix or within a <code>pixi shell</code> session. This ensures proper dependency management and environment isolation.</p>"},{"location":"contributing/setup/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/setup/#running-tests","title":"Running Tests","text":"<p>Run the full test suite:</p> <pre><code>pixi run test\n</code></pre> <p>Run a specific test file:</p> <pre><code>pixi run -e tests pytest tests/path/to/specific_test.py\n</code></pre> <p>Run a specific test function:</p> <pre><code>pixi run -e tests pytest tests/path/to/specific_test.py::test_function\n</code></pre> <p>Test Environment</p> <p>Tests run in a separate environment (<code>tests</code>). Use <code>pixi run -e tests</code> when running pytest commands directly.</p>"},{"location":"contributing/setup/#running-the-cli","title":"Running the CLI","text":"<p>Test that the CLI is working:</p> <pre><code>pixi run llamabot-cli\n</code></pre> <p>This runs <code>llamabot --help</code> to verify the CLI is properly installed.</p>"},{"location":"contributing/setup/#working-with-documentation","title":"Working with Documentation","text":"<p>Serve documentation locally:</p> <pre><code>pixi run docs\n</code></pre> <p>This starts a local MkDocs server (usually at <code>http://127.0.0.1:8000</code>).</p> <p>Build documentation:</p> <pre><code>pixi run build-docs\n</code></pre> <p>Marimo Notebooks</p> <p>This project uses Marimo notebooks (<code>.py</code> files), not traditional Jupyter notebooks (<code>.ipynb</code> files). When creating or editing notebooks, always run <code>uvx marimo check &lt;path/to/notebook.py</code> to validate them.</p>"},{"location":"contributing/setup/#building-mcp-documentation","title":"Building MCP Documentation","text":"<p>Build the MCP documentation database:</p> <pre><code>pixi run build-mcp-docs\n</code></pre>"},{"location":"contributing/setup/#code-quality","title":"Code Quality","text":""},{"location":"contributing/setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>This project uses pre-commit hooks to ensure code quality. Pre-commit should be installed globally:</p> <pre><code>uv tool install pre-commit\n</code></pre> <p>Install the hooks:</p> <pre><code>pre-commit install\n</code></pre> <p>The hooks will automatically run on commit and check for:</p> <ul> <li>Code formatting (Black)</li> <li>Linting (Ruff)</li> <li>Docstring coverage (interrogate)</li> <li>Docstring style (pydoclint)</li> <li>And more</li> </ul> <p>Run hooks manually:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"contributing/setup/#markdown-linting","title":"Markdown Linting","text":"<p>Markdown files are linted using <code>markdownlint</code>. Install it globally:</p> <pre><code>pixi global install markdownlint\n</code></pre> <p>Lint a markdown file:</p> <pre><code>markdownlint filename.md\n</code></pre> <p>Always run markdownlint on any markdown file you edit.</p>"},{"location":"contributing/setup/#project-structure","title":"Project Structure","text":""},{"location":"contributing/setup/#key-directories","title":"Key Directories","text":"<ul> <li><code>llamabot/</code> - Main source code</li> <li><code>bot/</code> - Bot implementations (SimpleBot, QueryBot, etc.)</li> <li><code>components/</code> - Modular components (messages, docstore, tools, etc.)</li> <li><code>cli/</code> - CLI commands</li> <li><code>web/</code> - Web interface (FastAPI + HTMX)</li> <li><code>tests/</code> - Test suite (mirrors source structure)</li> <li><code>docs/</code> - Documentation</li> <li><code>examples/</code> - Example scripts</li> <li><code>notebooks/</code> - Marimo notebooks</li> </ul>"},{"location":"contributing/setup/#environment-features","title":"Environment Features","text":"<p>Pixi environments are organized by features (defined in <code>pyproject.toml</code>):</p> <ul> <li>default - Full development environment (tests, devtools, docs, notebooks, rag, agent, cli)</li> <li>tests - Testing dependencies (pytest, hypothesis, etc.)</li> <li>docs - Documentation tools (mkdocs, etc.)</li> <li>notebooks - Notebook dependencies (ollama, etc.)</li> <li>bare - Minimal environment (devtools only)</li> </ul>"},{"location":"contributing/setup/#common-tasks","title":"Common Tasks","text":""},{"location":"contributing/setup/#adding-a-new-dependency","title":"Adding a New Dependency","text":"<ol> <li>Add it to the appropriate section in <code>pyproject.toml</code>:</li> <li><code>[project.dependencies]</code> for runtime dependencies</li> <li><code>[project.optional-dependencies.&lt;group&gt;]</code> for optional dependencies</li> <li><code>[tool.pixi.feature.&lt;feature&gt;.pypi-dependencies]</code> for feature-specific dependencies</li> <li>Run <code>pixi install</code> to update the environment</li> </ol>"},{"location":"contributing/setup/#running-python-code","title":"Running Python Code","text":"<p>Always use pixi:</p> <pre><code># \u2705 Correct\npixi run python script.py\n\n# \u274c Incorrect (will fail)\npython script.py\n</code></pre>"},{"location":"contributing/setup/#installing-the-package-in-editable-mode","title":"Installing the Package in Editable Mode","text":"<p>The package is automatically installed in editable mode when you run <code>pixi install</code>. You can verify:</p> <pre><code>pixi run python -c \"import llamabot; print(llamabot.__file__)\"\n</code></pre>"},{"location":"contributing/setup/#getting-help","title":"Getting Help","text":"<ul> <li>Check the AGENTS.md file for detailed development patterns</li> <li>Review existing code in the repository for examples</li> <li>Open an issue on GitHub for questions or problems</li> </ul>"},{"location":"contributing/setup/#next-steps","title":"Next Steps","text":"<p>Once your environment is set up:</p> <ol> <li>Explore the codebase structure</li> <li>Read the AGENTS.md for development patterns</li> <li>Pick an issue or feature to work on</li> <li>Write tests for your changes</li> <li>Submit a pull request</li> </ol> <p>Happy contributing!</p>"},{"location":"design/agents-build-own-tools/","title":"Secure Script Execution for LLM Agents","text":"<p>A secure pattern for executing self-written code from LLM agents in local environments.</p>"},{"location":"design/agents-build-own-tools/#overview","title":"Overview","text":"<p>This design document outlines a secure approach for allowing LLM agents to write and execute Python code in a sandboxed environment. The pattern uses Docker containers and PEP 723 metadata to create a secure execution environment for agent-generated code.</p>"},{"location":"design/agents-build-own-tools/#core-components","title":"Core Components","text":""},{"location":"design/agents-build-own-tools/#1-script-metadata","title":"1. Script Metadata","text":"<p>Scripts are written with PEP 723 metadata headers that specify dependencies and requirements:</p> <pre><code># /// script\n# requires-python = \"&gt;=3.11\"\n# dependencies = [\n#   \"requests&lt;3\",\n#   \"rich\",\n# ]\n# auth = \"agent-id-hash\"\n# purpose = \"task-description\"\n# timestamp = \"iso-timestamp\"\n# ///\n\n# agent code here\n</code></pre>"},{"location":"design/agents-build-own-tools/#2-docker-configuration","title":"2. Docker Configuration","text":"<p>The execution environment uses the Astral UV image for optimal Python package management:</p> <pre><code># Use a Python image with uv pre-installed\nFROM ghcr.io/astral-sh/uv:python3.12-bookworm-slim\n\n# Install the project into `/app`\nWORKDIR /app\n\n# Enable bytecode compilation\nENV UV_COMPILE_BYTECODE=1\n\n# Copy from the cache instead of linking since it's a mounted volume\nENV UV_LINK_MODE=copy\n\n# Reset the entrypoint, don't invoke `uv`\nENTRYPOINT []\n\n# Run as non-root user for security\nUSER nobody\n\n# Run the script with uv\nCMD [\"uv\", \"run\", \"--system-site-packages=false\"]\n</code></pre>"},{"location":"design/agents-build-own-tools/#3-script-execution-flow","title":"3. Script Execution Flow","text":"<ol> <li>Agent generates Python code with metadata</li> <li>Code is written to a temporary directory</li> <li>Docker container is built with security constraints</li> <li>Script is executed in isolated environment</li> <li>Results are captured and returned to agent</li> </ol>"},{"location":"design/agents-build-own-tools/#4-security-measures","title":"4. Security Measures","text":"<p>The implementation includes multiple layers of security:</p> <ol> <li>Container Restrictions:</li> <li>Read-only root filesystem</li> <li>No network access by default</li> <li>Limited CPU (1 core) and memory (512MB)</li> <li>Mounted script directory is read-only</li> <li>Results directory is write-only</li> <li>All capabilities dropped</li> <li> <p>No privilege escalation</p> </li> <li> <p>Code Validation:</p> </li> <li>PEP 723 metadata verification</li> <li>Execution timeout enforcement</li> <li>Structured error handling</li> <li>Output validation</li> </ol>"},{"location":"design/agents-build-own-tools/#implementation","title":"Implementation","text":"<p>The core implementation consists of two main classes:</p> <ol> <li><code>ScriptMetadata</code>: Pydantic model for script metadata</li> <li><code>ScriptExecutor</code>: Handles script writing and secure execution</li> </ol> <p>Example usage:</p> <pre><code>@tool\ndef write_and_execute_script(\n    code: str,\n    python_version: str = \"&gt;=3.11\",\n    dependencies: Optional[List[str]] = None,\n    purpose: str = \"\",\n    timeout: int = 30,\n) -&gt; Dict[str, Any]:\n    \"\"\"Write and execute a Python script in a secure sandbox.\"\"\"\n    metadata = ScriptMetadata(\n        requires_python=python_version,\n        dependencies=dependencies or [],\n        auth=str(uuid4()),\n        purpose=purpose,\n        timestamp=datetime.now(),\n    )\n\n    executor = ScriptExecutor()\n    script_path = executor.write_script(code, metadata)\n    return executor.run_script(script_path, timeout)\n</code></pre>"},{"location":"design/agents-build-own-tools/#benefits","title":"Benefits","text":"<ol> <li>Security: Multiple layers of isolation and restrictions</li> <li>Flexibility: Agents can write custom code solutions</li> <li>Dependency Management: Clean environment for each execution</li> <li>Resource Control: Strict limits on compute resources</li> <li>Auditability: Metadata tracking and logging</li> </ol>"},{"location":"design/agents-build-own-tools/#testing","title":"Testing","text":"<p>The implementation includes comprehensive tests:</p> <ul> <li>Script writing and metadata handling</li> <li>Execution in sandbox environment</li> <li>Timeout enforcement</li> <li>Error handling</li> <li>Dependency management</li> <li>Resource restrictions</li> </ul>"},{"location":"design/agents-build-own-tools/#future-enhancements","title":"Future Enhancements","text":"<p>Potential areas for improvement:</p> <ol> <li>Network access controls for specific domains</li> <li>Resource usage monitoring and logging</li> <li>Script validation and static analysis</li> <li>Caching of commonly used dependencies</li> <li>Support for additional runtime environments</li> </ol>"},{"location":"design/log_viewer/","title":"LlamaBot Log Viewer Design Document","text":""},{"location":"design/log_viewer/#overview","title":"Overview","text":"<p>The LlamaBot Log Viewer is a web-based interface for inspecting and analyzing LlamaBot call logs. It provides a comprehensive view of all interactions with the bot, including prompts, responses, and tool usage.</p>"},{"location":"design/log_viewer/#core-features","title":"Core Features","text":""},{"location":"design/log_viewer/#1-log-inspection","title":"1. Log Inspection \u2705","text":"<ul> <li>Split Panel Layout</li> <li>Left panel: List of log entries</li> <li>Right panel: Detailed view of selected log</li> <li>Both panels scroll independently</li> <li> <p>Fixed headers for easy navigation</p> </li> <li> <p>Log Entry List (Left Panel)</p> </li> <li>Sortable columns:<ul> <li>ID</li> <li>Object Name</li> <li>Timestamp</li> <li>Model Name</li> <li>Temperature</li> <li>Prompts Used</li> </ul> </li> <li>Clickable rows to view details</li> <li>Visual indicators for helpful/not helpful ratings</li> <li> <p>Compact view with essential information</p> </li> <li> <p>Log Details (Right Panel)</p> </li> <li>Full conversation history</li> <li>Expandable/collapsible messages</li> <li>Syntax highlighting for code blocks</li> <li>Tool call visualization</li> <li>Prompt template display</li> <li>Rating controls (helpful/not helpful)</li> </ul>"},{"location":"design/log_viewer/#2-filtering-and-search","title":"2. Filtering and Search \u2705","text":"<ul> <li>Text Search</li> <li>Real-time filtering as you type</li> <li>Search across all fields</li> <li> <p>Highlight matching text</p> </li> <li> <p>Function Name Filter</p> </li> <li>Dropdown of available prompt functions</li> <li>Shows version count for each function</li> <li>Multi-select capability</li> </ul>"},{"location":"design/log_viewer/#3-export-functionality","title":"3. Export Functionality \u2705","text":"<ul> <li>Export Formats</li> <li>OpenAI format (JSONL)</li> <li> <p>Support for additional formats in future</p> </li> <li> <p>Export Options</p> </li> <li>Filter by text search</li> <li>Filter by function name</li> <li>Export only helpful responses</li> <li>Include/exclude specific fields</li> </ul>"},{"location":"design/log_viewer/#4-prompt-comparison","title":"4. Prompt Comparison \u2705","text":"<ul> <li>Version Comparison</li> <li>Side-by-side diff view</li> <li>Syntax highlighting</li> <li>Line-by-line comparison</li> <li> <p>Highlight changes</p> </li> <li> <p>Selection Interface</p> </li> <li>Dropdown to select prompt versions</li> <li>Preview of selected prompts</li> <li>Easy switching between versions</li> </ul>"},{"location":"design/log_viewer/#5-experiment-view","title":"5. Experiment View \u2705","text":"<ul> <li>Experiment List</li> <li>Name and run count</li> <li>Timestamp of last run</li> <li>Success/failure indicators</li> <li> <p>Column visibility controls:</p> <ul> <li>Toggle buttons for each column</li> <li>Persistent column visibility preferences</li> <li>Quick show/hide all columns</li> <li>Responsive layout adjustments</li> </ul> </li> <li> <p>Run Details</p> </li> <li>Metrics visualization</li> <li>Message log links</li> <li>Prompt versions used</li> <li>Performance statistics</li> </ul>"},{"location":"design/log_viewer/#technical-implementation","title":"Technical Implementation","text":""},{"location":"design/log_viewer/#data-structures","title":"Data Structures","text":"<ul> <li>Maintain existing database schema</li> <li>No changes to current data models</li> <li>Leverage existing relationships between tables</li> </ul>"},{"location":"design/log_viewer/#ui-components","title":"UI Components","text":"<ul> <li>Use HTMX for dynamic updates</li> <li>Bootstrap for responsive layout</li> <li>Custom CSS for specialized components</li> <li>Jinja2 templates for server-side rendering</li> </ul>"},{"location":"design/log_viewer/#templates-structure","title":"Templates Structure","text":"<ul> <li>Base Template (<code>base.html</code>)</li> <li>Common layout structure</li> <li>Navigation bar with links to each page</li> <li>Footer</li> <li>Common CSS/JS includes</li> <li> <p>HTMX setup</p> </li> <li> <p>Log Viewer Page (<code>logs/index.html</code>)</p> </li> <li>Main log viewer container</li> <li>Split panel layout</li> <li><code>log_table.html</code>: Left panel log list</li> <li><code>log_details.html</code>: Right panel log details</li> <li><code>log_tbody.html</code>: Dynamic log table body for HTMX updates</li> <li><code>message_log.html</code>: Message list component</li> <li> <p><code>rating_buttons.html</code>: Helpful/not helpful rating controls</p> </li> <li> <p>Prompt Comparison Page (<code>prompts/index.html</code>)</p> </li> <li>Main comparison view</li> <li><code>prompt_selector.html</code>: Version selection interface</li> <li> <p><code>prompt_diff.html</code>: Side-by-side diff view</p> </li> <li> <p>Experiment Page (<code>experiments/index.html</code>)</p> </li> <li>List of experiments</li> <li><code>experiment_details.html</code>: Run details and metrics</li> <li> <p><code>experiment_metrics.html</code>: Metrics visualization</p> </li> <li> <p>Macros (<code>macros.html</code>)</p> </li> <li><code>message_expansion</code>: Message display with expand/collapse</li> <li><code>prompt_display</code>: Prompt template rendering</li> <li><code>tool_call</code>: Tool call visualization</li> <li><code>metric_card</code>: Metric display component</li> <li><code>diff_view</code>: Diff visualization</li> <li><code>column_visibility</code>: Column visibility controls component</li> </ul>"},{"location":"design/log_viewer/#router-structure","title":"Router Structure","text":"<ul> <li>Log Router (<code>/logs</code>)</li> <li><code>GET /</code>: Main log viewer page with split panel layout</li> <li><code>GET /filtered_logs</code>: HTMX endpoint for filtered log list</li> <li><code>GET /{log_id}</code>: Individual log details</li> <li><code>GET /{log_id}/expand</code>: Expand all messages</li> <li><code>GET /{log_id}/collapse</code>: Collapse all messages</li> <li><code>POST /{log_id}/rate</code>: Rate log as helpful/not helpful</li> <li> <p><code>GET /export/{format}</code>: Export logs in specified format</p> </li> <li> <p>Prompt Router (<code>/prompts</code>)</p> </li> <li><code>GET /</code>: Main prompt comparison page</li> <li><code>GET /history</code>: View prompt version history</li> <li><code>GET /functions</code>: Get list of prompt functions</li> <li><code>GET /{prompt_hash}</code>: View specific prompt version</li> <li> <p><code>GET /compare</code>: Compare two prompt versions</p> </li> <li> <p>Experiment Router (<code>/experiments</code>)</p> </li> <li><code>GET /</code>: Main experiment page</li> <li><code>GET /details</code>: View experiment details</li> <li><code>GET /runs</code>: List runs for an experiment</li> <li> <p><code>GET /metrics</code>: Get experiment metrics</p> </li> <li> <p>Common Patterns</p> </li> <li>All endpoints use FastAPI dependency injection for DB sessions</li> <li>Consistent error handling with HTTPException</li> <li>HTMX integration for dynamic updates</li> <li>Proper type hints and docstrings</li> <li>Logging for debugging and monitoring</li> </ul>"},{"location":"design/log_viewer/#static-files-structure","title":"Static Files Structure","text":"<ul> <li>CSS (<code>static/styles.css</code>)</li> <li>Base styles for layout and components</li> <li>Responsive design utilities</li> <li>Custom component styles</li> <li>Dark/light theme support</li> <li> <p>Print styles for exports</p> </li> <li> <p>JavaScript (<code>static/script.js</code>)</p> </li> <li>HTMX event handlers</li> <li>Modal management</li> <li>Export functionality</li> <li>Keyboard shortcuts</li> <li> <p>Dynamic UI updates</p> </li> <li> <p>Third-party Dependencies</p> </li> <li> <p>Core Dependencies (loaded from CDN with SRI)</p> <ul> <li>Bootstrap 5.3.6 (CSS only)</li> <li>HTMX 2.0.0</li> <li>Highlight.js 11.11.1</li> </ul> </li> <li> <p>Loading Strategy</p> <ul> <li>Defer non-critical CSS/JS</li> <li>Use async loading for highlight.js</li> <li>Implement resource hints (preconnect)</li> <li>Bundle and minify custom CSS/JS</li> <li>Use SRI hashes for security</li> </ul> </li> <li> <p>Version Management</p> <ul> <li>Lock dependency versions</li> <li>Regular security audits</li> <li>Automated dependency updates</li> <li>Fallback CDN sources</li> </ul> </li> <li> <p>Asset Organization</p> </li> <li>Modular CSS with component-specific styles</li> <li>Vanilla JS for minimal dependencies</li> <li>No build step required</li> <li>Easy to maintain and extend</li> </ul>"},{"location":"design/log_viewer/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Pagination for large log sets</li> <li>Lazy loading of log details</li> <li>Efficient database queries</li> <li>Caching of frequently accessed data</li> </ul>"},{"location":"design/log_viewer/#user-experience","title":"User Experience","text":""},{"location":"design/log_viewer/#navigation","title":"Navigation","text":"<ul> <li>Clear visual hierarchy</li> <li>Consistent layout across views</li> <li>Intuitive filtering controls</li> <li>Quick access to common actions</li> </ul>"},{"location":"design/log_viewer/#responsiveness","title":"Responsiveness","text":"<ul> <li>Mobile-friendly design</li> <li>Adaptive layout for different screen sizes</li> <li>Touch-friendly controls</li> <li>Keyboard shortcuts for power users</li> </ul>"},{"location":"design/log_viewer/#visual-feedback","title":"Visual Feedback","text":"<ul> <li>Loading indicators</li> <li>Success/error messages</li> <li>Clear status indicators</li> <li>Helpful tooltips</li> </ul>"},{"location":"design/log_viewer/#security","title":"Security","text":"<ul> <li>Input sanitization</li> <li>SQL injection prevention</li> <li>XSS protection</li> <li>Rate limiting</li> </ul>"},{"location":"design/log_viewer/#accessibility","title":"Accessibility","text":"<ul> <li>ARIA labels</li> <li>Keyboard navigation</li> <li>Screen reader support</li> <li>High contrast mode</li> </ul>"},{"location":"design/log_viewer/#implementation-plan","title":"Implementation Plan","text":""},{"location":"design/log_viewer/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":"<ol> <li>Base Setup</li> <li>Set up FastAPI application structure</li> <li>Configure database connections</li> <li>Implement basic routing</li> <li> <p>Set up static file serving</p> </li> <li> <p>Basic Templates</p> </li> <li>Create base template with layout</li> <li>Implement navigation structure</li> <li>Set up HTMX integration</li> <li>Add Bootstrap base styles</li> </ol>"},{"location":"design/log_viewer/#phase-2-log-viewing","title":"Phase 2: Log Viewing","text":"<ol> <li>Log List View</li> <li>Implement log table component</li> <li>Add basic filtering</li> <li>Set up pagination</li> <li> <p>Add sorting functionality</p> </li> <li> <p>Log Details</p> </li> <li>Create log detail view</li> <li>Implement message expansion</li> <li>Add syntax highlighting</li> <li>Set up rating system</li> </ol>"},{"location":"design/log_viewer/#phase-3-advanced-features","title":"Phase 3: Advanced Features","text":"<ol> <li>Search and Filter</li> <li>Implement text search</li> <li>Add function name filtering</li> <li>Create advanced filter UI</li> <li> <p>Add filter persistence</p> </li> <li> <p>Export Functionality</p> </li> <li>Implement OpenAI format export</li> <li>Add helpful-only export</li> <li>Create export UI</li> <li>Add progress indicators</li> </ol>"},{"location":"design/log_viewer/#phase-4-prompt-management","title":"Phase 4: Prompt Management","text":"<ol> <li>Prompt Comparison</li> <li>Create prompt version list</li> <li>Implement diff view</li> <li>Add version selection</li> <li> <p>Set up comparison UI</p> </li> <li> <p>Experiment View</p> </li> <li>Create experiment list</li> <li>Implement run details</li> <li>Add metrics visualization</li> <li>Set up experiment navigation</li> </ol>"},{"location":"design/log_viewer/#phase-5-polish-and-optimization","title":"Phase 5: Polish and Optimization","text":"<ol> <li>Performance</li> <li>Implement caching</li> <li>Optimize database queries</li> <li>Add lazy loading</li> <li> <p>Improve response times</p> </li> <li> <p>UI/UX Refinement</p> </li> <li>Add loading states</li> <li>Implement error handling</li> <li>Add keyboard shortcuts</li> <li>Improve accessibility</li> </ol>"},{"location":"design/log_viewer/#phase-6-testing-and-documentation","title":"Phase 6: Testing and Documentation","text":"<ol> <li>Testing</li> <li>Write unit tests</li> <li>Add integration tests</li> <li>Implement E2E tests</li> <li> <p>Set up CI/CD</p> </li> <li> <p>Documentation</p> </li> <li>Write API documentation</li> <li>Create user guide</li> <li>Add code comments</li> <li>Update README</li> </ol> <p>Each phase builds upon the previous one, ensuring we have a working system at each step. We can adjust the order based on priorities or dependencies.</p>"},{"location":"design/observability/","title":"Observability in LlamaBot","text":"<p>This document describes the observability features built into LlamaBot's various bot implementations. These features help track bot behavior, performance, and usage patterns.</p>"},{"location":"design/observability/#span-based-observability","title":"Span-Based Observability","text":"<p>All bots now automatically create spans for observability. Spans provide structured tracing of bot operations with timing, attributes, and hierarchical relationships. Spans are always enabled by default - no need to call <code>enable_span_recording()</code>.</p>"},{"location":"design/observability/#span-attributes","title":"Span Attributes","text":"<p>Each bot call creates a span with relevant attributes:</p> <ul> <li> <p>SimpleBot: <code>query</code>, <code>model</code>, <code>temperature</code>, <code>input_message_count</code>, <code>input_user_messages</code>, <code>input_assistant_messages</code>, <code>tool_calls</code>, <code>tool_calls_count</code></p> </li> <li> <p>QueryBot: <code>query</code>, <code>n_results</code>, <code>model</code>, <code>docstore_results</code>, <code>memory_results</code></p> </li> <li> <p>StructuredBot: <code>query</code>, <code>model</code>, <code>pydantic_model</code>, <code>num_attempts</code>, <code>validation_attempts</code>, <code>validation_success</code>, <code>validation_time</code>, <code>schema_fields</code>, <code>schema_nested_models</code></p> </li> <li> <p>AgentBot: <code>query</code>, <code>max_iterations</code>, <code>result</code>, <code>iterations</code></p> </li> </ul>"},{"location":"design/observability/#nested-spans","title":"Nested Spans","text":"<p>Bots create nested spans for detailed observability:</p> <ul> <li><code>simplebot_call</code> / <code>querybot_call</code> / <code>structuredbot_call</code> / <code>agentbot_call</code> (root)</li> <li><code>llm_request</code> (LLM API call)</li> <li><code>llm_response</code> (response processing)</li> <li><code>retrieval</code> (for QueryBot - document retrieval)</li> <li><code>memory_retrieval</code> (for QueryBot - memory retrieval)</li> </ul>"},{"location":"design/observability/#accessing-spans","title":"Accessing Spans","text":"<p>View spans in several ways:</p> <pre><code>from llamabot import get_spans\n\n# Get all spans\nall_spans = get_spans()\n\n# Get spans for a specific operation\nspans = get_spans(operation_name=\"simplebot_call\")\n\n# Display bot's spans (in marimo notebooks)\nbot = SimpleBot(...)\nbot(\"Hello\")  # Make a call\nbot  # Display spans automatically via _repr_html_()\n</code></pre>"},{"location":"design/observability/#legacy-run_meta-deprecated","title":"Legacy: run_meta (Deprecated)","text":"<p>Note: The <code>run_meta</code> dictionary is deprecated for bots. Spans now provide all the same information and more. <code>run_meta</code> is only used internally when span recording is disabled, which no longer happens for bots since spans are always enabled.</p>"},{"location":"design/observability/#best-practices","title":"Best Practices","text":"<ol> <li>Use Span Visualization: Display bot objects in marimo notebooks to automatically see span hierarchies and timing</li> <li>Query by Operation: Use <code>get_spans(operation_name=\"...\")</code> to filter spans and see only relevant operations</li> <li>Monitor Validation: For <code>StructuredBot</code>, check <code>validation_attempts</code> and <code>validation_success</code> attributes in spans</li> <li>Track Retrieval Performance: For <code>QueryBot</code>, check <code>docstore_results</code> and <code>memory_results</code> attributes</li> <li>Analyze Tool Usage: Check <code>tool_calls</code> and <code>tool_calls_count</code> attributes to understand tool execution patterns</li> <li>View Hierarchies: Spans automatically show parent-child relationships, making it easy to understand execution flow</li> </ol>"},{"location":"design/observability/#logging","title":"Logging","text":"<p>All bot interactions are automatically logged using <code>sqlite_log</code>, which stores:</p> <ul> <li>Complete message history</li> <li>Tool calls and their results</li> <li>Timing information</li> <li>Bot configuration</li> </ul> <p>This logging can be used for:</p> <ul> <li>Debugging issues</li> <li>Analyzing bot performance</li> <li>Training data collection</li> <li>Usage pattern analysis</li> </ul>"},{"location":"design/unified_chat_memory/","title":"Unified Chat Memory Design","text":""},{"location":"design/unified_chat_memory/#overview","title":"Overview","text":"<p>This document outlines the design for a unified chat memory system that consolidates linear and graph-based memory into a single, configurable class. The system separates storage and retrieval concerns while providing multiple API levels for different use cases.</p>"},{"location":"design/unified_chat_memory/#quick-start","title":"Quick Start","text":"<pre><code>import llamabot as lmb\n\n# Simple linear memory (fast, no LLM calls)\nmemory = lmb.ChatMemory()\n\n# Intelligent graph memory with threading\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n\n# Use with any bot\nbot = lmb.SimpleBot(system_prompt=\"You are helpful\", memory=memory)\nresponse = bot(\"Hello!\")  # Memory automatically stores and retrieves context\n</code></pre>"},{"location":"design/unified_chat_memory/#core-design-principles","title":"Core Design Principles","text":"<ol> <li>Unified Interface: Single class handles both linear and graph-based memory</li> <li>Separation of Concerns: Storage operations (append) are separate from retrieval operations (search/context)</li> <li>Configuration at Instantiation: Memory mode and behavior are set once and never change</li> <li>NetworkX Backend: Direct use of NetworkX for graph operations without over-abstraction</li> <li>Optional Summarization: Summarization is optional and can be disabled for performance</li> </ol>"},{"location":"design/unified_chat_memory/#key-concepts","title":"Key Concepts","text":""},{"location":"design/unified_chat_memory/#linear-vs-graph-memory","title":"Linear vs Graph Memory","text":"Feature Linear Memory Graph Memory Speed Fast (no LLM calls) Slower (LLM for threading) Intelligence Simple (last N messages) Smart (semantic threading) Use Case Simple conversations Complex multi-threaded chats LLM Calls None 1-2 per message (optional)"},{"location":"design/unified_chat_memory/#conversation-threading","title":"Conversation Threading","text":"<p>Linear Memory: Messages are stored in order, retrieved as recent history</p> <pre><code>H1 \u2192 A1 \u2192 H2 \u2192 A2 \u2192 H3 \u2192 A3\n</code></pre> <p>Graph Memory: Messages are intelligently connected based on content</p> <pre><code>H1: \"Let's talk about Python\"\n\u2514\u2500\u2500 A1: \"Python is great for data science\"\n    \u251c\u2500\u2500 H2: \"What about machine learning?\" \u2192 A2: \"ML libraries include...\"\n    \u2514\u2500\u2500 H3: \"Tell me about databases\" \u2192 A3: \"SQL databases are...\"\n</code></pre>"},{"location":"design/unified_chat_memory/#architecture","title":"Architecture","text":""},{"location":"design/unified_chat_memory/#api-levels","title":"API Levels","text":""},{"location":"design/unified_chat_memory/#high-level-api-opinionated","title":"High-Level API (Opinionated)","text":"<pre><code># Default linear memory\nmemory = ChatMemory()  # Uses LinearNodeSelector by default\n\n# Graph memory with LLM-based threading\nmemory = ChatMemory.threaded(model=\"gpt-4o-mini\")\n</code></pre>"},{"location":"design/unified_chat_memory/#low-level-api-full-configurability","title":"Low-Level API (Full Configurability)","text":"<pre><code>memory = ChatMemory(\n    node_selector=LLMNodeSelector(model=\"gpt-4o-mini\"),\n    summarizer=LLMSummarizer(model=\"gpt-4o-mini\"),  # Optional\n    context_depth=5  # Default context depth for retrieval\n)\n</code></pre>"},{"location":"design/unified_chat_memory/#factory-methods-implementation","title":"Factory Methods Implementation","text":"<pre><code>@classmethod\ndef threaded(cls, model: str = \"gpt-4o-mini\", **kwargs) -&gt; \"ChatMemory\":\n    \"\"\"Create ChatMemory with LLM-based threading.\n\n    :param model: LLM model name for node selection and summarization\n    :param kwargs: Additional arguments passed to ChatMemory constructor\n    \"\"\"\n    return cls(\n        node_selector=LLMNodeSelector(model=model),\n        summarizer=LLMSummarizer(model=model),  # Optional but recommended for threading\n        **kwargs\n    )\n</code></pre>"},{"location":"design/unified_chat_memory/#under-the-hood-what-threaded-actually-does","title":"Under the Hood: What <code>.threaded()</code> Actually Does","text":"<p>When you call <code>ChatMemory.threaded(model=\"gpt-4o-mini\")</code>, here's exactly what happens:</p> <pre><code># 1. Factory method creates LLMNodeSelector\nllm_selector = LLMNodeSelector(model=\"gpt-4o-mini\")\n# This creates a selector that will use GPT-4o-mini to choose conversation threads\n\n# 2. Factory method creates LLMSummarizer\nllm_summarizer = LLMSummarizer(model=\"gpt-4o-mini\")\n# This creates a summarizer that will generate message summaries for better threading\n\n# 3. Factory method calls the main constructor\nmemory = ChatMemory(\n    node_selector=llm_selector,\n    summarizer=llm_summarizer,\n    context_depth=5  # Default value\n)\n\n# 4. Constructor initializes the memory system\ndef __init__(self, node_selector, summarizer, context_depth=5):\n    self.graph = nx.DiGraph()  # Empty conversation graph\n    self.node_selector = llm_selector  # Will use LLM for thread selection\n    self.summarizer = llm_summarizer   # Will generate message summaries\n    self.context_depth = context_depth # How far back to look for context\n    self._next_node_id = 1             # Start numbering nodes from 1\n</code></pre> <p>Result: You get a <code>ChatMemory</code> instance that:</p> <ul> <li>Uses LLM-based intelligent threading instead of linear memory</li> <li>Automatically generates message summaries for better thread selection</li> <li>Maintains a conversation graph with parent-child relationships</li> <li>Can retrieve context by traversing conversation threads</li> </ul> <p>Equivalent Manual Creation:</p> <pre><code># This is exactly what .threaded() does internally\nmemory = ChatMemory(\n    node_selector=LLMNodeSelector(model=\"gpt-4o-mini\"),\n    summarizer=LLMSummarizer(model=\"gpt-4o-mini\"),\n    context_depth=5\n)\n</code></pre> <p>Note: We chose the factory method pattern over alternatives like constructor with mode parameters or separate classes. The factory pattern provides clearer intent through descriptive method names while keeping the <code>__init__</code> method clean and focused on low-level configuration. This approach makes the API more readable and maintainable, especially as we add more memory modes and configuration options.</p>"},{"location":"design/unified_chat_memory/#data-model","title":"Data Model","text":""},{"location":"design/unified_chat_memory/#conversationnode","title":"ConversationNode","text":"<pre><code>@dataclass\nclass ConversationNode:\n    id: int  # Auto-incremented based on number of nodes in graph\n    message: BaseMessage  # Single message (not conversation turn)\n    summary: Optional[MessageSummary] = None\n    parent_id: Optional[int] = None\n    timestamp: datetime = field(default_factory=datetime.now)\n</code></pre> <p>Key Points:</p> <ul> <li>Each node represents a single message (human or assistant)</li> <li>id: Auto-incremented integer providing natural ordering</li> <li>parent_id: Creates threading relationships (None for root)</li> <li>message: Contains role information (human/assistant) via BaseMessage</li> <li>timestamp: Metadata for when the message was created</li> <li>summary: Optional for performance</li> <li>Immutable once created</li> </ul>"},{"location":"design/unified_chat_memory/#messagesummary","title":"MessageSummary","text":"<pre><code>class MessageSummary(BaseModel):\n    title: str = Field(..., description=\"Title of the message\")\n    summary: str = Field(..., description=\"Summary of the message. Two sentences max.\")\n</code></pre>"},{"location":"design/unified_chat_memory/#usage-examples","title":"Usage Examples","text":""},{"location":"design/unified_chat_memory/#basic-simplebot-with-linear-memory","title":"Basic SimpleBot with Linear Memory","text":"<pre><code>import llamabot as lmb\n\n# Create a bot with simple linear memory (fast, no LLM calls)\nmemory = lmb.ChatMemory()  # Default linear\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\",\n    memory=memory\n)\n\n# Chat loop automatically uses memory\nresponse1 = bot(\"Hello! How are you?\")\nresponse2 = bot(\"What did I just ask you?\")  # Bot can reference previous conversation\n</code></pre>"},{"location":"design/unified_chat_memory/#simplebot-with-graph-memory","title":"SimpleBot with Graph Memory","text":"<pre><code>import llamabot as lmb\n\n# Create a bot with intelligent threading (uses LLM for smart connections)\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\",\n    memory=memory\n)\n\n# Bot can now handle conversation threading intelligently\nresponse1 = bot(\"Let's talk about Python programming.\")\nresponse2 = bot(\"What are the benefits of using Python?\")  # Continues Python thread\nresponse3 = bot(\"Now let's discuss machine learning.\")  # Starts new thread\nresponse4 = bot(\"What libraries should I use for ML in Python?\")  # Connects back to Python thread\n</code></pre>"},{"location":"design/unified_chat_memory/#custom-memory-configuration","title":"Custom Memory Configuration","text":"<pre><code>import llamabot as lmb\n\n# Custom memory configuration (advanced users)\nmemory = lmb.ChatMemory(\n    node_selector=lmb.LLMNodeSelector(model=\"gpt-4o-mini\"),\n    summarizer=lmb.LLMSummarizer(model=\"gpt-4o-mini\"),  # Optional for better threading\n    context_depth=10  # How far back to look for context\n)\n\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a coding assistant.\",\n    model_name=\"gpt-4o-mini\",\n    memory=memory\n)\n</code></pre>"},{"location":"design/unified_chat_memory/#memory-retrieval-in-chat-loop","title":"Memory Retrieval in Chat Loop","text":"<pre><code>import llamabot as lmb\n\n# Bot with memory that can retrieve context\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\",\n    memory=memory\n)\n\n# Simulate a conversation\nbot(\"I'm working on a Python project.\")\nbot(\"I need to handle file I/O.\")\nbot(\"What's the best way to read CSV files?\")\nbot(\"Can you remind me what we discussed about file I/O?\")  # Bot retrieves relevant context\n</code></pre>"},{"location":"design/unified_chat_memory/#memory-export-and-visualization","title":"Memory Export and Visualization","text":"<pre><code>from llamabot.bot.simplebot import SimpleBot\nfrom llamabot.components.chat_memory import ChatMemory\n\n# Create bot with graph memory\nmemory = ChatMemory.threaded(model=\"gpt-4o-mini\")\nbot = SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\",\n    memory=memory\n)\n\n# Have a conversation\nbot(\"Let's discuss Python.\")\nbot(\"What about data structures?\")\nbot(\"Now let's talk about machine learning.\")\nbot(\"What ML libraries work well with Python?\")\n\n# Export conversation graph\nmermaid_diagram = memory.to_mermaid()\nprint(mermaid_diagram)\n\n# Get conversation statistics\nprint(f\"Total messages: {len(memory.graph.nodes())}\")\nprint(f\"Conversation threads: {len([n for n in memory.graph.nodes() if memory.graph.out_degree(n) == 0])}\")\n</code></pre>"},{"location":"design/unified_chat_memory/#when-to-use-each-memory-type","title":"When to Use Each Memory Type","text":"Use Case Memory Type Why Simple Q&amp;A <code>lmb.ChatMemory()</code> Fast, no LLM calls needed Multi-topic conversations <code>lmb.ChatMemory.threaded()</code> Smart threading connects related topics Performance critical <code>lmb.ChatMemory()</code> No additional LLM latency Complex discussions <code>lmb.ChatMemory.threaded()</code> Maintains conversation context across topics Real-time chat <code>lmb.ChatMemory()</code> Immediate responses Research/analysis <code>lmb.ChatMemory.threaded()</code> Can reference earlier parts of conversation"},{"location":"design/unified_chat_memory/#memory-in-different-bot-types","title":"Memory in Different Bot Types","text":"<pre><code>import llamabot as lmb\n\n# Linear memory for simple conversations (fast)\nlinear_memory = lmb.ChatMemory()  # Default linear\nsimple_bot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    memory=linear_memory\n)\n\n# Graph memory for complex conversations with threading (smart)\ngraph_memory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\nquery_bot = lmb.QueryBot(\n    system_prompt=\"You are a helpful assistant.\",\n    memory=graph_memory\n)\n\n# Custom memory for specific needs (advanced)\ncustom_memory = lmb.ChatMemory(\n    node_selector=lmb.LLMNodeSelector(model=\"gpt-4o-mini\"),\n    summarizer=None  # No summarization for performance\n)\nstructured_bot = StructuredBot(\n    system_prompt=\"You are a helpful assistant.\",\n    pydantic_model=SomeModel,\n    memory=custom_memory\n)\n</code></pre>"},{"location":"design/unified_chat_memory/#memory-reset-and-state-management","title":"Memory Reset and State Management","text":"<pre><code>from llamabot.bot.simplebot import SimpleBot\nfrom llamabot.components.chat_memory import ChatMemory\n\n# Create bot with memory\nmemory = ChatMemory()  # Default linear\nbot = SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\",\n    memory=memory\n)\n\n# Have a conversation\nbot(\"Hello!\")\nbot(\"How are you?\")\n\n# Reset memory for new conversation\nmemory.reset()\n\n# Bot no longer remembers previous conversation\nresponse = bot(\"What did we just talk about?\")  # Bot won't remember\n</code></pre>"},{"location":"design/unified_chat_memory/#core-operations","title":"Core Operations","text":""},{"location":"design/unified_chat_memory/#storage-operations","title":"Storage Operations","text":""},{"location":"design/unified_chat_memory/#appendhuman_message-basemessage-assistant_message-basemessage","title":"append(human_message: BaseMessage, assistant_message: BaseMessage)","text":"<ul> <li>Adds both messages to the graph</li> <li>Creates parent-child relationship between messages</li> <li>Uses node selector to determine threading (linear vs graph mode)</li> <li>Prevents cycles and orphaned nodes</li> </ul>"},{"location":"design/unified_chat_memory/#reset","title":"reset()","text":"<ul> <li>Clears all stored messages</li> <li>Resets graph to empty state</li> </ul>"},{"location":"design/unified_chat_memory/#retrieval-operations","title":"Retrieval Operations","text":""},{"location":"design/unified_chat_memory/#retrievequery-str-n_results-int-10-context_depth-int-5-listbasemessage","title":"retrieve(query: str, n_results: int = 10, context_depth: int = 5) -&gt; List[BaseMessage]","text":"<ul> <li>Smart retrieval that adapts based on memory configuration</li> <li>Linear memory: Ignores query, returns recent messages (fast)</li> <li>Graph memory: Uses semantic search with BM25 or similar algorithm, then traverses up thread paths</li> <li>n_results: Number of relevant messages to find via semantic search</li> <li>context_depth: Number of nodes to traverse up each thread path for context</li> <li>Returns most relevant messages with their conversation context</li> <li>Works like existing docstore implementations</li> </ul> <p>Context Depth Example:</p> <pre><code>H1: \"Let's talk about Python\" (root)\n\u2514\u2500\u2500 A1: \"Python is great for data science\"\n    \u251c\u2500\u2500 H2: \"What about machine learning?\"\n    \u2502   \u2514\u2500\u2500 A2: \"ML libraries include scikit-learn\"\n    \u2514\u2500\u2500 H3: \"Tell me about databases\"\n        \u2514\u2500\u2500 A3: \"SQL databases are...\"\n\n# Thread path for A2: A2 \u2190 H2 \u2190 A1 \u2190 H1 (root)\nmemory.retrieve(query=\"machine learning\", n_results=1, context_depth=2)\n# Returns: [A2, H2, A1] (relevant message + 2 messages up thread path)\n</code></pre>"},{"location":"design/unified_chat_memory/#threading-model","title":"Threading Model","text":""},{"location":"design/unified_chat_memory/#conversation-structure","title":"Conversation Structure","text":"<p>The graph memory uses a tree structure where all nodes are connected in a single conversation tree:</p> <pre><code>H1: \"Let's talk about Python\" (root - first human message)\n\u2514\u2500\u2500 A1: \"Python is great for data science\"\n    \u251c\u2500\u2500 H2: \"What about machine learning?\"\n    \u2502   \u2514\u2500\u2500 A2: \"ML libraries include scikit-learn\"\n    \u2514\u2500\u2500 H3: \"Tell me about databases\"\n        \u2514\u2500\u2500 A3: \"SQL databases are...\"\n\n# Thread paths:\n# Thread 1: H1 \u2192 A1 \u2192 H2 \u2192 A2\n# Thread 2: H1 \u2192 A1 \u2192 H3 \u2192 A3\n</code></pre>"},{"location":"design/unified_chat_memory/#threading-rules","title":"Threading Rules","text":"<ol> <li>Root Node: The first human message becomes the root of the conversation tree</li> <li>Branching Rules:</li> <li>Human messages can only branch from assistant messages</li> <li>Assistant messages can only branch from human messages</li> <li>This enforces the conversation turn structure: Human \u2192 Assistant \u2192 Human \u2192 Assistant...</li> <li>Thread Definition: Threads are paths from root to leaf nodes (active conversation endpoints)</li> <li>Leaf nodes: Nodes with no out-edges (no children)</li> <li>Root node: First human message with no parent (parent_id = None)</li> </ol>"},{"location":"design/unified_chat_memory/#node-selection-strategies","title":"Node Selection Strategies","text":""},{"location":"design/unified_chat_memory/#linearnodeselector","title":"LinearNodeSelector","text":"<ul> <li>Always selects the leaf assistant node (node with no out-edges that is an assistant message)</li> <li>Creates linear conversation flow</li> <li>No LLM calls required</li> <li>Used in linear mode</li> <li>Constraint: Can only select assistant messages as parents for human messages</li> </ul>"},{"location":"design/unified_chat_memory/#llmnodeselector","title":"LLMNodeSelector","text":"<ul> <li>Uses LLM to intelligently select which assistant message to branch from</li> <li>Considers message content and conversation context</li> <li>Supports retry logic with feedback</li> <li>Used in graph mode</li> <li>Constraint: Can only select assistant messages as parents for human messages</li> <li>First message handling: If no assistant messages exist, creates root node (parent_id = None)</li> </ul>"},{"location":"design/unified_chat_memory/#node-selection-logic","title":"Node Selection Logic","text":"<ul> <li>First message: If no assistant nodes exist, message becomes root (<code>parent_id = None</code>)</li> <li>Subsequent messages: LLM selects best assistant message as parent</li> <li>No fallbacks: LLM selection should be reliable; if it fails, message becomes root</li> </ul>"},{"location":"design/unified_chat_memory/#usage-patterns","title":"Usage Patterns","text":""},{"location":"design/unified_chat_memory/#memory-usage-in-chat-loop","title":"Memory Usage in Chat Loop","text":"<p>The following example shows how memory is used inside a bot's <code>__call__</code> method. This is the standard pattern that all memory types should follow:</p> <pre><code>def __call__(self, *human_messages):\n    # 1. Process incoming messages\n    processed_messages = to_basemessage(human_messages)\n\n    # 2. RETRIEVAL: Get relevant context from memory\n    memory_messages = []\n    if self.memory:\n        # Memory system handles the complexity internally\n        memory_messages = self.memory.retrieve(\n            query=f\"From our conversation history, give me the most relevant information to the query, {[p.content for p in processed_messages]}\",\n            n_results=10,\n            context_depth=5\n        )\n\n    # 3. Build message list with context\n    messages = [self.system_prompt] + memory_messages + processed_messages\n\n    # 4. Generate response\n    response_message = AIMessage(content=content, tool_calls=tool_calls)\n\n    # 5. STORAGE: Save conversation turn to memory\n    if self.memory:\n        self.memory.append(processed_messages[-1], response_message)\n\n    return response_message\n</code></pre> <p>Key Points:</p> <ul> <li>Retrieval happens before response generation to provide context</li> <li>Storage happens after response generation to save the conversation turn</li> <li>Memory is self-aware - the <code>retrieve()</code> method automatically chooses the best strategy based on memory type</li> <li>No mode checking needed - bot implementers don't need to know about memory internals</li> <li>Performance optimization is automatic - linear memory skips expensive semantic search</li> <li>Memory is optional - bot works without memory, just with less context</li> <li>Unified API - same method calls work for all memory types</li> </ul> <p>Note: This example shows the core memory operations with logging stripped out for clarity. Real implementations should include appropriate logging and error handling.</p>"},{"location":"design/unified_chat_memory/#export-and-visualization","title":"Export and Visualization","text":""},{"location":"design/unified_chat_memory/#mermaid-export","title":"Mermaid Export","text":"<pre><code># Export conversation graph\nmermaid_diagram = memory.to_mermaid()\n\n# Filter by role for cleaner visualization\nassistant_nodes = [n for n in memory.graph.nodes()\n                  if memory.graph.nodes[n]['node'].message.role == 'assistant']\n</code></pre>"},{"location":"design/unified_chat_memory/#implementation-details","title":"Implementation Details","text":""},{"location":"design/unified_chat_memory/#modular-architecture","title":"Modular Architecture","text":"<p>The implementation uses a modular approach to keep the main <code>ChatMemory</code> class clean and focused:</p> <pre><code>llamabot/components/chat_memory/\n\u251c\u2500\u2500 __init__.py        # Exports main classes and functions\n\u251c\u2500\u2500 memory.py          # Main ChatMemory class\n\u251c\u2500\u2500 retrieval.py       # Retrieval functions\n\u251c\u2500\u2500 storage.py         # Storage functions\n\u251c\u2500\u2500 visualization.py   # Visualization functions\n\u2514\u2500\u2500 selectors.py       # Node selection strategies\n</code></pre> <p>Module Exports in <code>__init__.py</code>:</p> <pre><code># Main classes\nfrom .memory import ChatMemory\nfrom .selectors import LinearNodeSelector, LLMNodeSelector\nfrom .storage import append_linear, append_with_threading\nfrom .retrieval import get_recent_messages, semantic_search_with_context\nfrom .visualization import to_mermaid\n\n__all__ = [\n    \"ChatMemory\",\n    \"LinearNodeSelector\",\n    \"LLMNodeSelector\",\n    \"append_linear\",\n    \"append_with_threading\",\n    \"get_recent_messages\",\n    \"semantic_search_with_context\",\n    \"to_mermaid\"\n]\n</code></pre> <p>Test Structure Should Mirror Components:</p> <pre><code>tests/components/chat_memory/\n\u251c\u2500\u2500 test_memory.py     # Test main ChatMemory class\n\u251c\u2500\u2500 test_retrieval.py  # Test retrieval functions\n\u251c\u2500\u2500 test_storage.py    # Test storage functions\n\u251c\u2500\u2500 test_visualization.py  # Test visualization functions\n\u2514\u2500\u2500 test_selectors.py  # Test node selection strategies\n</code></pre>"},{"location":"design/unified_chat_memory/#main-class-clean-and-focused","title":"Main Class (Clean and Focused)","text":"<pre><code>class ChatMemory:\n    def __init__(self,\n                 node_selector: Optional[NodeSelector] = None,\n                 summarizer: Optional[Summarizer] = None,\n                 context_depth: int = 5):\n        \"\"\"Initialize chat memory with configuration.\n\n        :param node_selector: Strategy for selecting parent nodes (None = LinearNodeSelector)\n        :param summarizer: Optional summarization strategy (None = no summarization)\n        :param context_depth: Default depth for context retrieval\n        \"\"\"\n        # Initialize NetworkX graph for storage\n        self.graph = nx.DiGraph()\n\n        # Set node selector (linear by default, LLM-based if provided)\n        self.node_selector = node_selector or LinearNodeSelector()\n\n        # Set optional summarizer\n        self.summarizer = summarizer\n\n        # Validate and store context depth\n        if context_depth &lt; 0:\n            raise ValueError(\"context_depth must be non-negative\")\n        self.context_depth = context_depth\n\n        # Track next node ID for auto-incrementing\n        self._next_node_id = 1\n\n    def retrieve(self, query: str, n_results: int = 10, context_depth: int = None) -&gt; List[BaseMessage]:\n        \"\"\"Smart retrieval that adapts based on memory configuration.\"\"\"\n        context_depth = context_depth or self.context_depth\n\n        if isinstance(self.node_selector, LinearNodeSelector):\n            return get_recent_messages(self.graph, n_results)\n        else:\n            return semantic_search_with_context(self.graph, query, n_results, context_depth)\n\n    def append(self, human_message: BaseMessage, assistant_message: BaseMessage):\n        \"\"\"Add conversation turn to memory.\"\"\"\n        if isinstance(self.node_selector, LinearNodeSelector):\n            append_linear(self.graph, human_message, assistant_message, self._next_node_id)\n            self._next_node_id += 2  # Increment for both messages\n        else:\n            append_with_threading(self.graph, human_message, assistant_message, self.node_selector, self._next_node_id)\n            self._next_node_id += 2  # Increment for both messages\n</code></pre>"},{"location":"design/unified_chat_memory/#separate-functions-implementation-details","title":"Separate Functions (Implementation Details)","text":"<pre><code># retrieval.py\ndef get_recent_messages(graph: nx.DiGraph, n_results: int) -&gt; List[BaseMessage]:\n    \"\"\"Get the most recent N messages from linear memory.\"\"\"\n\ndef semantic_search_with_context(graph: nx.DiGraph, query: str, n_results: int, context_depth: int) -&gt; List[BaseMessage]:\n    \"\"\"Find relevant nodes via semantic search, then traverse up their thread paths for context.\"\"\"\n\ndef traverse_thread_path(graph: nx.DiGraph, node: int, depth: int) -&gt; List[BaseMessage]:\n    \"\"\"Traverse up a conversation thread path from a given node.\"\"\"\n\n# storage.py\ndef append_linear(graph: nx.DiGraph, human_message: BaseMessage, assistant_message: BaseMessage, next_node_id: int):\n    \"\"\"Append messages to linear memory.\"\"\"\n    # Create human node\n    human_node = ConversationNode(\n        id=next_node_id,\n        message=human_message,\n        parent_id=find_leaf_assistant_node(graph) if graph.nodes() else None\n    )\n    graph.add_node(next_node_id, node=human_node)\n\n    # Create assistant node\n    assistant_node = ConversationNode(\n        id=next_node_id + 1,\n        message=assistant_message,\n        parent_id=next_node_id\n    )\n    graph.add_node(next_node_id + 1, node=assistant_node)\n\n    # Add edges\n    if human_node.parent_id:\n        graph.add_edge(human_node.parent_id, next_node_id)\n    graph.add_edge(next_node_id, next_node_id + 1)\n\ndef append_with_threading(graph: nx.DiGraph, human_message: BaseMessage, assistant_message: BaseMessage, node_selector, next_node_id: int):\n    \"\"\"Append messages with intelligent threading following conversation turn structure.\"\"\"\n    # Use node selector to find best parent for human message\n    parent_id = node_selector.select_parent(graph, human_message)\n\n    # Create human node\n    human_node = ConversationNode(\n        id=next_node_id,\n        message=human_message,\n        parent_id=parent_id\n    )\n    graph.add_node(next_node_id, node=human_node)\n\n    # Create assistant node\n    assistant_node = ConversationNode(\n        id=next_node_id + 1,\n        message=assistant_message,\n        parent_id=next_node_id\n    )\n    graph.add_node(next_node_id + 1, node=assistant_node)\n\n    # Add edges\n    if parent_id:\n        graph.add_edge(parent_id, next_node_id)\n    graph.add_edge(next_node_id, next_node_id + 1)\n\n# visualization.py\ndef to_mermaid(graph: nx.DiGraph, **kwargs) -&gt; str:\n    \"\"\"Convert graph to Mermaid diagram.\"\"\"\n</code></pre> <p>Benefits:</p> <ul> <li>Cleaner main class - focuses on high-level API</li> <li>Easier testing - can test functions independently</li> <li>Better separation of concerns - each function has one job</li> <li>More modular - functions can be reused or swapped</li> <li>Easier to understand - main class shows the \"what\", functions show the \"how\"</li> </ul> <p>Testing Benefits:</p> <ul> <li>Unit tests for each function in isolation</li> <li>Integration tests for the main ChatMemory class</li> <li>Mock testing of LLM components without real API calls</li> <li>Test coverage for each component independently</li> <li>Regression testing when modifying individual functions</li> </ul> <p>Import Benefits:</p> <ul> <li>Clean imports: <code>from llamabot.components.chat_memory import ChatMemory</code></li> <li>Function access: <code>from llamabot.components.chat_memory import append_linear</code></li> <li>Selector access: <code>from llamabot.components.chat_memory import LLMNodeSelector</code></li> <li>Top-level exports: All main functionality available from module root</li> </ul>"},{"location":"design/unified_chat_memory/#networkx-backend","title":"NetworkX Backend","text":"<ul> <li>Direct use of NetworkX DiGraph for storage</li> <li>No abstraction layer needed</li> <li>Leverages NetworkX algorithms for graph operations</li> <li>Efficient for small to medium conversation graphs</li> </ul>"},{"location":"design/unified_chat_memory/#implementation-details_1","title":"Implementation Details","text":""},{"location":"design/unified_chat_memory/#auto-incremented-ids","title":"Auto-Incremented IDs","text":"<ul> <li>Node IDs start at 1 and increment for each new node</li> <li>Provides natural chronological ordering</li> <li>Simple integer-based identification</li> <li>No UUID complexity or collision concerns</li> </ul>"},{"location":"design/unified_chat_memory/#networkx-graph-storage","title":"NetworkX Graph Storage","text":"<ul> <li>Each node stores a <code>ConversationNode</code> object as node data</li> <li>Node ID is the NetworkX node identifier</li> <li>Edges represent parent-child relationships</li> <li>Graph maintains conversation tree structure</li> </ul>"},{"location":"design/unified_chat_memory/#node-selection-process","title":"Node Selection Process","text":"<ol> <li>Linear Mode: Find leaf assistant node (no out-edges, role=\"assistant\")</li> <li>Graph Mode:</li> <li>Get all assistant nodes as candidates</li> <li>Use LLM to select best parent based on message content</li> <li>Validate selection is an assistant node</li> <li>If no candidates exist, message becomes root</li> </ol>"},{"location":"design/unified_chat_memory/#error-handling","title":"Error Handling","text":"<p>The system uses actionable error handling - only raising errors for issues that humans can actually fix:</p>"},{"location":"design/unified_chat_memory/#actionable-errors-user-can-fix","title":"Actionable Errors (User Can Fix)","text":"<ul> <li>Configuration errors: Invalid parameters like negative <code>context_depth</code></li> <li>File system errors: Permission denied, disk full, invalid file paths</li> <li>Input validation: Wrong message types, empty message content</li> </ul>"},{"location":"design/unified_chat_memory/#graceful-handling-no-errors","title":"Graceful Handling (No Errors)","text":"<ul> <li>Empty memory: Returns empty list instead of error</li> <li>LLM selection failures: Falls back to most recent valid node</li> <li>Summarization failures: Continues without summary</li> <li>Graph corruption: Clear error message with reset instruction</li> </ul>"},{"location":"design/unified_chat_memory/#error-message-examples","title":"Error Message Examples","text":"<p>Configuration Error (Actionable):</p> <pre><code># Validated at instantiation\nif context_depth &lt; 0:\n    raise ValueError(\"context_depth must be non-negative\")\n</code></pre> <p>File System Error (Actionable):</p> <pre><code>if \"Permission denied\" in str(e):\n    raise PersistenceError(\n        f\"Cannot save to {file_path}. Check file permissions or choose a different location.\"\n    )\nelif \"No space left\" in str(e):\n    raise PersistenceError(\n        f\"Disk is full. Free up space or choose a different location.\"\n    )\n</code></pre> <p>Graph Corruption (Actionable):</p> <pre><code>raise InvalidGraphStateError(\n    \"Conversation graph has become corrupted. \"\n    \"This can happen if the same message was processed multiple times. \"\n    \"Use memory.reset() to clear the conversation and start fresh.\"\n)\n</code></pre> <p>Graceful Handling Examples:</p> <pre><code># Empty memory - no error, just empty result\nif not graph.nodes():\n    return []\n\n# LLM failure - fallback to most recent node\nif llm_response not in valid_candidates:\n    return valid_candidates[-1] if valid_candidates else None\n\n# Summarization failure - continue without summary\ntry:\n    summary = summarizer.summarize(message)\nexcept Exception:\n    summary = None\n</code></pre>"},{"location":"design/unified_chat_memory/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Optional summarization for linear mode</li> <li>Lazy loading of summaries when needed</li> <li>Efficient graph traversal for retrieval</li> <li>Memory-efficient storage of large conversations</li> </ul>"},{"location":"design/unified_chat_memory/#benefits","title":"Benefits","text":"<ol> <li>Simplified API: Single class for all memory operations</li> <li>Better Performance: Optional summarization reduces LLM calls</li> <li>Clearer Separation: Storage and retrieval are distinct concerns</li> <li>Easier Testing: Smaller, focused components</li> <li>Future Extensibility: Pluggable node selectors and retrieval strategies</li> <li>Type Safety: Clear interfaces and error handling</li> </ol>"},{"location":"design/unified_chat_memory/#persistence-design","title":"Persistence Design","text":""},{"location":"design/unified_chat_memory/#storage-format","title":"Storage Format","text":"<p>The conversation memory uses a JSON-based format for persistence that is easily parseable and human-readable:</p> <pre><code>{\n  \"version\": \"1.0\",\n  \"metadata\": {\n    \"created_at\": \"2024-01-15T10:30:00Z\",\n    \"last_modified\": \"2024-01-15T14:45:00Z\",\n    \"mode\": \"graph\",\n    \"total_messages\": 12\n  },\n  \"nodes\": [\n    {\n      \"id\": 1,\n      \"role\": \"user\",\n      \"content\": \"Let's talk about Python\",\n      \"timestamp\": \"2024-01-15T10:30:00Z\",\n      \"summary\": {\n        \"title\": \"Python Discussion Start\",\n        \"summary\": \"User wants to discuss Python programming.\"\n      },\n      \"parent_id\": null\n    },\n    {\n      \"id\": 2,\n      \"role\": \"assistant\",\n      \"content\": \"Python is great for data science\",\n      \"timestamp\": \"2024-01-15T10:30:05Z\",\n      \"summary\": {\n        \"title\": \"Python Benefits\",\n        \"summary\": \"Assistant explains Python's benefits for data science.\"\n      },\n      \"parent_id\": 1\n    }\n  ],\n  \"edges\": [\n    {\"from\": 1, \"to\": 2},\n    {\"from\": 2, \"to\": 3},\n    {\"from\": 2, \"to\": 5}\n  ]\n}\n</code></pre>"},{"location":"design/unified_chat_memory/#persistence-operations","title":"Persistence Operations","text":""},{"location":"design/unified_chat_memory/#savefile_path-str-none","title":"save(file_path: str) -&gt; None","text":"<ul> <li>Serializes conversation memory to JSON file</li> <li>Includes metadata for versioning and tracking</li> <li>Preserves all node data and edge relationships</li> <li>Handles BaseMessage serialization</li> </ul>"},{"location":"design/unified_chat_memory/#loadfile_path-str-chatmemory","title":"load(file_path: str) -&gt; ChatMemory","text":"<ul> <li>Deserializes JSON file to recreate memory</li> <li>Validates graph structure integrity</li> <li>Reconstructs NetworkX graph from JSON data</li> <li>Handles version compatibility</li> </ul>"},{"location":"design/unified_chat_memory/#exportformat-str-json-str","title":"export(format: str = \"json\") -&gt; str","text":"<ul> <li>Exports conversation in various formats</li> <li>JSON: Full conversation with metadata</li> <li>JSONL: OpenAI-compatible format for fine-tuning</li> <li>Mermaid: Visualization format</li> <li>Plain text: Simple conversation transcript</li> </ul>"},{"location":"design/unified_chat_memory/#implementation-details_2","title":"Implementation Details","text":""},{"location":"design/unified_chat_memory/#json-serialization-strategy","title":"JSON Serialization Strategy","text":"<pre><code>def to_json(self) -&gt; dict:\n    \"\"\"Convert conversation memory to JSON-serializable dict.\"\"\"\n    return {\n        \"version\": \"1.0\",\n        \"metadata\": {\n            \"created_at\": self.created_at.isoformat(),\n            \"last_modified\": datetime.now().isoformat(),\n            \"mode\": self.mode,\n            \"total_messages\": len(self.graph.nodes())\n        },\n        \"nodes\": [\n            {\n                \"id\": node_id,\n                \"role\": node_data[\"node\"].message.role,\n                \"content\": node_data[\"node\"].message.content,\n                \"timestamp\": node_data[\"node\"].timestamp.isoformat(),\n                \"summary\": node_data[\"node\"].summary.dict() if node_data[\"node\"].summary else None,\n                \"parent_id\": node_data[\"node\"].parent_id\n            }\n            for node_id, node_data in self.graph.nodes(data=True)\n        ],\n        \"edges\": [\n            {\"from\": u, \"to\": v}\n            for u, v in self.graph.edges()\n        ]\n    }\n</code></pre>"},{"location":"design/unified_chat_memory/#graph-reconstruction","title":"Graph Reconstruction","text":"<pre><code>def from_json(data: dict) -&gt; ChatMemory:\n    \"\"\"Reconstruct conversation memory from JSON data.\"\"\"\n    memory = ChatMemory(mode=data[\"metadata\"][\"mode\"])\n\n    # Reconstruct nodes\n    for node_data in data[\"nodes\"]:\n        message = create_message(node_data[\"role\"], node_data[\"content\"])\n        node = ConversationNode(\n            id=node_data[\"id\"],\n            message=message,\n            summary=MessageSummary(**node_data[\"summary\"]) if node_data[\"summary\"] else None,\n            parent_id=node_data[\"parent_id\"],\n            timestamp=datetime.fromisoformat(node_data[\"timestamp\"])\n        )\n        memory.graph.add_node(node_data[\"id\"], node=node)\n\n    # Reconstruct edges\n    for edge in data[\"edges\"]:\n        memory.graph.add_edge(edge[\"from\"], edge[\"to\"])\n\n    return memory\n</code></pre>"},{"location":"design/unified_chat_memory/#benefits-of-json-format","title":"Benefits of JSON Format","text":"<ol> <li>Human-readable: Easy to inspect and debug</li> <li>Version control friendly: Diff-able and merge-able</li> <li>Language agnostic: Can be parsed by any language</li> <li>Extensible: Easy to add new fields</li> <li>Standard format: Well-supported across tools</li> <li>No security risks: Unlike pickle, no code execution</li> </ol>"},{"location":"design/unified_chat_memory/#file-naming-convention","title":"File Naming Convention","text":"<pre><code>conversations/\n\u251c\u2500\u2500 session_2024-01-15_10-30-00.json\n\u251c\u2500\u2500 session_2024-01-15_14-45-00.json\n\u2514\u2500\u2500 backup_2024-01-15_18-00-00.json\n</code></pre>"},{"location":"design/unified_chat_memory/#open-questions-and-future-enhancements","title":"Open Questions and Future Enhancements","text":""},{"location":"design/unified_chat_memory/#concurrency-handling","title":"Concurrency Handling","text":"<ul> <li>How should multiple threads/processes access the same memory file?</li> <li>Should we use file locking or database backend for concurrent access?</li> <li>What happens if two processes try to append simultaneously?</li> </ul>"},{"location":"design/unified_chat_memory/#advanced-retrieval-strategies","title":"Advanced Retrieval Strategies","text":"<ul> <li>Semantic search across message content</li> <li>Time-based retrieval (messages from last hour/day)</li> <li>User-specific retrieval (only messages from specific user)</li> <li>Context-aware retrieval (messages related to current topic)</li> </ul>"},{"location":"design/unified_chat_memory/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Lazy loading of large conversation histories</li> <li>Caching frequently accessed message paths</li> <li>Compression for long conversations</li> <li>Incremental graph updates</li> </ul>"},{"location":"design/unified_chat_memory/#integration-with-external-systems","title":"Integration with External Systems","text":"<ul> <li>Export to chat platforms (Slack, Discord, etc.)</li> <li>Integration with vector databases for semantic search</li> <li>Webhook support for real-time updates</li> <li>API endpoints for external access</li> </ul>"},{"location":"design/unified_chat_memory/#migration-strategy","title":"Migration Strategy","text":""},{"location":"design/unified_chat_memory/#phase-1-deprecation-v0130","title":"Phase 1: Deprecation (v0.13.0)","text":"<ul> <li>Add deprecation warnings to existing <code>ChatMemory</code> class</li> <li>Document new <code>ChatMemory</code> API</li> <li>Update examples to use new API</li> <li>Ensure <code>ChatMemory</code> is top-level in <code>llamabot/__init__.py</code> \u2705</li> <li>Update tests to reflect modular component structure \u2705</li> </ul>"},{"location":"design/unified_chat_memory/#phase-2-transition-v0140","title":"Phase 2: Transition (v0.14.0)","text":"<ul> <li>Make <code>ChatMemory</code> the default</li> <li>Keep <code>ChatMemory</code> as alias with deprecation warning</li> <li>Update all internal usage</li> </ul>"},{"location":"design/unified_chat_memory/#phase-3-removal-v0150","title":"Phase 3: Removal (v0.15.0)","text":"<ul> <li>Remove <code>ChatMemory</code> class entirely</li> <li>Remove deprecated methods</li> <li>Clean up imports and references</li> </ul>"},{"location":"design/unified_chat_memory/#migration-guide","title":"Migration Guide","text":"<p>Old API:</p> <pre><code>from llamabot.components.chat_memory import ChatMemory\n\nmemory = ChatMemory()\nmemory.add_message(\"user\", \"Hello\")\nmessages = memory.get_messages()\n</code></pre> <p>New API:</p> <pre><code>from llamabot.components.chat_memory import ChatMemory\n\nmemory = ChatMemory()\nmemory.append(\"user\", \"Hello\")\nmessages = memory.retrieve()\n</code></pre> <p>Key Changes:</p> <ul> <li><code>ChatMemory</code> \u2192 <code>ChatMemory</code> (same name, new implementation)</li> <li><code>add_message()</code> \u2192 <code>append()</code></li> <li><code>get_messages()</code> \u2192 <code>retrieve()</code></li> <li>New threading support with <code>ChatMemory.threaded()</code></li> <li>New persistence methods: <code>save()</code>, <code>load()</code>, <code>export()</code></li> </ul> <p>Imports:</p> <pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n</code></pre>"},{"location":"design/unified_chat_memory/#conclusion","title":"Conclusion","text":"<p>This unified design addresses the core issues with the current implementation while maintaining the flexibility needed for different use cases. The separation of storage and retrieval concerns makes the system more maintainable and easier to understand, while the multiple API levels provide the right level of abstraction for different users.</p>"},{"location":"examples/anthropic_api/","title":"Test with Anthropic API (Claude Sonnet)","text":"<pre><code>import os\nfrom llamabot import SimpleBot\nfrom dotenv import load_dotenv\n</code></pre> <p>URL for Anthropic API key: https://console.anthropic.com/settings/keys</p> <p>If it's your first time using an Anthropic LLM, you will need to create an account and load funds (min $5).</p> <p>Simple calls to the API like the ones seen below cost about 1 cent per request</p> <p>Set up the API Key:</p> <p>Create a .env file (beware that your OS does not append another file extension). Explicitly:</p> <pre><code>export ANTHROPIC_API_KEY=\"YOUR KEY\"\n</code></pre> <p>Save this file to the same directory as this notebook</p> <p>We now load the .env file and store the API key as an environment variable</p> <pre><code>load_dotenv(\n    \".env\"\n)  # explicitly state that we are looking for .env in the same directory as notebook\n</code></pre> <pre><code>True\n</code></pre> <p>Setting up the SimpleBot. The API key will be automatically read from the environment variable</p> <pre><code>system_prompt = \"\"\"You are a bot that will respond to questions written by a human curious about using Anthropic's LLMs.\nWhen applicable, you should give an unbiased comparison between the various LLMs.\nThe post should be written in professional English and in first-person tone for the human.\n\"\"\"\nclaude = SimpleBot(\n    system_prompt=system_prompt,\n    stream_target=\"stdout\",  # this is the default!,\n    model_name=\"claude-3-5-sonnet-20240620\",\n)\n</code></pre> <p>Asking a question</p> <pre><code>first_question = claude(\"What types of tasks does claude 3.5 sonnet excel at\")\n</code></pre> <pre><code>As an AI assistant, I'll do my best to provide an informative and unbiased overview of Claude 3.5 Sonnet's capabilities:\n\n    Claude 3.5 Sonnet excels at a wide range of natural language processing tasks. Some key areas where it performs particularly well include:\n\n    1. Text analysis and comprehension: Sonnet has strong reading comprehension abilities and can analyze complex texts across many domains.\n\n    2. Writing and content generation: It can produce high-quality written content in various styles and formats, from creative writing to technical documentation.\n\n    3. Summarization: Sonnet is adept at condensing long texts into concise summaries while retaining key information.\n\n    4. Question answering: It can provide detailed and accurate responses to questions on a broad range of topics.\n\n    5. Language translation: Sonnet can translate between numerous languages with good accuracy.\n\n    6. Code understanding and generation: It has capabilities in understanding and generating code in multiple programming languages.\n\n    7. Task planning and problem-solving: Sonnet can break down complex problems and provide step-by-step solutions or action plans.\n\n    8. Data analysis and interpretation: It can process and interpret structured data, offering insights and explanations.\n\n    While Sonnet is highly capable, it's important to note that its performance can vary depending on the specific task and context. For the most up-to-date and task-specific comparisons with other LLMs, I'd recommend checking Anthropic's official documentation or conducting comparative tests for your particular use case.\n\n    Additionally, Claude 3.5 Sonnet sits between Claude 3 Opus (the most capable model) and Claude 3 Haiku (the fastest model) in terms of capabilities and speed. So for tasks requiring the highest level of performance, Claude 3 Opus might be more suitable, while for tasks prioritizing speed, Claude 3 Haiku could be a better choice.\n</code></pre> <pre><code>unsafe_question = claude(\n    \"Why is it not secure to copy API keys directly into an example Jupyter notebook\"\n)\n</code></pre> <pre><code>As an AI assistant, I'll explain why copying API keys directly into an example Jupyter notebook is not secure:\n\n    Copying API keys directly into a Jupyter notebook is generally considered insecure for several important reasons:\n\n    1. Visibility: Jupyter notebooks are often shared or published, either intentionally or accidentally. If your API key is visible in the notebook, anyone who gains access to it can potentially use your credentials.\n\n    2. Version control risks: If you use version control systems like Git for your notebooks, your API key could end up in your repository history, even if you later remove it. This makes it accessible to anyone with access to the repository.\n\n    3. Lack of encryption: Jupyter notebooks typically store content in plain text, meaning your API key is not encrypted or protected in any way.\n\n    4. Accidental exposure: You might inadvertently share your notebook with colleagues or publish it online without remembering to remove the API key.\n\n    5. Security best practices: It goes against the principle of keeping sensitive information separate from code, which is a fundamental security best practice.\n\n    Instead of directly copying API keys into notebooks, I would recommend using more secure methods such as:\n\n    1. Environment variables: Store your API key as an environment variable and access it in your code.\n    2. Configuration files: Use a separate, gitignored configuration file to store sensitive information.\n    3. Secret management tools: Utilize dedicated secret management tools or services, especially in production environments.\n    4. Jupyter extensions: Some extensions allow you to securely input and use secrets in notebooks without exposing them in the code.\n\n    By using these methods, you can work with API keys more securely while still leveraging the power and convenience of Jupyter notebooks. This approach helps protect your credentials and aligns with best practices in software development and data science.\n</code></pre>"},{"location":"examples/chatbot_nb/","title":"Chatbot nb","text":"<p>Let's see how to use the ChatBot class to enable you to chat with Mistral inside a Jupyter notebook.</p> <pre><code>from llamabot import ChatBot\n\ncode_tester = ChatBot(\n    \"\"\"\nYou are a Python quality assurance developer who delivers high quality unit tests for code.\nYou write tests using PyTest and not the built-in unittest library.\nWrite the tests using test functions and not using classes and class methods\nHere is the code to write tests against:\n\"\"\",\n    session_name=\"code-tested\",\n    model_name=\"mistral/mistral-medium\",\n    stream_target=\"stdout\",\n)\n</code></pre> <pre><code>code_tester(\n    '''\nclass ChatBot:\n    \"\"\"Chat Bot that is primed with a system prompt, accepts a human message.\n\n    Automatic chat memory management happens.\n\n    h/t Andrew Giessel/GPT4 for the idea.\n    \"\"\"\n\n    def __init__(self, system_prompt, temperature=0.0, model_name=\"gpt-4\"):\n        \"\"\"Initialize the ChatBot.\n\n        :param system_prompt: The system prompt to use.\n        :param temperature: The model temperature to use.\n            See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\n            for more information.\n        :param model_name: The name of the OpenAI model to use.\n        \"\"\"\n        self.model = ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            streaming=True,\n            verbose=True,\n            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n        )\n        self.chat_history = [\n            SystemMessage(content=\"Always return Markdown-compatible text.\"),\n            SystemMessage(content=system_prompt),\n        ]\n\n    def __call__(self, human_message) -&amp;gt; Response:\n        \"\"\"Call the ChatBot.\n\n        :param human_message: The human message to use.\n        :return: The response to the human message, primed by the system prompt.\n        \"\"\"\n        self.chat_history.append(HumanMessage(content=human_message))\n        response = self.model(self.chat_history)\n        self.chat_history.append(response)\n        return response\n'''\n)\n</code></pre> <pre>\n<code>Here are some tests for the ChatBot class using PyTest and test functions:\n<pre><code>import pytest\nfrom chatbot import ChatBot, SystemMessage, HumanMessage\nfrom openai_mock import ChatOpenAI\n\ndef test_chatbot_init():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    assert len(chatbot.chat_history) == 2\n    assert isinstance(chatbot.chat_history[0], SystemMessage)\n    assert isinstance(chatbot.chat_history[1], SystemMessage)\n    assert chatbot.chat_history[0].content == \"Always return Markdown-compatible text.\"\n    assert chatbot.chat_history[1].content == system_prompt\n\ndef test_chatbot_call():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    human_message = \"What is the weather like today?\"\n    response = chatbot(human_message)\n    assert len(chatbot.chat_history) == 4\n    assert isinstance(chatbot.chat_history[2], HumanMessage)\n    assert isinstance(chatbot.chat_history[3], response.__class__)\n    assert chatbot.chat_history[2].content == human_message\n\ndef test_chatbot_response():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt, model_name=\"gpt-3.5-turbo\")\n    human_message = \"What is the weather like today?\"\n    response = chatbot(human_message)\n    assert response.content.startswith(\"The weather today is\")\n\n@pytest.fixture\ndef chatbot():\n    system_prompt = \"You are a helpful assistant.\"\n    return ChatBot(system_prompt, model_name=\"gpt-3.5-turbo\")\n\ndef test_chatbot_multiple_calls(chatbot):\n    human_message1 = \"What is the weather like today?\"\n    human_message2 = \"What is the traffic like today?\"\n    chatbot(human_message1)\n    chatbot(human_message2)\n    assert len(chatbot.chat_history) == 6\n    assert chatbot.chat_history[2].content == human_message1\n    assert chatbot.chat_history[4].content == human_message2\n\ndef test_chatbot_temperature(chatbot):\n    human_message = \"Tell me a joke.\"\n    response1 = chatbot(human_message, temperature=0.0)\n    response2 = chatbot(human_message, temperature=1.0)\n    assert response1.content != response2.content\n\ndef test_chatbot_model_name(chatbot):\n    human_message = \"Tell me a joke.\"\n    chatbot2 = ChatBot(\"You are a funny comedian.\", model_name=\"davinci\")\n    response1 = chatbot(human_message)\n    response2 = chatbot2(human_message)\n    assert response1.content != response2.content\n</code></pre>\nNote: I used the `openai_mock` library to mock the `ChatOpenAI` class for testing purposes. You can replace it with the actual implementation of the `ChatOpenAI` class.\n\nAlso, I assumed that the `Response` class has a `content` attribute that contains the text response from the model. If it's different, please adjust the tests accordingly.</code>\n</pre> <pre>\n<code>AIMessage(content='Here are some tests for the ChatBot class using PyTest and test functions:\\n```\\nimport pytest\\nfrom chatbot import ChatBot, SystemMessage, HumanMessage\\nfrom openai_mock import ChatOpenAI\\n\\ndef test_chatbot_init():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt)\\n    assert len(chatbot.chat_history) == 2\\n    assert isinstance(chatbot.chat_history[0], SystemMessage)\\n    assert isinstance(chatbot.chat_history[1], SystemMessage)\\n    assert chatbot.chat_history[0].content == \"Always return Markdown-compatible text.\"\\n    assert chatbot.chat_history[1].content == system_prompt\\n\\ndef test_chatbot_call():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt)\\n    human_message = \"What is the weather like today?\"\\n    response = chatbot(human_message)\\n    assert len(chatbot.chat_history) == 4\\n    assert isinstance(chatbot.chat_history[2], HumanMessage)\\n    assert isinstance(chatbot.chat_history[3], response.__class__)\\n    assert chatbot.chat_history[2].content == human_message\\n\\ndef test_chatbot_response():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt, model_name=\"gpt-3.5-turbo\")\\n    human_message = \"What is the weather like today?\"\\n    response = chatbot(human_message)\\n    assert response.content.startswith(\"The weather today is\")\\n\\n@pytest.fixture\\ndef chatbot():\\n    system_prompt = \"You are a helpful assistant.\"\\n    return ChatBot(system_prompt, model_name=\"gpt-3.5-turbo\")\\n\\ndef test_chatbot_multiple_calls(chatbot):\\n    human_message1 = \"What is the weather like today?\"\\n    human_message2 = \"What is the traffic like today?\"\\n    chatbot(human_message1)\\n    chatbot(human_message2)\\n    assert len(chatbot.chat_history) == 6\\n    assert chatbot.chat_history[2].content == human_message1\\n    assert chatbot.chat_history[4].content == human_message2\\n\\ndef test_chatbot_temperature(chatbot):\\n    human_message = \"Tell me a joke.\"\\n    response1 = chatbot(human_message, temperature=0.0)\\n    response2 = chatbot(human_message, temperature=1.0)\\n    assert response1.content != response2.content\\n\\ndef test_chatbot_model_name(chatbot):\\n    human_message = \"Tell me a joke.\"\\n    chatbot2 = ChatBot(\"You are a funny comedian.\", model_name=\"davinci\")\\n    response1 = chatbot(human_message)\\n    response2 = chatbot2(human_message)\\n    assert response1.content != response2.content\\n```\\nNote: I used the `openai_mock` library to mock the `ChatOpenAI` class for testing purposes. You can replace it with the actual implementation of the `ChatOpenAI` class.\\n\\nAlso, I assumed that the `Response` class has a `content` attribute that contains the text response from the model. If it\\'s different, please adjust the tests accordingly.', role='assistant')</code>\n</pre> <p>As you can see, ChatBot keeps track of conversation memory/history automatically. We can even access any item in the conversation by looking at the conversation history.</p> <p>The <code>__repr__</code> of a chatbot will simply print out the entire history:</p> <pre><code>code_tester\n</code></pre> <pre>\n<code>[Human]\n\nclass ChatBot:\n    \"\"\"Chat Bot that is primed with a system prompt, accepts a human message.\n\n    Automatic chat memory management happens.\n\n    h/t Andrew Giessel/GPT4 for the idea.\n    \"\"\"\n\n    def __init__(self, system_prompt, temperature=0.0, model_name=\"gpt-4\"):\n        \"\"\"Initialize the ChatBot.\n\n        :param system_prompt: The system prompt to use.\n        :param temperature: The model temperature to use.\n            See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\n            for more information.\n        :param model_name: The name of the OpenAI model to use.\n        \"\"\"\n        self.model = ChatOpenAI(\n            model_name=model_name,\n            temperature=temperature,\n            streaming=True,\n            verbose=True,\n            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n        )\n        self.chat_history = [\n            SystemMessage(content=\"Always return Markdown-compatible text.\"),\n            SystemMessage(content=system_prompt),\n        ]\n\n    def __call__(self, human_message) -&gt; Response:\n        \"\"\"Call the ChatBot.\n\n        :param human_message: The human message to use.\n        :return: The response to the human message, primed by the system prompt.\n        \"\"\"\n        self.chat_history.append(HumanMessage(content=human_message))\n        response = self.model(self.chat_history)\n        self.chat_history.append(response)\n        return response\n\n\n[AI]\nHere are some tests for the ChatBot class using PyTest and test functions:\n<pre><code>import pytest\nfrom chatbot import ChatBot, SystemMessage, HumanMessage\nfrom openai_mock import ChatOpenAI\n\ndef test_chatbot_init():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    assert len(chatbot.chat_history) == 2\n    assert isinstance(chatbot.chat_history[0], SystemMessage)\n    assert isinstance(chatbot.chat_history[1], SystemMessage)\n    assert chatbot.chat_history[0].content == \"Always return Markdown-compatible text.\"\n    assert chatbot.chat_history[1].content == system_prompt\n\ndef test_chatbot_call():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    human_message = \"What is the weather like today?\"\n    response = chatbot(human_message)\n    assert len(chatbot.chat_history) == 4\n    assert isinstance(chatbot.chat_history[2], HumanMessage)\n    assert isinstance(chatbot.chat_history[3], response.__class__)\n    assert chatbot.chat_history[2].content == human_message\n\ndef test_chatbot_response():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt, model_name=\"gpt-3.5-turbo\")\n    human_message = \"What is the weather like today?\"\n    response = chatbot(human_message)\n    assert response.content.startswith(\"The weather today is\")\n\n@pytest.fixture\ndef chatbot():\n    system_prompt = \"You are a helpful assistant.\"\n    return ChatBot(system_prompt, model_name=\"gpt-3.5-turbo\")\n\ndef test_chatbot_multiple_calls(chatbot):\n    human_message1 = \"What is the weather like today?\"\n    human_message2 = \"What is the traffic like today?\"\n    chatbot(human_message1)\n    chatbot(human_message2)\n    assert len(chatbot.chat_history) == 6\n    assert chatbot.chat_history[2].content == human_message1\n    assert chatbot.chat_history[4].content == human_message2\n\ndef test_chatbot_temperature(chatbot):\n    human_message = \"Tell me a joke.\"\n    response1 = chatbot(human_message, temperature=0.0)\n    response2 = chatbot(human_message, temperature=1.0)\n    assert response1.content != response2.content\n\ndef test_chatbot_model_name(chatbot):\n    human_message = \"Tell me a joke.\"\n    chatbot2 = ChatBot(\"You are a funny comedian.\", model_name=\"davinci\")\n    response1 = chatbot(human_message)\n    response2 = chatbot2(human_message)\n    assert response1.content != response2.content\n</code></pre>\nNote: I used the `openai_mock` library to mock the `ChatOpenAI` class for testing purposes. You can replace it with the actual implementation of the `ChatOpenAI` class.\n\nAlso, I assumed that the `Response` class has a `content` attribute that contains the text response from the model. If it's different, please adjust the tests accordingly.\n</code>\n</pre> <p>On the other hand, accessing the <code>.messages</code> attribute of the ChatBot will give you access to all of the messages inside the conversation.</p> <pre><code>code_tester.messages\n</code></pre> <pre>\n<code>[HumanMessage(content='\\nclass ChatBot:\\n    \"\"\"Chat Bot that is primed with a system prompt, accepts a human message.\\n\\n    Automatic chat memory management happens.\\n\\n    h/t Andrew Giessel/GPT4 for the idea.\\n    \"\"\"\\n\\n    def __init__(self, system_prompt, temperature=0.0, model_name=\"gpt-4\"):\\n        \"\"\"Initialize the ChatBot.\\n\\n        :param system_prompt: The system prompt to use.\\n        :param temperature: The model temperature to use.\\n            See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\\n            for more information.\\n        :param model_name: The name of the OpenAI model to use.\\n        \"\"\"\\n        self.model = ChatOpenAI(\\n            model_name=model_name,\\n            temperature=temperature,\\n            streaming=True,\\n            verbose=True,\\n            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\\n        )\\n        self.chat_history = [\\n            SystemMessage(content=\"Always return Markdown-compatible text.\"),\\n            SystemMessage(content=system_prompt),\\n        ]\\n\\n    def __call__(self, human_message) -&gt; Response:\\n        \"\"\"Call the ChatBot.\\n\\n        :param human_message: The human message to use.\\n        :return: The response to the human message, primed by the system prompt.\\n        \"\"\"\\n        self.chat_history.append(HumanMessage(content=human_message))\\n        response = self.model(self.chat_history)\\n        self.chat_history.append(response)\\n        return response\\n\\n    def __repr__(self):\\n        \"\"\"Return a string representation of the ChatBot.\\n\\n        :return: A string representation of the ChatBot.\\n        \"\"\"\\n        representation = \"\"\\n\\n        for message in self.chat_history:\\n            if isinstance(message, SystemMessage):\\n                prefix = \"[System]\\n\"\\n            elif isinstance(message, HumanMessage):\\n                prefix = \"[Human]\\n\"\\n            elif isinstance(message, AIMessage):\\n                prefix = \"[AI]\\n\"\\n\\n            representation += f\"{prefix}{message.content}\" + \"\\n\\n\"\\n        return representation\\n\\n    def panel(self, show: bool = True):\\n        \"\"\"Create a Panel app that wraps a LlamaBot.\\n\\n        :param show: Whether to show the app.\\n            If False, we return the Panel app directly.\\n            If True, we call `.show()` on the app.\\n        :return: The Panel app, either showed or directly.\\n        \"\"\"\\n\\n        text_input = pn.widgets.TextAreaInput(placeholder=\"Start chatting...\")\\n        chat_history = pn.Column(*[])\\n        send_button = pn.widgets.Button(name=\"Send\", button_type=\"primary\")\\n\\n        def b(event):\\n            \"\"\"Button click handler.\\n\\n            :param event: The button click event.\\n            \"\"\"\\n            chat_messages = []\\n            for message in self.chat_history:\\n                if isinstance(message, SystemMessage):\\n                    pass\\n                elif isinstance(message, HumanMessage):\\n                    chat_markdown = pn.pane.Markdown(f\"Human: {message.content}\")\\n                    chat_messages.append(chat_markdown)\\n                elif isinstance(message, AIMessage):\\n                    chat_markdown = pn.pane.Markdown(f\"Bot: {message.content}\")\\n                    chat_messages.append(chat_markdown)\\n\\n            chat_messages.append(pn.pane.Markdown(f\"Human: {text_input.value}\"))\\n            bot_reply = pn.pane.Markdown(\"Bot: \")\\n            chat_messages.append(bot_reply)\\n            chat_history.objects = chat_messages\\n            markdown_handler = PanelMarkdownCallbackHandler(bot_reply)\\n            self.model.callback_manager.set_handler(markdown_handler)\\n            self(text_input.value)\\n            text_input.value = \"\"\\n\\n        send_button.on_click(b)\\n        input_pane = pn.Row(text_input, send_button)\\n        output_pane = pn.Column(chat_history, scroll=True, height=500)\\n\\n        main = pn.Row(input_pane, output_pane)\\n        app = pn.template.FastListTemplate(\\n            site=\"ChatBot\",\\n            title=\"ChatBot\",\\n            main=main,\\n            main_max_width=\"768px\",\\n        )\\n        if show:\\n            return app.show()\\n        return app\\n\\n', role='user'),\n AIMessage(content='Here are some tests for the ChatBot class using PyTest:\\n```python\\nimport pytest\\nfrom your_module import ChatBot, SystemMessage, HumanMessage\\n\\ndef test_init():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt)\\n    assert len(chatbot.chat_history) == 2\\n    assert isinstance(chatbot.chat_history[0], SystemMessage)\\n    assert isinstance(chatbot.chat_history[1], SystemMessage)\\n    assert chatbot.chat_history[0].content == \"Always return Markdown-compatible text.\"\\n    assert chatbot.chat_history[1].content == system_prompt\\n\\ndef test_call():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt)\\n    human_message = \"What\\'s the weather like today?\"\\n    response = chatbot(human_message)\\n    assert len(chatbot.chat_history) == 4\\n    assert isinstance(chatbot.chat_history[2], HumanMessage)\\n    assert isinstance(chatbot.chat_history[3], response.__class__)\\n    assert chatbot.chat_history[2].content == human_message\\n\\ndef test_repr():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt)\\n    human_message = \"What\\'s the weather like today?\"\\n    chatbot(human_message)\\n    expected_repr = (\\n        \"[System]\\\\n\"\\n        \"Always return Markdown-compatible text.\\\\n\\\\n\"\\n        \"[System]\\\\n\"\\n        \"You are a helpful assistant.\\\\n\\\\n\"\\n        \"[Human]\\\\n\"\\n        \"What\\'s the weather like today?\\\\n\\\\n\"\\n        \"[AI]\\\\n\"\\n    )\\n    assert repr(chatbot) == expected_repr\\n\\ndef test_panel():\\n    system_prompt = \"You are a helpful assistant.\"\\n    chatbot = ChatBot(system_prompt)\\n    app = chatbot.panel()\\n    assert isinstance(app, type(pn.template.FastListTemplate()))\\n```\\nNote that the `test_panel` function assumes that the `pn` module is available in the test environment. If it is not, you may need to install it or mock it out for testing purposes.\\n\\nAlso note that the `test_call` function assumes that the `response` object has a `__class__` attribute that can be used to check its type. If this is not the case, you may need to modify the test to use a different method of checking the type of the response object.\\n\\nFinally, note that these tests are not exhaustive and may not cover all possible edge cases or error conditions. You may want to add additional tests to ensure that the `ChatBot` class is working correctly in all scenarios.', role='assistant')]</code>\n</pre> <p>You can even access any arbitrary message.</p> <pre><code>print(code_tester.messages[-1].content)\n</code></pre> <pre>\n<code>Here are some tests for the ChatBot class using PyTest:\n<pre><code>import pytest\nfrom your_module import ChatBot, SystemMessage, HumanMessage\n\ndef test_init():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    assert len(chatbot.chat_history) == 2\n    assert isinstance(chatbot.chat_history[0], SystemMessage)\n    assert isinstance(chatbot.chat_history[1], SystemMessage)\n    assert chatbot.chat_history[0].content == \"Always return Markdown-compatible text.\"\n    assert chatbot.chat_history[1].content == system_prompt\n\ndef test_call():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    human_message = \"What's the weather like today?\"\n    response = chatbot(human_message)\n    assert len(chatbot.chat_history) == 4\n    assert isinstance(chatbot.chat_history[2], HumanMessage)\n    assert isinstance(chatbot.chat_history[3], response.__class__)\n    assert chatbot.chat_history[2].content == human_message\n\ndef test_repr():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    human_message = \"What's the weather like today?\"\n    chatbot(human_message)\n    expected_repr = (\n        \"[System]\\n\"\n        \"Always return Markdown-compatible text.\\n\\n\"\n        \"[System]\\n\"\n        \"You are a helpful assistant.\\n\\n\"\n        \"[Human]\\n\"\n        \"What's the weather like today?\\n\\n\"\n        \"[AI]\\n\"\n    )\n    assert repr(chatbot) == expected_repr\n\ndef test_panel():\n    system_prompt = \"You are a helpful assistant.\"\n    chatbot = ChatBot(system_prompt)\n    app = chatbot.panel()\n    assert isinstance(app, type(pn.template.FastListTemplate()))\n</code></pre>\nNote that the `test_panel` function assumes that the `pn` module is available in the test environment. If it is not, you may need to install it or mock it out for testing purposes.\n\nAlso note that the `test_call` function assumes that the `response` object has a `__class__` attribute that can be used to check its type. If this is not the case, you may need to modify the test to use a different method of checking the type of the response object.\n\nFinally, note that these tests are not exhaustive and may not cover all possible edge cases or error conditions. You may want to add additional tests to ensure that the `ChatBot` class is working correctly in all scenarios.\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"examples/chatbot_nb/#chatbots-in-a-jupyter-notebook","title":"ChatBots in a Jupyter Notebook","text":""},{"location":"examples/docstring_checker/","title":"Docstring checker","text":"<pre><code>from pydantic import BaseModel, Field\n\n\nclass DocstringDescribesFunction(BaseModel):\n    docs_match_source: bool = Field(\n        default=False,\n        description=\"Whether or not the docstring matches the function source.\",\n    )\n    reasons: list[str] = Field(\n        default_factory=list,\n        description=\"Reasons why the docstring doesn't match the function source.\",\n    )\n</code></pre> <pre><code>from llamabot import prompt\n\n\n@prompt\ndef docstringbot_sysprompt() -&amp;gt; str:\n    \"\"\"You are an expert at documenting functions.\n\n    You will be given a docstring and a function source.\n    Your job is to determine if the docstring matches the function source.\n\n    If it does match, respond with no reasons and respond with \"True\".\n\n    If it doesn't match,\n    respond with a list of reasons why the docstring doesn't match the function source.\n    Be specific about the reasons, such as:\n\n    - \"The docstring is mismatched with the function. The function does &lt;something&gt;,\n      but the docstring says &lt;something_else&gt;.\"\n    - \"The docstring is completely missing.\"\n    \"\"\"\n</code></pre> <p>Next up, let's create the StructuredBot. Upon initializing, we provide the system prompt and the Pydantic model that it needs to reference.</p> <pre><code>from llamabot import StructuredBot\n\ndocstringbot = StructuredBot(\n    docstringbot_sysprompt(), pydantic_model=DocstringDescribesFunction\n)\n</code></pre> <pre><code>def fibbonacci(n: int) -&amp;gt; int:\n    \"\"\"Return the nth Fibonacci number.\n\n    Mathematically, the nth Fibonnaci number is defined as\n    the sum of the (n-1)th and (n-2)th Fibonacci numbers.\n\n    As such, this is what is returned.\n\n    :param n: The position of the Fibonacci number to return.\n    \"\"\"\n    if n &amp;lt;= 0:\n        raise ValueError(\"n must be a positive integer.\")\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return fibbonacci(n - 1) + fibbonacci(n - 2)\n</code></pre> <pre><code>from inspect import getsource\n\nsource_code = getsource(fibbonacci)\ndocstringbot(source_code)\n</code></pre> <p>Let's try now an example where the docstring is completely missing.</p> <pre><code>def fibbonacci(n: int) -&amp;gt; int:\n    if n &amp;lt;= 0:\n        raise ValueError(\"n must be a positive integer.\")\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return fibbonacci(n - 1) + fibbonacci(n - 2)\n\n\nsource_code = getsource(fibbonacci)\ndocstringbot(source_code)\n</code></pre> <p>And now let's try an example where the docstring doesn't match the function source.</p> <pre><code>def fibbonacci(n: int) -&amp;gt; int:\n    \"\"\"This function bakes a cake of the Fibonacci sequence.\"\"\"\n    if n &amp;lt;= 0:\n        raise ValueError(\"n must be a positive integer.\")\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return fibbonacci(n - 1) + fibbonacci(n - 2)\n\n\nsource_code = getsource(fibbonacci)\ndocstringbot(source_code)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/docstring_checker/#build-a-bot-that-checks-that-docstring-descriptions-match-function-source","title":"Build a bot that checks that docstring descriptions match function source","text":"<p>In this notebook, we are going to build an LLM-powered bot that checks that docstring descriptions match function source. We will use the LlamaBot's StructuredBot and Pydantic to make this happen.</p>"},{"location":"examples/docstring_checker/#define-the-behaviour-of-our-bot","title":"Define the behaviour of our bot","text":"<p>The bot's ideal behaviour will look like this:</p> <ol> <li>It will be given function's source code.</li> <li>It will then be asked to return a boolean judgment call:<ol> <li>If the docstring matches the function source, the answer will be \"True\".</li> <li>If the docstring doesn't match, the answer will be \"False\" along with a list of reasons.</li> </ol> </li> </ol>"},{"location":"examples/docstring_checker/#define-pydantic-model","title":"Define Pydantic model","text":"<p>The desired behaviour above means we need the following Pydantic model:</p>"},{"location":"examples/docstring_checker/#define-the-structuredbots-behaviour","title":"Define the StructuredBot's behaviour","text":"<p>We will now design the prompt for StructuredBot, particularly focusing in on the system prompt for the StructuredBot.</p> <p>The system prompt is an opportunity for us to steer the behaviour of StructuredBot. Here, we leave instructions for the bot to follow. Doing so here allows us to ensure that the bot's <code>__call__</code> method  only needs to be concerned with receiving a function's source (as a string).</p>"},{"location":"examples/docstring_checker/#test-docstringbot-on-different-functions","title":"Test docstringbot on different functions","text":""},{"location":"examples/groq/","title":"Groq","text":"<pre><code>from llamabot import SimpleBot\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ngroq_bot = SimpleBot(\n    system_prompt=\"You are a cheerful llama.\",\n    model_name=\"groq/mixtral-8x7b-32768\",\n)\n</code></pre> <pre><code>%timeit -r1 -n1 groq_bot(\"What's up?\")\n</code></pre> <pre><code>openai_bot = SimpleBot(\n    system_prompt=\"You are a cheerful llama.\", model_name=\"gpt-4-turbo\"\n)\n</code></pre> <pre><code>%timeit -r1 -n1 openai_bot(\"What's up?\")\n</code></pre> <p>In my own testing, Groq's mixtral implementation is ~3-4x faster than OpenAI's GPT-4 turbo model.</p> <p>As of 2024-07-20, with LiteLLM 1.35.38 (which is the current version that LlamaBot is pinned to), Groq does not support streaming with JSON mode via LiteLLM. This GitHub issue has been filed in response.</p>"},{"location":"examples/groq/#using-llamabot-with-groq","title":"Using LlamaBot with Groq","text":"<p>Groq is a super, super fast API for LLMs.</p>"},{"location":"examples/imagebot/","title":"Imagebot","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code># Define the API base (for LM Studio), API key, and model name\n#\n# NOTE: If you are using another service with a real API key,\n# you should NOT store it in plain text here. You should probably\n# use environment variables to manage sensitive information.\nAPI_BASE = \"http://localhost:1234/v1\"\nAPI_KEY = \"lm-studio\"  # This is a dummy value to bypass the check\nMODEL_NAME = \"lm_studio/gemma-3n-e4b-it-mlx\"\n\n# Define the temperature for the model's responses\nTEMPERATURE = 0.2\n</code></pre> <p>Now we can create a <code>SimpleBot</code> instance and connect to the LLM.</p> <pre><code>import llamabot as lmb\nfrom llamabot import SimpleBot\nfrom pathlib import Path\n\n# This example code was written and tested on an Apple Silicon Mac\n# using the LM Studio application to host a Gemma 3n model downloaded\n# from Hugging Face:\n# https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-MLX-bf16\n#\n# Use lm_studio/ prefix to access local models through LM Studio.\n# You can also use other models (e.g. OpenAI or Ollama models)\n# as long as they support image inputs.  See the documentation for details.\n\nsystem_prompt = \"\"\"You are a helpful assistant that can analyze images and \nprovide detailed descriptions of those images.  You will also try to answer\nany questions about the images to the best of your ability.\"\"\"\n\nbot = SimpleBot(\n    system_prompt,\n    temperature=TEMPERATURE,\n    api_base=API_BASE,\n    api_key=API_KEY,\n    model_name=MODEL_NAME,\n)\n</code></pre> <p>Now we use the bot to process a message that includes an image.  We can do this by passing a list of messages to the bot, one of which is an image file path. The image we will use is shown below:</p> <p></p> <p>Image Credit: Photo by Juan Cabanela and is provided under a CC BY-NC 4.0 license.</p> <pre><code># Ask the bot to describe an image localed at the given path\nimage_path = Path(\"./Bearly_There.JPG\")\n\nfirst_message = [\n    lmb.user(\"Briefly (in less than 25 words) describe the following image: \"),\n    lmb.user(image_path),\n]\n\nresponse = bot(first_message)\n</code></pre> <p>So the previous cell properly ingested the image and passed it to the LLM.  The LLM then generated a response based on the image content.  However, when using <code>SimpleBot</code> the context is not saved, so we cannot ask follow-up questions about the image.  </p> <p>For example, if we try to ask a follow-up question about the image, the bot will not remember the previous interaction, and thus will respond in a way that does not reference the image.</p> <pre><code># This will not work as you might expect because the bot has no memory\n\nfollowup_message = [\n    lmb.user(\"What else can you tell me about the bear?\"),\n]\n\nresponse2 = bot(followup_message)\n</code></pre> <p>We can address this by creating a memory store for the chat which can hold the context of the first conversation and the response.  Here we will use a simple list to hold the chat history.</p> <pre><code># Create a memory store for the chat which can hold the context of the\n# conversation.\nchat_memory = []\n\n# Combine the initial message and the response into the chat memory\nchat_memory.extend([first_message, response])\n</code></pre> <p>Now we can ask follow-up questions about the image and the bot will remember the context.  NOTE: You may need to increase the number of tokens the model can use to ensure it has enough context to answer the question.</p> <pre><code># Starting with the chat memory for the previous interaction,\n# ask a followup question about the image and then send all\n# of that to the bot with memory.\nmessages = chat_memory + followup_message\n\n# Call the bot with the full message history\nresponse2 = bot(messages)\n</code></pre> <pre><code># Load an OpenAI API Key from an environment variable and select an\n# OpenAI model to use\nimport os\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# We will use the DALL-E 3 model for image generation, which is not\n# the newest model but is still quite capable.\nOPENAI_MODEL = \"dall-e-3\"\n</code></pre> <p>Now let's call the <code>ImageBot</code> to generate an image from a text prompt.  The generated image will be returned as a URL that is used to display the image in the notebook.  If you want to save the image locally, you can use the <code>requests</code> library to download it.</p> <pre><code>from llamabot.bot.imagebot import ImageBot\n\n# Create an ImageBot instance\n# The supported sizes are: '1024x1024', '1024x1792', and '1792x1024'\n# with the default being '1024x1024'.\nimg_gen_bot = ImageBot(\n    size=\"1024x1024\",  # The default size is 1024x1024\n)\n\nimg_gen_bot(\n    \"A grizzly bear eating some berries at a picnic table in a forest.\",\n    return_url=True,\n    save_path=Path(\"./generated_bear_image.png\"),\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/imagebot/#imagebot","title":"ImageBot","text":"<p>This notebook shows how to use the ImageBot API to ingest or generate images from text.</p>"},{"location":"examples/imagebot/#image-ingestion","title":"Image Ingestion","text":"<p>For image ingestion, we will use the <code>SimpleBot</code> class, which can take an iterable of messages and pass them to the LLM.  Making one of the messages an image URL or a local file path will automatically convert it into a format that can be used by the LLM.</p> <p>In this example, we will use a local LLM (Gemma 3n) hosted on LM Studio on an Apple Silicon Mac.  You can choose any LLM that is compatible with your computer architecture (including non-local models) as long as they can process images.</p> <p>First you need to set up the environment variable to point to your LM Studio instance.  You can skip this step if you are using a non-local model.</p>"},{"location":"examples/imagebot/#image-generation","title":"Image Generation","text":"<p>Image generation, due to the rather large memory requirements, is normally not available on local models. We will need to use an visual language model, which is available through the OpenAI API. It is assumed you have set up your OpenAI API key in the environment variable (as per OpenAI's best practices documentation).</p> <p>Once we have set up the environment variable, we can load the API key:</p>"},{"location":"examples/json_mode/","title":"Json mode","text":"<pre><code>from llamabot import SimpleBot\n\nbot = SimpleBot(\"You are a bot proficient at returning JSON.\", model_name=\"ollama/mistral\", json_mode=True)\nbot(\"What is the weather like today? Return in JSON with the following structure: {'location': 'City Name', 'temperature': 'Temperature in Celsius', 'description': 'Weather Description'}\")\n</code></pre>"},{"location":"examples/knowledge-graph-bot/","title":"Knowledge graph bot","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>Firstly, let's start by defining a class <code>KnowledgeGraphTriple</code> to represent a triple from a knowledge graph.</p> <pre><code>from llamabot import StructuredBot\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass KnowledgeGraphTriple(BaseModel):\n    \"\"\"A triple from a knowledge graph.\"\"\"\n\n    sub: str = Field(description=\"The subject of the triple.\")\n    pred: str = Field(description=\"The predicate of the triple.\")\n    obj: str = Field(description=\"The object of the triple.\")\n    quote: str = Field(\n        description=\"Quote from the provided text that indicates the triple relationship.\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_lengths(self):\n        \"\"\"Validate the lengths of subjects, predicates, and objects.\n\n        They should not be &amp;gt;5 words each.\n        \"\"\"\n        if len(self.sub.split()) &amp;gt; 5:\n            raise ValueError(\n                f\"Subject '{self.sub}' is too long. It needs to be &amp;lt;=5 words.\"\n            )\n\n        if len(self.pred.split()) &amp;gt; 5:\n            raise ValueError(\n                f\"Predicate '{self.pred}' is too long. It needs to be &amp;lt;=5 words.\"\n            )\n\n        if len(self.obj.split()) &amp;gt; 5:\n            raise ValueError(\n                f\"Object '{self.obj}' is too long. It needs to be &amp;lt;=5 words.\"\n            )\n\n        return self\n</code></pre> <p>Within the KnowledgeGraphTriple class,  we've added a <code>validate_lengths</code> method  to ensure that the lengths of the subject, predicate, and object  are not greater than 5 words.</p> <p>Next, I am going to define a <code>KnowledgeGraphTriplets</code> class  that houses a collection of KnowledgeGraphTriple objects. This class will allow me to instruct the StructuredBot  to extract multiple triples from a given text.</p> <pre><code>class KnowledgeGraphTriplets(BaseModel):\n    \"\"\"A list of knowledge graph triples.\"\"\"\n\n    triples: list[KnowledgeGraphTriple] = Field(description=\"A list of triples.\")\n\n    def draw(self):\n        import networkx as nx\n        from IPython.display import Image\n        import tempfile\n\n        G = nx.DiGraph()\n        for triple in self.triples:\n            G.add_edge(triple.sub, triple.obj, label=triple.pred)\n\n        A = nx.nx_agraph.to_agraph(G)\n\n        # Create a temporary file to store the image.\n        with tempfile.NamedTemporaryFile(suffix=\".png\") as f:\n            A.draw(f.name, prog=\"circo\", format=\"png\")\n            return Image(filename=f.name)\n</code></pre> <p>Now, let's set up the StructuredBot, which we will call <code>kgbot</code>, which stands for \"Knowledge Graph Bot\". We will use the Groq-hosted <code>llama-3.1-70b-versatile</code>, which is a powerful open-source language model developed by Meta AI and hosted on Groq for lightning fast text generation.</p> <pre><code>from llamabot import prompt\n\n\n@prompt\ndef kgbot_sysprompt() -&amp;gt; str:\n    \"\"\"You are an expert at knowledge graph triples.\n\n    You will be given a paragraph of text and you need to extract triples from it.\n    Each triple should be presented as a structured object.\n    \"\"\"\n\n\nkgbot = StructuredBot(\n    system_prompt=kgbot_sysprompt(),\n    pydantic_model=KnowledgeGraphTriplets,\n    stream_target=\"none\",\n    model_name=\"groq/llama-3.1-70b-versatile\",\n)\n</code></pre> <p>The LLM of choice here is GPT-4. We could choose to use <code>ollama/llama3</code>, which is a powerful open source language model developed by Meta AI. It needs just over 16GB of RAM to run. It's going to be much slower than calling on the GPT-4 API.</p> <p>With the bot instantiated, let's now analyze a paragraph of text about the citric acid cycle.</p> <pre><code>txt = \"\"\"\nThe citric acid cycle\u2014also known as the Krebs cycle, Szent\u2013Gy\u00f6rgyi\u2013Krebs cycle or the TCA cycle (tricarboxylic acid cycle)[1][2]\u2014is a series of biochemical reactions to release the energy stored in nutrients through the oxidation of acetyl-CoA derived from carbohydrates, fats, and proteins. The chemical energy released is available under the form of ATP. The Krebs cycle is used by organisms that respire (as opposed to organisms that ferment) to generate energy, either by anaerobic respiration or aerobic respiration. In addition, the cycle provides precursors of certain amino acids, as well as the reducing agent NADH, that are used in numerous other reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest components of metabolism.[3][4] Even though it is branded as a \"cycle\", it is not necessary for metabolites to follow only one specific route; at least three alternative segments of the citric acid cycle have been recognized.[5]\n\"\"\"\n\n\ntriples_set = kgbot(txt, verbose=True)\n</code></pre> <p>Let's visualize the triples extracted from the given text.</p> <pre><code>triples_set.draw()\n</code></pre> <p>Effectively we have gained an auto-generated mind map for ourselves!</p> <p>Let's try another text, one that is on pair programming, from Martin Fowler's website.</p> <pre><code>text = \"\"\"Pair programming essentially means that two people write code together on one machine. It is a very collaborative way of working and involves a lot of communication. While a pair of developers work on a task together, they do not only write code, they also plan and discuss their work. They clarify ideas on the way, discuss approaches and come to better solutions.\"\"\"\n\npair_programming_knowledge = kgbot(text)\n</code></pre> <pre><code>pair_programming_knowledge.draw()\n</code></pre> <p>Applying the bot to more text:</p> <pre><code>text = \"\"\"\nRenewable energy sources are becoming increasingly vital as the world grapples with the adverse effects of climate change and the depletion of fossil fuels. Unlike traditional energy sources such as coal, oil, and natural gas, renewable energy comes from resources that are naturally replenished on a human timescale, including sunlight, wind, rain, tides, waves, and geothermal heat.\n\nOne of the most prominent forms of renewable energy is solar power. Solar panels convert sunlight directly into electricity through photovoltaic cells. This technology has seen significant advancements over the past decade, leading to increased efficiency and reduced costs. Solar energy is abundant and can be harnessed in virtually every part of the world, making it a cornerstone of future energy strategies.\n\nWind energy is another major player in the renewable energy sector. Wind turbines capture kinetic energy from wind and convert it into electrical power. Wind farms, both onshore and offshore, have been developed across the globe. The scalability of wind power, from small residential turbines to large commercial farms, provides versatility in its applications.\n\nHydropower, the largest source of renewable electricity globally, generates power by using the energy of moving water. Dams built on large rivers create reservoirs that can be used to generate electricity on demand. Small-scale hydro projects also contribute significantly to local energy needs, especially in remote areas.\n\nGeothermal energy harnesses heat from within the Earth. This heat can be used directly for heating or to generate electricity. Regions with high geothermal activity, such as Iceland and parts of the United States, have successfully integrated geothermal energy into their power grids.\n\nBioenergy, derived from organic materials such as plant and animal waste, is a versatile renewable energy source. It can be used for electricity generation, heating, and as a biofuel for transportation. The use of bioenergy can also help manage waste and reduce greenhouse gas emissions.\n\nThe transition to renewable energy sources is not without challenges. The intermittency of solar and wind power requires advancements in energy storage technologies to ensure a stable supply. Moreover, the initial investment costs for renewable energy infrastructure can be high, although these are often offset by long-term savings and environmental benefits.\n\nGovernment policies and international cooperation play crucial roles in promoting renewable energy. Subsidies, tax incentives, and research funding are essential to accelerate the development and adoption of these technologies. Public awareness and community involvement also contribute to the successful implementation of renewable energy projects.\n\nIn conclusion, renewable energy represents a sustainable and essential path forward for global energy needs. By reducing reliance on fossil fuels, we can mitigate the impacts of climate change, enhance energy security, and create a cleaner, healthier environment for future generations.\n\"\"\"\nenergy_kg = kgbot(text)\n</code></pre> <pre><code>energy_kg.draw()\n</code></pre> <p>Finally, let's look at inductive Bible study, a technique for studying Biblical texts  using inductive reasoning and analytic thinking.</p> <p>We will create a different bot (<code>ibsbot</code>, which stands for \"Inductive Bible Study Bot\"), to help us parse and extract structured information from the text.</p> <p>Firstly, let's define the pydantic models for what we want to extract from scripture.</p> <pre><code>from IPython.display import Image\n\n\nclass IBSLogicalRelationship(BaseModel):\n    statement1: str = Field(description=\"First statement made by the author.\")\n    joiner: str = Field(\n        description=\"Joiner between statement1 and statement2. Should be a logical conjunction, such as 'but', 'so that', 'in order that', 'for', 'because', 'and', 'if'.\"\n    )\n    statement2: str = Field(\n        description=\"Second statement logically related to statement1.\"\n    )\n\n\nclass IBSRelationships(BaseModel):\n    \"\"\"A list of inductive Bible study relationships.\"\"\"\n\n    relationships: list[IBSLogicalRelationship] = Field(\n        description=\"A list of relationships.\"\n    )\n\n    def draw(self) -&amp;gt; Image:\n        \"\"\"Draw a diagram of the relationships.\"\"\"\n        import networkx as nx\n        from IPython.display import Image\n        import tempfile\n\n        G = nx.DiGraph()\n        for triple in self.relationships:\n            G.add_edge(triple.statement1, triple.statement2, label=triple.joiner)\n\n        A = nx.nx_agraph.to_agraph(G)\n\n        # Create a temporary file to store the image.\n        with tempfile.NamedTemporaryFile(suffix=\".png\") as f:\n            A.draw(f.name, prog=\"circo\", format=\"png\")\n            return Image(filename=f.name)\n</code></pre> <p>Now, let's create the system prompt for the IBSBot.</p> <pre><code>@prompt\ndef ibsbot_sysprompt() -&amp;gt; str:\n    \"\"\"You are an expert at inductive Bible study.\n\n    Your goal is to extract, from a given text,\n    logical relationships between the author's statements.\n\n    Segment the text into atomic segments that each contain a single idea\n    that can be related to each other through a conjunction.\n\n    Then, populate the provided pydantic model with the extracted relationships.\n    Reuse as many atomic segments as possible.\n    \"\"\"\n</code></pre> <p>Finally, we create the <code>ibsbot</code>.</p> <pre><code>ibsbot = StructuredBot(\n    system_prompt=ibsbot_sysprompt(),\n    pydantic_model=IBSRelationships,\n    stream_target=\"none\",\n    model_name=\"groq/llama-3.1-70b-versatile\",\n)\n</code></pre> <p>Now, let's extract and visualize relationships from the text, Ephesians 2:1-10.</p> <pre><code>ephesians_text = \"\"\"2 As for you, you were dead in your transgressions and sins, 2 in which you used to live when you followed the ways of this world and of the ruler of the kingdom of the air, the spirit who is now at work in those who are disobedient. 3 All of us also lived among them at one time, gratifying the cravings of our flesh[a] and following its desires and thoughts. Like the rest, we were by nature deserving of wrath. 4 But because of his great love for us, God, who is rich in mercy, 5 made us alive with Christ even when we were dead in transgressions\u2014it is by grace you have been saved. 6 And God raised us up with Christ and seated us with him in the heavenly realms in Christ Jesus, 7 in order that in the coming ages he might show the incomparable riches of his grace, expressed in his kindness to us in Christ Jesus. 8 For it is by grace you have been saved, through faith\u2014and this is not from yourselves, it is the gift of God\u2014 9 not by works, so that no one can boast. 10 For we are God\u2019s handiwork, created in Christ Jesus to do good works, which God prepared in advance for us to do.\"\"\"\nephesians_kg = ibsbot(ephesians_text)\n</code></pre> <p>What have we extracted? Let's visualize that.</p> <pre><code>ephesians_kg.draw()\n</code></pre>"},{"location":"examples/knowledge-graph-bot/#extract-knowledge-graph-triples-from-text","title":"Extract knowledge graph triples from text","text":"<p>In this notebook, we will show how to use LlamaBot's StructuredBot  to extract knowledge graph triples from text.</p>"},{"location":"examples/pdf/","title":"Pdf","text":"<pre><code># PDF Chatbot\n%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code># Download pre-built index.json file from Dropbox\nimport requests\n\nheaders = {\"user-agent\": \"Wget/1.16 (linux-gnu)\"}  # &amp;lt;-- the key is here!\nr = requests.get(\n    \"https://www.dropbox.com/s/wrixlu7e3noi43q/Ma%20et%20al.%20-%202021%20-%20Machine-Directed%20Evolution%20of%20an%20Imine%20Reductase%20f.pdf?dl=0\",\n    stream=True,\n    headers=headers,\n)\npdf_fname = \"/tmp/machine-directed-evolution.pdf\"\nwith open(pdf_fname, \"wb\") as f:\n    for chunk in r.iter_content(chunk_size=1024):\n        if chunk:\n            f.write(chunk)\n</code></pre> <pre><code>from llamabot import QueryBot\nfrom pyprojroot import here\n\n# If you're prototyping with your own PDF, uncomment the following code and use it instead of the saved index path:\n# bot = QueryBot(\n#     \"You are a bot that reads a PDF book and responds to questions about that book.\",\n#     document_paths=[pdf_fname],\n#     collection_name=\"machine-directed-evolution-paper\",\n#     model_name=\"mistral/mistral-medium\",\n# )\n\nbot = QueryBot(\n    \"You are a bot that reads a PDF book and responds to questions about that book.\",\n    collection_name=\"machine-directed-evolution-paper\",\n    model_name=\"mistral/mistral-medium\",\n)\n</code></pre> <pre><code>prompt = \"I'd like to use the workflow of this paper to educate colleagues. What are the main talking points I should use?\"\nbot(prompt)\n</code></pre> <pre><code>prompt = \"My colleagues are interested in evolving another enzyme. However, they may be unaware of how machine learning approaches will help them there. Based on this paper, what can I highlight that might overcome their lack of knowledge?\"\nbot(prompt)\n</code></pre> <pre><code>prompt = \"What data from the paper helped show this point, 'Machine-directed evolution is an efficient strategy for enzyme engineering, as it can help navigate enzyme sequence space more effectively and reduce the number of enzyme variants to be measured en route to a desirable enzyme under realistic process conditions.'?\"\nbot(prompt)\n</code></pre> <pre><code>prompt = \"How can I succinctly present the SGM vs. EPPCR results to my colleagues? Or in other words, how would Richard Feynman present these results?\"\nbot(prompt)\n</code></pre> <p>Using SimpleBot below should prove that we are indeed querying a book and not just relying on the LLM's training set.</p> <pre><code>from llamabot import SimpleBot\n\n\nsbot = SimpleBot(\"You are a bot that responds to human questions.\")\nsbot(prompt)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/querybot/","title":"Querybot","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>from llamabot import QueryBot\nimport git\nfrom IPython.display import display, Markdown\n</code></pre> <pre><code>import tempfile\nfrom pathlib import Path\n\n# Create a temporary directory\ntemp_dir = tempfile.TemporaryDirectory(dir=\"/tmp\")\n\n\nrepo_url = \"https://github.com/duckdb/duckdb-web\"\n# Clone the repository into the temporary directory\nrepo = git.Repo.clone_from(repo_url, temp_dir.name)\n\n# Set the root directory to the cloned repository\nroot_dir = Path(temp_dir.name)\n</code></pre> <pre><code>from slugify import slugify\nimport chromadb\n\nclient = chromadb.PersistentClient(path=str(Path.home() / \".llamabot\" / \"chroma.db\"))\ncollection = client.create_collection(slugify(repo_url), get_or_create=True)\n\nresults = collection.get()\n</code></pre> <pre><code>source_file_extensions = [\n    \"py\",\n    \"jl\",\n    \"R\",\n    \"ipynb\",\n    \"md\",\n    \"tex\",\n    \"txt\",\n    \"lr\",\n    \"rst\",\n]\n\n\nsource_files = []\nfor extension in source_file_extensions:\n    files = list(root_dir.rglob(f\"*.{extension}\"))\n    print(f\"Found {len(files)} files with extension {extension}.\")\n    source_files.extend(files)\n</code></pre> <pre><code>from slugify import slugify\nbot = QueryBot(\n    system_prompt=\"You are an expert in the code repository given to you.\",\n    collection_name=slugify(repo_url),\n    document_paths=source_files,\n)\n</code></pre> <pre><code>bot(\"Give me an example of lambda functions in DuckDB.\")\n</code></pre> <pre><code>bot(\"What is your view on building a digital portfolio?\")\n</code></pre> <pre><code>bot(\"What were your experiences with the SciPy conference?\")\n</code></pre> <pre><code>bot(\"What tutorials did you attend at the SciPy conference in 2023?\")\n</code></pre> <pre><code>from numpy import source\nfrom llamabot.file_finder import recursive_find\nfrom pyprojroot import here\n\nsource_python_files = recursive_find(root_dir=here() / \"llamabot\", file_extension=\".py\")\n\ncodebot = QueryBot(\n    \"You are an expert in code Q&amp;amp;A.\",\n    collection_name=\"llamabot\",\n    document_paths=source_python_files,\n    model_name=\"gpt-4-1106-preview\",\n)\n</code></pre> <pre><code>codebot(\"How do I find all the files in a directory?\")\n</code></pre> <pre><code>codebot(\"Which Bot do I use to chat with my documents?\")\n</code></pre> <pre><code>codebot(\"Explain to me the architecture of SimpleBot.\")\n</code></pre> <pre><code>codebot(\"What are the CLI functions available in LlamaBot?\")\n</code></pre> <pre><code>from llamabot.bot.qabot import DocQABot\n\ncodebot = DocQABot(\n    collection_name=\"llamabot\",\n)\ncodebot.add_documents(document_paths=source_python_files)\n</code></pre> <pre><code>codebot(\n    \"Does LlamaBot provide a function to find all files recursively in a directory?\"\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/querybot/#eric-ma-qa","title":"Eric Ma Q&amp;A","text":"<p>This shows how to build a blog Q&amp;A bot using the text contents of Eric Ma's blog.</p>"},{"location":"examples/querybot/#setup-download-blog-data","title":"Setup: Download blog data","text":""},{"location":"examples/querybot/#llamabot-code-query","title":"LlamaBot Code Query","text":""},{"location":"examples/recorder/","title":"Recorder","text":"<pre><code>from llamabot import SimpleBot, PromptRecorder\n</code></pre> <pre><code>bot = SimpleBot(\"You are a bot.\")\n</code></pre> <pre><code># Try three different prompts.\n\nprompt1 = (\n    \"You are a fitness coach who responds in 25 words or less. How do I gain muscle?\"\n)\nprompt2 = \"You are an expert fitness coach who responds in 100 words or less. How do I gain muscle?\"\nprompt3 = \"You are an expert fitness coach who responds in 25 words or less and will not give lethal advice. How do I gain muscle?\"\n\nrecorder = PromptRecorder()\n</code></pre> <pre><code>with recorder:\n    bot(prompt1)\n    bot(prompt2)\n    bot(prompt3)\n</code></pre> <pre><code>recorder.prompts_and_responses\n</code></pre> <pre><code>import pandas as pd\n\npd.DataFrame(recorder.prompts_and_responses)\n</code></pre> <pre><code>prompt4 = \"You are an expert fitness coach who responds in 25 words or less, and you help people who only have access to body weight exercises. How do I gain muscle?\"\n\nwith recorder:\n    bot(prompt4)\n</code></pre> <pre><code>recorder.panel()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/recorder/#recording-prompts","title":"Recording Prompts","text":"<p>One challenge I've found when working with prompts is recording what I get back when I try out different prompts. Copying and pasting is clearly not what I'd like to do. So I decided to write some functionality into Llamabot that lets us do recording of prompts  and the responses returned by GPT.</p> <p>Here's how to use it.</p>"},{"location":"examples/simple_panel/","title":"Simple panel","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>This notebook shows how to create a simple Panel app surrounding SimpleBot.</p> <p>Firstly, our imports:</p> <pre><code>from llamabot import SimpleBot\n</code></pre> <p>Then we create the bot, in this case, a Feynman bot:</p> <pre><code>feynman = SimpleBot(\n    \"\"\"\nYou are Richard Feynman.\nYou will be given a difficult concept, and your task is to explain it back.\n\"\"\"\n)\n</code></pre> <p>We'll build an app that lets others take in a chunk of text (an abstract) that the Feynman bot will re-explain back to us. </p> <p>For that, we'll need to start with a text area input:</p> <pre><code>app = feynman.panel(\n    input_text_label=\"Abstract\",\n    output_text_label=\"Summary\",\n    site_name=\"Feynman Bot\",\n    title=\"Feynman Bot\",\n)\n</code></pre> <pre><code>app.show()\n</code></pre> <p>To run this, execute the following command from the repo root:</p> <pre><code>panel run docs/simple_panel.ipynb\n</code></pre>"},{"location":"examples/simple_panel/#simplebot-apps","title":"SimpleBot Apps","text":""},{"location":"examples/simple_panel/#build-the-ui","title":"Build the UI","text":""},{"location":"examples/simplebot/","title":"Simplebot","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>Let's say we have the text of a blog...</p> <pre><code>with open(\"../../data/blog_text.txt\", \"r+\") as f:\n    blog_text = f.read()\nblog_text[0:100] + \"...\"\n</code></pre> <p>And we'd like to create a function that takes in the text and gives us a draft LinkedIn post, complete with emojis, that is designed to entice others to read the blog post. LLaMaBot's <code>SimpleBot</code> lets us build that function easily.</p> <pre><code>from llamabot import SimpleBot\n\nsystem_prompt = \"\"\"You are a LinkedIn post generator bot.\nA human will give you the text of a blog post that they've authored,\nand you will compose a LinkedIn post that advertises it.\nThe post is intended to hook a reader into reading the blog post.\nThe LinkedIn post should be written with one line per sentence.\nEach sentence should begin with an emoji appropriate to that sentence.\nThe post should be written in professional English and in first-person tone for the human.\n\"\"\"\n\nlinkedin = SimpleBot(\n    system_prompt=system_prompt,\n    stream_target=\"stdout\",  # this is the default!,\n    model_name=\"gpt-4-0125-preview\",\n)\n</code></pre> <p>Note that SimpleBot by default will always stream.  All that you need to configure is where you want to stream to.</p> <p>With <code>linkedin</code>, we can now pass in the blog text and - voila! - get back a draft LinkedIn post.</p> <pre><code>linkedin_post = linkedin(blog_text)\n</code></pre> <p>Now, you can edit it to your hearts content! :-)</p> <p>Next up, we have streaming that is compatible with Panel's Chat interface, which expects the text to be returned in its entirety as it is being built up.</p> <pre><code>linkedin_panel = SimpleBot(\n    system_prompt=system_prompt,\n    stream_target=\"panel\",\n)\n</code></pre> <pre><code>linkedin_post = linkedin_panel(blog_text)\n</code></pre> <pre><code>for post in linkedin_post:\n    print(post)\n</code></pre> <p>And finally, we have streaming via the API. We return a generator that yields individual parts of text as they are being generated.</p> <pre><code>linkedin_api = SimpleBot(\n    system_prompt=system_prompt,\n    stream_target=\"api\",\n)\n\nlinkedin_post = linkedin_api(blog_text)\nfor post in linkedin_post:\n    print(post, end=\"\")\n</code></pre> <p>If you have an Ollama server running, you can hit the API using SimpleBot. The pre-requisite is that you have already run <code>ollama pull &lt;modelname&gt;</code>  to download the model to the Ollama server. </p> <pre><code>print(system_prompt)\n</code></pre> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nlinkedin_ollama = SimpleBot(\n    model_name=\"ollama/mistral\",  # Specifying Ollama via the model_name argument is necessary!s\n    system_prompt=system_prompt,\n    stream_target=\"stdout\",  # this is the default!\n    api_base=f\"http://{os.getenv('OLLAMA_SERVER')}:11434\",\n)\nlinkedin_post = linkedin_ollama(blog_text)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/simplebot/#llamabots-simplebot-in-under-5-minutes","title":"LLaMaBot's SimpleBot in under 5 minutes","text":""},{"location":"examples/structuredbot/","title":"Structuredbot","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>When using LLMs, an ideal goal would be to  pull structured data out of unstructured text.  When the data is structured,  we can then use it programmatically in later steps.</p> <p>In this example, we'll look at a small dataset of SciPy videos uploaded to YouTube.  The videos are given a title and a description.  We want to extract the name of the speaker giving the talk,  and the topics the talk is about. We also want to be able to validate the data we've extracted  not only matches the structured format we expect,  but that it also meets some custom requirements.</p> <pre><code># load in unstructured text data\nimport pandas as pd\n\ndf = pd.read_json(\"../scipy_videos.json\", orient=\"index\")\ndf\n</code></pre> <p>Let's now define a Pydantic schema for the data that we wish to extract from movie entry. This is doen by defining a BaseModel class and field validators.</p> <pre><code>from typing import List, Optional\nfrom pydantic import BaseModel, Field, field_validator\n\n\nclass TopicExtract(BaseModel):\n    \"\"\"This object stores the name of the speaker presenting the video.\n\n    It also generates a list of topics\n    that best describe what this talk is about.\n    \"\"\"\n\n    speaker_name: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The name of the speaker giving this talk. \"\n            \"If there is no speaker named, leave empty.\"\n        ),\n    )\n    topics: List[str] = Field(\n        description=(\n            \"A list of upto 5 topics that this text is about. \"\n            \"Each topic should be at most 1 or 2 word descriptions. \"\n            \"All lowercase.\"\n        )\n    )\n\n    @field_validator(\"topics\")\n    def validate_num_topics(cls, topics):\n        # validate that the list of topics contains atleast 1, and no more than 5 topics\n        if len(topics) &amp;lt;= 0 or len(topics) &amp;gt; 5:\n            raise ValueError(\"The list of topics can be no more than 5 items\")\n        return topics\n\n    @field_validator(\"topics\")\n    def validate_num_topic_words(cls, topics):\n        # for each topic the model generated, ensure that the topic contains no more than 2 words\n        for topic in topics:\n            if len(topic.split()) &amp;gt; 2:\n                # make the validation message helpful to the LLM.\n                # Here we repeat which topic is failing validation, and remind it what it must do to pass the validation.\n                raise ValueError(\n                    f'The topic \"{topic}\" has too many words, A topic can contain AT MOST 2 words'\n                )\n        return topics\n</code></pre> <p>Now we can initialize the PydanticBot and assign this model to it.</p> <pre><code>from llamabot import prompt, StructuredBot\n\n\n@prompt\ndef topicbot_sysprompt() -&amp;gt; str:\n    \"\"\"You are an expert topic labeller.\n    You read a video title and description\n    and extract the speakers name and the topics the video is about.\n    \"\"\"\n\n\n# Will use the OpenAI API by default, which requires an API key.\n# If you want to, you can change this to a local LLM (from Ollama)\n# by specifying, say, `model_name=\"ollama/mistral\"`.\nbot = StructuredBot(\n    system_prompt=topicbot_sysprompt(),\n    temperature=0,\n    pydantic_model=TopicExtract,\n    # model_name=\"ollama/mistral\"\n)\n</code></pre> <p>Now we can pass in our text, and extract the topics</p> <pre><code>video_extracts = []\nfor index, video_row in df.iterrows():\n    video_text = f\"video title: {video_row['name']}\\nvideo description: {video_row['description']}\"\n\n    extract = bot(video_text)\n\n    video_extracts.append(extract)\n</code></pre> <p>Let's now inspect what the topics looked like.</p> <pre><code>for video in video_extracts:\n    print(video)\n</code></pre> <p>Look's pretty accurate!</p>"},{"location":"examples/structuredbot/#llamabots-structuredbot-in-under-5-minutes","title":"LLaMaBot's <code>StructuredBot</code> in under 5 minutes","text":""},{"location":"examples/structuredbot/#read-video-descriptions","title":"Read video descriptions","text":"<p>Firstly, let's look at the video descriptions file.  It is stored as a JSON file. We can read it into pandas by using <code>pd.read_json</code>:</p>"},{"location":"getting-started/which-bot/","title":"Which Bot Should I Use?","text":"<p>LlamaBot provides several bot types, each optimized for different use cases. This guide helps you choose the right bot for your needs.</p>"},{"location":"getting-started/which-bot/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>flowchart TD\n    A(\"Do you need to generate images?\") --&gt;|Yes| B[ImageBot]\n    A --&gt;|No| C(\"Do you need structured, validated output (Pydantic models)?\")\n    C --&gt;|Yes| D[StructuredBot]\n    C --&gt;|No| E(\"Do you need to query documents or a knowledge base?\")\n    E --&gt;|Yes| F[QueryBot]\n    E --&gt;|No| G(\"Do you need to execute tools/functions?\")\n    G --&gt;|Yes| H(\"Do you need multi-step planning with multiple tools?\")\n    H --&gt;|Yes| I[AgentBot]\n    H --&gt;|No| J[ToolBot]\n    G --&gt;|No| K[SimpleBot]</code></pre>"},{"location":"getting-started/which-bot/#bot-comparison","title":"Bot Comparison","text":"Bot Type Best For Key Features Memory Support SimpleBot General-purpose chat, prompt experimentation Stateless or stateful conversations, simple Q&amp;A ChatMemory (optional) ToolBot Single-turn function calling, automation Execute one tool per request, access globals ChatMemory (optional) AgentBot Multi-step workflows, complex tasks Graph-based tool orchestration, multi-turn planning Built-in QueryBot Document Q&amp;A, RAG applications Document retrieval, semantic search ChatMemory + DocStore StructuredBot Data extraction, API responses Pydantic validation, guaranteed schema ChatMemory (optional) ImageBot Image generation DALL-E integration, image saving None"},{"location":"getting-started/which-bot/#detailed-use-cases","title":"Detailed Use Cases","text":""},{"location":"getting-started/which-bot/#simplebot","title":"SimpleBot","text":"<p>Use SimpleBot when:</p> <ul> <li>You need a general-purpose chatbot</li> <li>You're experimenting with prompts</li> <li>You want stateless or simple conversational interactions</li> <li>You don't need tool execution or document retrieval</li> </ul> <p>Example scenarios:</p> <ul> <li>Creating a chatbot that explains concepts</li> <li>Building a customer service bot</li> <li>Prompt engineering and experimentation</li> <li>Simple Q&amp;A without external data</li> </ul> <pre><code>import llamabot as lmb\n\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\"\n)\n\nresponse = bot(\"Explain quantum computing\")\n</code></pre>"},{"location":"getting-started/which-bot/#toolbot","title":"ToolBot","text":"<p>Use ToolBot when:</p> <ul> <li>You need to execute a single function/tool per request</li> <li>You want to access global variables in your Python session</li> <li>You need automation workflows with function calling</li> <li>You're building data analysis assistants</li> </ul> <p>Example scenarios:</p> <ul> <li>Data analysis workflows (access DataFrames, execute code)</li> <li>API integrations (call external services)</li> <li>Single-step automation tasks</li> <li>Code execution in notebooks</li> </ul> <pre><code>import llamabot as lmb\nfrom llamabot.components.tools import write_and_execute_code\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a data analyst.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[write_and_execute_code(globals_dict=globals())]\n)\n\nresponse = bot(\"Calculate the mean of the sales_data DataFrame\")\n</code></pre>"},{"location":"getting-started/which-bot/#agentbot","title":"AgentBot","text":"<p>Use AgentBot when:</p> <ul> <li>You need multi-step planning and execution</li> <li>Your task requires multiple tools working together</li> <li>You want graph-based tool orchestration</li> <li>You need complex workflows with decision-making</li> </ul> <p>Example scenarios:</p> <ul> <li>Research assistants that search, analyze, and summarize</li> <li>Multi-step data pipelines</li> <li>Complex automation workflows</li> <li>Agents that need to plan before executing</li> </ul> <pre><code>import llamabot as lmb\n\n@lmb.tool\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Implementation here\n    return results\n\n@lmb.tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to the user.\"\"\"\n    return response\n\nbot = lmb.AgentBot(\n    tools=[search_web, respond_to_user],\n    model_name=\"gpt-4o-mini\"\n)\n\nresponse = bot(\"Research the latest developments in AI and summarize them\")\n</code></pre>"},{"location":"getting-started/which-bot/#querybot","title":"QueryBot","text":"<p>Use QueryBot when:</p> <ul> <li>You need to answer questions from documents</li> <li>You're building a RAG (Retrieval-Augmented Generation) application</li> <li>You have a knowledge base to query</li> <li>You need semantic search over documents</li> </ul> <p>Example scenarios:</p> <ul> <li>Document Q&amp;A systems</li> <li>Knowledge base assistants</li> <li>Code documentation helpers</li> <li>Internal wiki query systems</li> </ul> <pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(table_name=\"my_docs\")\ndocstore.extend([doc1, doc2, doc3])\n\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore,\n    memory=lmb.ChatMemory()\n)\n\nresponse = bot(\"What does the documentation say about authentication?\")\n</code></pre>"},{"location":"getting-started/which-bot/#structuredbot","title":"StructuredBot","text":"<p>Use StructuredBot when:</p> <ul> <li>You need guaranteed structured output</li> <li>You're extracting data from unstructured text</li> <li>You need Pydantic model validation</li> <li>You're building APIs that require specific schemas</li> </ul> <p>Example scenarios:</p> <ul> <li>Data extraction from text</li> <li>Form processing with validation</li> <li>API response generation</li> <li>Structured data parsing</li> </ul> <pre><code>import llamabot as lmb\nfrom pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n\nperson = bot(\"John is 25 years old. Email: john@example.com\")\n# Returns validated Person object\n</code></pre>"},{"location":"getting-started/which-bot/#imagebot","title":"ImageBot","text":"<p>Use ImageBot when:</p> <ul> <li>You need to generate images from text prompts</li> <li>You're building creative applications</li> <li>You need DALL-E integration</li> </ul> <p>Example scenarios:</p> <ul> <li>Image generation for content creation</li> <li>Creative AI applications</li> <li>Visual content generation</li> </ul> <pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot()\n\nimage_path = bot(\"A painting of a sunset over mountains\")\n</code></pre>"},{"location":"getting-started/which-bot/#memory-options","title":"Memory Options","text":""},{"location":"getting-started/which-bot/#chatmemory-conversational-memory","title":"ChatMemory (Conversational Memory)","text":"<p>Available for: SimpleBot, ToolBot, StructuredBot, QueryBot</p> <ul> <li>Linear memory: Fast, no LLM calls</li> </ul> <pre><code>memory = lmb.ChatMemory()\n</code></pre> <ul> <li>Threaded memory: Intelligent conversation threading</li> </ul> <pre><code>memory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n</code></pre>"},{"location":"getting-started/which-bot/#document-store-rag-memory","title":"Document Store (RAG Memory)","text":"<p>Available for: QueryBot</p> <ul> <li>LanceDBDocStore: Default, fast vector search</li> <li>BM25DocStore: Keyword-based search</li> </ul>"},{"location":"getting-started/which-bot/#migration-guide","title":"Migration Guide","text":""},{"location":"getting-started/which-bot/#from-simplebot-to-toolbot","title":"From SimpleBot to ToolBot","text":"<p>If you find yourself needing to execute code or call functions:</p> <pre><code># Before: SimpleBot\nbot = lmb.SimpleBot(\"You are a helper.\")\n\n# After: ToolBot with code execution\nbot = lmb.ToolBot(\n    system_prompt=\"You are a helper.\",\n    tools=[write_and_execute_code(globals_dict=globals())]\n)\n</code></pre>"},{"location":"getting-started/which-bot/#from-toolbot-to-agentbot","title":"From ToolBot to AgentBot","text":"<p>If you need multi-step planning instead of single-turn execution:</p> <pre><code># Before: ToolBot (single turn)\nbot = lmb.ToolBot(tools=[my_tool])\n\n# After: AgentBot (multi-turn)\n@lmb.tool\ndef my_tool(arg: str) -&gt; str:\n    return result\n\nbot = lmb.AgentBot(tools=[my_tool])\n</code></pre>"},{"location":"getting-started/which-bot/#from-simplebot-to-querybot","title":"From SimpleBot to QueryBot","text":"<p>If you need to query documents:</p> <pre><code># Before: SimpleBot\nbot = lmb.SimpleBot(\"You are helpful.\")\n\n# After: QueryBot with documents\ndocstore = lmb.LanceDBDocStore(table_name=\"docs\")\ndocstore.extend(documents)\nbot = lmb.QueryBot(\n    system_prompt=\"You are helpful.\",\n    docstore=docstore\n)\n</code></pre>"},{"location":"getting-started/which-bot/#still-not-sure","title":"Still Not Sure?","text":"<p>If you're unsure which bot to use:</p> <ol> <li>Start with SimpleBot - It's the most flexible and can handle most basic use cases</li> <li>Add memory - If you need conversation context, add <code>ChatMemory</code></li> <li>Add tools - If you need function calling, switch to <code>ToolBot</code></li> <li>Add planning - If you need multi-step workflows, use <code>AgentBot</code></li> <li>Add documents - If you need RAG, use <code>QueryBot</code></li> </ol> <p>For most users, SimpleBot with ChatMemory covers 80% of use cases. Upgrade to specialized bots only when you need their specific features.</p>"},{"location":"how-to/cli-powered-by-llm/","title":"Cli Powered By Llm","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/ericmjl/llamabot/blob/main/docs/how-to/cli-powered-by-llm.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/cli-powered-by-llm/#how-to-build-an-llm-powered-cli","title":"How to Build an LLM-Powered CLI","text":"<p>Learn how to build a command-line interface that uses LLMs to generate structured outputs. This guide shows you how to create a CLI tool that automatically generates commit messages from git diffs using StructuredBot.</p>"},{"location":"how-to/cli-powered-by-llm/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Ollama installed and running locally: Visit ollama.ai to install</li> <li>Required Ollama model: Run <code>ollama pull gemma3n:latest</code> (or another model that supports structured outputs)</li> <li>Python 3.10+ with llamabot installed</li> <li>A git repository to test the CLI with</li> </ul> <p>All llamabot models in this guide use the <code>ollama_chat/</code> prefix for local execution.</p>"},{"location":"how-to/cli-powered-by-llm/#goal","title":"Goal","text":"<p>By the end of this guide, you'll have built a CLI command that:</p> <ul> <li>Takes a git diff as input</li> <li>Uses StructuredBot to generate a conventional commit message</li> <li>Returns a validated, structured commit message</li> <li>Can be integrated into git hooks for automatic commit message generation</li> </ul> <pre><code>from enum import Enum\n\nfrom pydantic import BaseModel, Field, model_validator\n\nimport llamabot as lmb\nfrom llamabot.bot.structuredbot import StructuredBot\nfrom llamabot.prompt_manager import prompt\n</code></pre>"},{"location":"how-to/cli-powered-by-llm/#step-1-define-your-data-schema","title":"Step 1: Define Your Data Schema","text":"<p>First, define the Pydantic model that represents your structured output. For commit messages, we'll use the conventional commit format.</p> <pre><code>class CommitType(str, Enum):\n    \"\"\"Type of commit following conventional commits.\"\"\"\n\n    fix = \"fix\"\n    feat = \"feat\"\n    build = \"build\"\n    chore = \"chore\"\n    ci = \"ci\"\n    docs = \"docs\"\n    style = \"style\"\n    refactor = \"refactor\"\n    perf = \"perf\"\n    test = \"test\"\n\nclass DescriptionEntry(BaseModel):\n    \"\"\"A single bullet point in the commit body.\"\"\"\n\n    txt: str = Field(\n        ...,\n        description=\"A single bullet point describing one major change in the commit.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_description(self):\n        \"\"\"Validate description length.\"\"\"\n        if len(self.txt) &gt; 160:\n            raise ValueError(\n                \"Description should be less than or equal to 160 characters.\"\n            )\n        return self\n\nclass CommitMessage(BaseModel):\n    \"\"\"Structured commit message following conventional commits format.\"\"\"\n\n    commit_type: CommitType = Field(\n        ...,\n        description=\"Type of change (fix, feat, docs, etc.)\",\n    )\n    scope: str = Field(\n        ...,\n        description=\"Scope of change (e.g., 'api', 'ui', 'auth')\",\n    )\n    description: str = Field(\n        ...,\n        description=\"Concise summary of what the commit accomplishes (present tense)\",\n    )\n    body: list[DescriptionEntry] = Field(\n        default_factory=list,\n        description=\"Optional detailed explanation as bullet points\",\n    )\n    breaking_change: bool = Field(\n        default=False,\n        description=\"Whether this commit introduces a breaking change\",\n    )\n</code></pre>"},{"location":"how-to/cli-powered-by-llm/#step-2-create-the-structuredbot","title":"Step 2: Create the StructuredBot","text":"<p>StructuredBot ensures the LLM output matches your Pydantic schema exactly. It automatically retries if validation fails.</p> <pre><code>@prompt(\"system\")\ndef commitbot_sysprompt() -&gt; str:\n    \"\"\"You are an expert software developer who writes excellent and accurate commit messages.\n    You will be given a git diff as input, and you will generate a structured commit message\n    following the conventional commits format. Ensure your output matches the provided schema exactly.\n    \"\"\"\n\ncommit_bot = StructuredBot(\n    system_prompt=commitbot_sysprompt(),\n    pydantic_model=CommitMessage,\n    model_name=\"ollama_chat/gemma3n:latest\",\n    stream_target=\"none\",\n)\n</code></pre>"},{"location":"how-to/cli-powered-by-llm/#step-3-test-the-bot","title":"Step 3: Test the Bot","text":"<p>Let's test the bot with a sample git diff to see how it generates structured commit messages.</p> <pre><code># Example git diff (in practice, you'd get this from `git diff --cached`)\nsample_diff = \"\"\"\ndiff --git a/src/api.py b/src/api.py\nindex 1234567..abcdefg 100644\n--- a/src/api.py\n+++ b/src/api.py\n@@ -10,6 +10,8 @@ def get_user(user_id: int):\n         raise ValueError(\"User ID must be positive\")\n     return db.query(User).filter(User.id == user_id).first()\n\n+def create_user(name: str, email: str):\n+    return db.add(User(name=name, email=email))\n+\n def delete_user(user_id: int):\n     db.query(User).filter(User.id == user_id).delete()\n\"\"\"\n\n# Generate commit message\ncommit_message = commit_bot(sample_diff)\ncommit_message\n</code></pre>"},{"location":"how-to/cli-powered-by-llm/#step-4-view-observability-with-spans","title":"Step 4: View Observability with Spans","text":"<p>StructuredBot automatically creates spans for observability. Let's see what information is tracked.</p> <pre><code># Display spans to see observability data\ncommit_bot.spans\n</code></pre> <p>The spans show:</p> <ul> <li>query: The input (git diff)</li> <li>model: Which model was used</li> <li>validation_attempts: How many times validation was attempted</li> <li>validation_success: Whether validation succeeded</li> <li>schema_fields: Fields in the Pydantic model</li> <li>duration_ms: How long the operation took</li> </ul> <p>This observability helps you debug issues and understand bot behavior.</p>"},{"location":"how-to/cli-powered-by-llm/#step-5-create-the-cli-command","title":"Step 5: Create the CLI Command","text":"<p>Now let's wrap this in a Typer CLI command that can be used from the terminal.</p> <pre><code>import subprocess\nfrom pathlib import Path\n\nimport typer\n</code></pre> <pre><code>app = typer.Typer()\n\n@app.command()\ndef compose():\n    \"\"\"Generate a commit message from the current git diff.\"\"\"\n    # Get the git diff\n    result = subprocess.run(\n        [\"git\", \"diff\", \"--cached\"],\n        capture_output=True,\n        text=True,\n    )\n\n    if not result.stdout.strip():\n        typer.echo(\n            \"No staged changes found. Stage some changes with `git add` first.\"\n        )\n        raise typer.Exit(1)\n\n    # Generate commit message\n    try:\n        commit_msg = commit_bot(result.stdout)\n\n        # Format the commit message\n        formatted = f\"{commit_msg.commit_type.value}({commit_msg.scope}){': ' if commit_msg.breaking_change else ': '}{commit_msg.description}\\n\\n\"\n        if commit_msg.body:\n            formatted += \"\\n\".join(f\"- {entry.txt}\" for entry in commit_msg.body)\n        if commit_msg.breaking_change:\n            formatted += (\n                \"\\n\\nBREAKING CHANGE: This commit introduces breaking changes.\"\n            )\n\n        typer.echo(formatted)\n\n        # Optionally write to .git/COMMIT_EDITMSG\n        commit_editmsg = Path(\".git/COMMIT_EDITMSG\")\n        if commit_editmsg.parent.exists():\n            commit_editmsg.write_text(formatted)\n            typer.echo(f\"\\nCommit message written to {commit_editmsg}\")\n\n    except Exception as e:\n        typer.echo(f\"Error generating commit message: {e}\", err=True)\n        raise typer.Exit(1)\n</code></pre>"},{"location":"how-to/cli-powered-by-llm/#step-6-test-the-cli","title":"Step 6: Test the CLI","text":"<p>You can now use this CLI command. In a real implementation, you'd register it with your main CLI app. For testing, you can call the function directly.</p>"},{"location":"how-to/cli-powered-by-llm/#step-7-integrate-with-git-hooks-optional","title":"Step 7: Integrate with Git Hooks (Optional)","text":"<p>To automatically generate commit messages, you can create a git hook:</p> <pre><code># Create the hook\ncat &gt; .git/hooks/prepare-commit-msg &lt;&lt; 'EOF'\n#!/bin/sh\nllamabot git compose\nEOF\n\nchmod +x .git/hooks/prepare-commit-msg\n</code></pre> <p>Now when you run <code>git commit</code> without a message, it will automatically generate one.</p>"},{"location":"how-to/cli-powered-by-llm/#summary","title":"Summary","text":"<p>You've built an LLM-powered CLI that:</p> <ul> <li>Uses StructuredBot to ensure validated, structured outputs</li> <li>Integrates with git to generate commit messages automatically</li> <li>Provides observability through spans</li> <li>Handles validation retries automatically</li> </ul> <p>Key Takeaways:</p> <ul> <li>Define your Pydantic schema first</li> <li>Use StructuredBot for guaranteed schema compliance</li> <li>Leverage spans for debugging and observability</li> <li>Wrap bots in CLI commands for easy terminal access</li> </ul>"},{"location":"how-to/data-analysis-agentbot/","title":"Data Analysis Agentbot","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/ericmjl/llamabot/blob/main/docs/how-to/data-analysis-agentbot.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#how-to-build-a-data-analysis-chatbot-with-agentbot","title":"How to Build a Data Analysis Chatbot with AgentBot","text":"<p>Learn how to build a chatbot that executes code for data analysis using AgentBot. Unlike ToolBot which handles single-turn function calls, AgentBot can orchestrate multi-step workflows and make decisions about which tools to use.</p>"},{"location":"how-to/data-analysis-agentbot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Ollama installed and running locally: Visit ollama.ai to install</li> <li>Required Ollama model: Run <code>ollama pull deepseek-r1:32b</code> (or another model that supports tool calling)</li> <li>Python 3.10+ with llamabot, pandas, and numpy installed</li> <li>Sample data to analyze (or we'll create some in this guide)</li> </ul> <p>All llamabot models in this guide use the <code>ollama_chat/</code> prefix for local execution.</p>"},{"location":"how-to/data-analysis-agentbot/#goal","title":"Goal","text":"<p>By the end of this guide, you'll have built a data analysis chatbot that:</p> <ul> <li>Executes Python code to analyze data</li> <li>Makes multi-step decisions about which analyses to perform</li> <li>Returns DataFrames and visualizations</li> <li>Provides observability through spans and workflow visualization</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\nimport llamabot as lmb\nfrom llamabot.bot.agentbot import AgentBot\nfrom llamabot.components.tools import tool\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#step-1-create-sample-data","title":"Step 1: Create Sample Data","text":"<p>Let's create some sample data to analyze. In a real scenario, you'd load your own data.</p> <pre><code># Create sample sales data\nnp.random.seed(42)\ndates = pd.date_range(\"2024-01-01\", periods=100, freq=\"D\")\nsales_data = pd.DataFrame(\n    {\n        \"date\": dates,\n        \"product\": np.random.choice([\"Widget A\", \"Widget B\", \"Widget C\"], 100),\n        \"sales\": np.random.randint(10, 100, 100),\n        \"revenue\": np.random.uniform(100, 1000, 100),\n        \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], 100),\n    }\n)\n\nsales_data.head()\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#step-2-create-data-analysis-tools","title":"Step 2: Create Data Analysis Tools","text":"<p>We'll create tools that the agent can use to analyze data. Each tool is decorated with <code>@tool</code> to make it agent-callable.</p> <pre><code>@tool\ndef calculate_statistics(\n    dataframe_name: str, column: str, _globals_dict: dict = None\n) -&gt; str:\n    \"\"\"Calculate basic statistics (mean, median, std) for a column in a DataFrame.\n\n    :param dataframe_name: Name of the DataFrame variable in globals\n    :param column: Name of the column to analyze\n    :param _globals_dict: Internal parameter - automatically injected by AgentBot\n    :return: String summary of statistics\n    \"\"\"\n    if _globals_dict is None or dataframe_name not in _globals_dict:\n        return f\"DataFrame '{dataframe_name}' not found in workspace.\"\n\n    df = _globals_dict[dataframe_name]\n    if column not in df.columns:\n        return f\"Column '{column}' not found in DataFrame.\"\n\n    stats = {\n        \"mean\": df[column].mean(),\n        \"median\": df[column].median(),\n        \"std\": df[column].std(),\n        \"min\": df[column].min(),\n        \"max\": df[column].max(),\n    }\n\n    return f\"Statistics for {column}:\\n\" + \"\\n\".join(\n        f\"  {k}: {v:.2f}\" for k, v in stats.items()\n    )\n</code></pre> <pre><code>@tool\ndef group_by_analysis(\n    dataframe_name: str,\n    group_by: str,\n    aggregate_column: str,\n    _globals_dict: dict = None,\n) -&gt; str:\n    \"\"\"Group DataFrame by a column and aggregate another column.\n\n    :param dataframe_name: Name of the DataFrame variable in globals\n    :param group_by: Column name to group by\n    :param aggregate_column: Column name to aggregate\n    :param _globals_dict: Internal parameter - automatically injected by AgentBot\n    :return: String representation of grouped results\n    \"\"\"\n    if _globals_dict is None or dataframe_name not in _globals_dict:\n        return f\"DataFrame '{dataframe_name}' not found in workspace.\"\n\n    df = _globals_dict[dataframe_name]\n    if group_by not in df.columns or aggregate_column not in df.columns:\n        return \"One or more columns not found in DataFrame.\"\n\n    grouped = (\n        df.groupby(group_by)[aggregate_column]\n        .agg([\"sum\", \"mean\", \"count\"])\n        .round(2)\n    )\n    return f\"Grouped analysis by {group_by}:\\n{grouped.to_string()}\"\n</code></pre> <pre><code>@tool\ndef execute_custom_code(code: str, _globals_dict: dict = None) -&gt; str:\n    \"\"\"Execute custom Python code for data analysis.\n\n    This tool allows the agent to execute arbitrary Python code for complex analyses.\n    Use this when standard tools aren't sufficient.\n\n    :param code: Python code to execute (must be safe and data-focused)\n    :param _globals_dict: Internal parameter - automatically injected by AgentBot\n    :return: String representation of the result\n    \"\"\"\n    if _globals_dict is None:\n        return \"No workspace available.\"\n\n    try:\n        # Execute code with access to globals (including DataFrames)\n        exec(code, _globals_dict)\n        return \"Code executed successfully. Check workspace for results.\"\n    except Exception as e:\n        return f\"Error executing code: {str(e)}\"\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#step-3-create-the-agentbot","title":"Step 3: Create the AgentBot","text":"<p>AgentBot orchestrates multiple tools and makes decisions about which ones to use. It uses a graph-based workflow where tools can loop back to the decision node.</p> <pre><code># Create AgentBot with our data analysis tools\nanalysis_agent = AgentBot(\n    tools=[calculate_statistics, group_by_analysis, execute_custom_code],\n    system_prompt=\"\"\"You are a data analysis assistant. You help users analyze data by:\n    1. Understanding what analysis they want\n    2. Selecting the appropriate tool(s) to use\n    3. Executing multi-step analyses when needed\n    4. Returning clear, informative results\n\n    Available tools:\n    - calculate_statistics: Get basic stats for a column\n    - group_by_analysis: Group and aggregate data\n    - execute_custom_code: Run custom Python code for complex analyses\n\n    Always use return_object_to_user() to return DataFrames or results to the user.\n    \"\"\",\n    model_name=\"ollama_chat/deepseek-r1:32b\",\n)\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#step-4-visualize-the-agent-workflow","title":"Step 4: Visualize the Agent Workflow","text":"<p>AgentBot automatically generates a mermaid diagram showing the workflow graph. Blue nodes are tools that loop back to the decision node, green nodes are terminal tools.</p> <pre><code># Display the agent to see the workflow graph\nanalysis_agent\n</code></pre> <p>The mermaid diagram shows:</p> <ul> <li>Decision node: Where the agent decides which tool to use</li> <li>Tool nodes (blue): Tools that can loop back for multi-step workflows</li> <li>Terminal nodes (green): Tools like <code>respond_to_user</code> that end the workflow</li> </ul> <p>This visualization helps you understand how the agent orchestrates tools.</p>"},{"location":"how-to/data-analysis-agentbot/#step-5-use-the-agent-for-data-analysis","title":"Step 5: Use the Agent for Data Analysis","text":"<p>Now let's use the agent to analyze our data. The agent will decide which tools to use and can perform multi-step analyses.</p> <pre><code># Use the agent to analyze data\n# The agent will decide which tools to use based on the query\nresult = analysis_agent(\n    \"Calculate the mean and standard deviation of sales, then group by region and show total revenue per region.\",\n    globals(),\n)\n\nprint(result)\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#step-6-view-observability-with-spans","title":"Step 6: View Observability with Spans","text":"<p>AgentBot creates spans that track the entire workflow, including decision-making and tool execution.</p> <pre><code># Display spans to see the agent's decision-making process\nanalysis_agent.spans\n</code></pre> <p>The spans show:</p> <ul> <li>agentbot_call: The main agent call with query and max_iterations</li> <li>iterations: How many tool calls were made</li> <li>result: The final result</li> <li>Nested spans: Each tool execution creates its own span</li> </ul> <p>This observability helps you understand:</p> <ul> <li>Which tools the agent chose to use</li> <li>How many steps were needed</li> <li>What decisions were made at each step</li> </ul>"},{"location":"how-to/data-analysis-agentbot/#step-7-create-an-interactive-chat-interface","title":"Step 7: Create an Interactive Chat Interface","text":"<p>Let's create a Marimo chat interface so users can interact with the agent naturally.</p> <pre><code>def chat_turn(messages, config):\n    \"\"\"Handle a chat turn with the data analysis agent.\"\"\"\n    user_message = messages[-1].content\n\n    # Make sure sales_data is available in globals\n    globals_dict = {\"sales_data\": sales_data}\n\n    # Call the agent\n    result = analysis_agent(user_message, globals_dict)\n\n    return result\n\n# Create chat interface with example prompts\nexample_prompts = [\n    \"What's the average sales by product?\",\n    \"Show me total revenue per region\",\n    \"Calculate statistics for the sales column\",\n]\n\nchat = mo.ui.chat(chat_turn, max_height=600, prompts=example_prompts)\n</code></pre> <pre><code>mo.vstack(\n    [\n        mo.md(\"### Data Analysis Agent\"),\n        mo.md(\n            \"Ask questions about the sales data. The agent will decide which tools to use.\"\n        ),\n        chat,\n    ]\n)\n</code></pre>"},{"location":"how-to/data-analysis-agentbot/#summary","title":"Summary","text":"<p>You've built a data analysis chatbot with AgentBot that:</p> <ul> <li>Executes code for data analysis</li> <li>Makes multi-step decisions about which tools to use</li> <li>Orchestrates complex workflows automatically</li> <li>Provides workflow visualization through mermaid diagrams</li> <li>Tracks observability through spans</li> <li>Offers an interactive chat interface</li> </ul> <p>Key Takeaways:</p> <ul> <li>AgentBot orchestrates multiple tools in a graph-based workflow</li> <li>Tools decorated with <code>@tool</code> become agent-callable</li> <li>Display the agent to see the workflow graph visualization</li> <li>Spans track decision-making and tool execution</li> <li>AgentBot can handle multi-step workflows automatically</li> <li>Use <code>globals_dict</code> to share data between tool calls</li> <li>Terminal tools (like <code>respond_to_user</code>) end the workflow</li> </ul>"},{"location":"how-to/generate-blog-banner-images/","title":"Generate Blog Banner Images","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/ericmjl/llamabot/blob/main/docs/how-to/generate-blog-banner-images.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#how-to-generate-blog-banner-images-with-imagebot","title":"How to Generate Blog Banner Images with ImageBot","text":"<p>Learn how to use ImageBot and StructuredBot together to automatically generate beautiful banner images for blog posts. This guide shows you how to create a workflow that generates DALL-E prompts from blog content and then creates custom banner images.</p>"},{"location":"how-to/generate-blog-banner-images/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>OpenAI API key: Set up your OpenAI API key (ImageBot uses DALL-E)</li> <li>Python 3.10+ with llamabot installed</li> <li>A blog post or text content to generate a banner for</li> </ul> <p>Note: ImageBot requires an OpenAI API key with access to DALL-E models. Make sure your API key is configured in your environment.</p>"},{"location":"how-to/generate-blog-banner-images/#goal","title":"Goal","text":"<p>By the end of this guide, you'll have built a workflow that:</p> <ul> <li>Takes blog post text as input</li> <li>Uses StructuredBot to generate a detailed DALL-E prompt</li> <li>Uses ImageBot to generate a banner image (16:4 aspect ratio)</li> <li>Creates beautiful, watercolor-style banner images for your blog posts</li> </ul> <pre><code>from pydantic import BaseModel, Field\n\nimport llamabot as lmb\nfrom llamabot.bot.imagebot import ImageBot\nfrom llamabot.bot.structuredbot import StructuredBot\nfrom llamabot.prompt_manager import prompt\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-1-define-the-dall-e-prompt-schema","title":"Step 1: Define the DALL-E Prompt Schema","text":"<p>First, define a Pydantic model to structure the DALL-E image generation prompt. This ensures we get well-formatted prompts that work well with DALL-E.</p> <pre><code>class DallEImagePrompt(BaseModel):\n    \"\"\"Structured prompt for DALL-E image generation.\"\"\"\n\n    content: str = Field(\n        ...,\n        description=\"A detailed, descriptive prompt for generating a banner image for the blog post.\",\n    )\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-2-create-the-prompt-generator-bot","title":"Step 2: Create the Prompt Generator Bot","text":"<p>Create a StructuredBot that generates detailed DALL-E prompts from blog post text. This bot will translate the key concepts and themes from your blog post into a visual description suitable for image generation.</p> <pre><code>@prompt(\"system\")\ndef bannerbot_dalle_prompter_sysprompt() -&gt; str:\n    \"\"\"You are a prompt designer for DALL-E image generation.\n\n    Your role is to create highly detailed and imaginative prompts for DALL-E,\n    designed to generate banner images for blog posts in a watercolor style,\n    with a 16:4 aspect ratio.\n\n    You will be given a chunk of text or a summary that comes from the blog post.\n    Your task is to translate the key concepts, ideas, and themes from the text\n    into an image prompt.\n\n    **Guidelines for creating the prompt:**\n    - Use vivid and descriptive language to specify the image's mood, colors,\n      composition, and style.\n    - Vary your approach significantly between prompts - avoid repetitive patterns,\n      elements, or compositions that could make images look similar.\n    - Explore diverse watercolor techniques: washes, wet-on-wet, dry brush,\n      salt effects, splattering, or layered glazes.\n    - Consider different artistic styles within watercolor: impressionistic,\n      expressionistic, minimalist, detailed botanical, atmospheric, or abstract.\n    - Vary the color palettes: warm vs cool tones, monochromatic vs complementary,\n      muted vs vibrant, or seasonal color schemes.\n    - Mix different compositional approaches: centered focal points, rule of thirds,\n      diagonal compositions, or asymmetrical balance.\n    - Incorporate varied symbolic elements: natural objects, architectural forms,\n      organic shapes, geometric patterns, or conceptual representations.\n    - Focus on maximizing the use of imagery and symbols to represent ideas,\n      avoiding any inclusion of text or character symbols in the image.\n    - If the text is vague or lacks detail, make thoughtful and creative assumptions\n      to create a compelling visual representation.\n\n    The prompt should be suitable for a variety of blog topics,\n    evoking an emotional or intellectual connection to the content.\n    Ensure the description specifies the watercolor art style,\n    the wide 16:4 banner aspect ratio,\n    and your chosen artistic approach.\n\n    Do **NOT** include any text or character symbols in the image description.\n    \"\"\"\n\ndalle_prompt_bot = StructuredBot(\n    system_prompt=bannerbot_dalle_prompter_sysprompt(),\n    pydantic_model=DallEImagePrompt,\n    model_name=\"gpt-4o\",\n)\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-3-create-the-imagebot","title":"Step 3: Create the ImageBot","text":"<p>Create an ImageBot configured for banner images. We'll use a 16:4 aspect ratio (1792x1024 pixels) which is perfect for blog banners.</p> <pre><code>bannerbot = ImageBot(size=\"1792x1024\", quality=\"hd\")\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-4-generate-a-banner-image","title":"Step 4: Generate a Banner Image","text":"<p>Now let's put it all together! We'll: 1. Generate a DALL-E prompt from blog post text 2. Use that prompt to generate a banner image 3. Display the result</p> <pre><code># Example blog post text\nblog_post_text = \"\"\"\nIn this blog post, we explore how to build AI agents that can reason about\ncomplex problems. We'll dive into graph-based agent architectures, tool\norchestration, and multi-step planning. Learn how to create agents that\ncan break down complex tasks into manageable steps and execute them\nsystematically.\n\"\"\"\n\n# Generate the DALL-E prompt\ndalle_prompt = dalle_prompt_bot(blog_post_text)\ndalle_prompt.content\n</code></pre> <pre><code># Generate the banner image\nbanner_url = bannerbot(dalle_prompt.content, return_url=True)\nbanner_url\n</code></pre> <pre><code># Display the generated banner image\nmo.image(banner_url)\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-5-save-the-image-locally","title":"Step 5: Save the Image Locally","text":"<p>You can also save the image to a file instead of just getting the URL. ImageBot will automatically generate a filename from the prompt if you don't specify a save path.</p> <pre><code>from pathlib import Path\n\n# Save the image to a specific path\nimage_path = bannerbot(\n    dalle_prompt.content,\n    save_path=Path(\"blog_banner.jpg\"),\n)\nimage_path\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-6-create-a-complete-workflow-function","title":"Step 6: Create a Complete Workflow Function","text":"<p>Let's create a reusable function that takes blog text and generates a banner image. This makes it easy to integrate into your blog publishing workflow.</p> <pre><code>def generate_blog_banner(\n    blog_text: str, save_path: Path | None = None\n) -&gt; str | Path:\n    \"\"\"Generate a banner image for a blog post.\n\n    :param blog_text: The text content of the blog post\n    :param save_path: Optional path to save the image. If None, returns URL.\n    :return: URL if save_path is None, otherwise the path to saved image\n    \"\"\"\n    # Step 1: Generate DALL-E prompt from blog text\n    dalle_prompt = dalle_prompt_bot(blog_text)\n\n    # Step 2: Generate the banner image\n    if save_path:\n        image_path = bannerbot(dalle_prompt.content, save_path=save_path)\n        return image_path\n    else:\n        banner_url = bannerbot(dalle_prompt.content, return_url=True)\n        return banner_url\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#step-7-test-with-different-blog-posts","title":"Step 7: Test with Different Blog Posts","text":"<p>Try generating banners for different types of blog posts to see how the prompt generator adapts to different topics and styles.</p> <pre><code># Example 1: Technical blog post\ntechnical_post = \"\"\"\nLearn how to optimize your Python code for performance. We'll cover\nprofiling techniques, optimization strategies, and best practices for\nwriting efficient Python code.\n\"\"\"\n\ntechnical_banner_url = generate_blog_banner(technical_post)\nmo.md(f\"**Technical Post Banner:**\")\nmo.image(technical_banner_url)\n</code></pre>"},{"location":"how-to/generate-blog-banner-images/#customization-options","title":"Customization Options","text":"<p>You can customize the banner generation in several ways:</p> <ul> <li>Change the aspect ratio: Modify the <code>size</code> parameter in ImageBot   (e.g., \"1024x1024\" for square, \"1024x1792\" for portrait)</li> <li>Adjust the style: Modify the system prompt to use different art styles   (e.g., digital art, photography, illustration)</li> <li>Change image quality: Use <code>quality=\"standard\"</code> for faster/cheaper generation   or <code>quality=\"hd\"</code> for higher quality (default)</li> <li>Modify the prompt generator: Adjust the system prompt to emphasize   different visual elements or styles</li> </ul>"},{"location":"how-to/generate-blog-banner-images/#summary","title":"Summary","text":"<p>You've built a complete workflow for generating blog banner images that:</p> <ul> <li>Uses StructuredBot to create detailed, structured DALL-E prompts</li> <li>Uses ImageBot to generate high-quality banner images</li> <li>Supports both URL-based and file-based image generation</li> <li>Can be easily integrated into blog publishing workflows</li> </ul> <p>Key Takeaways:</p> <ul> <li>Combine StructuredBot and ImageBot for AI-powered image generation</li> <li>Use detailed system prompts to guide the prompt generation</li> <li>Configure ImageBot with appropriate aspect ratios for your use case</li> <li>Create reusable functions to integrate into your workflows</li> </ul> <p>Next Steps:</p> <ul> <li>Integrate this into your blog publishing pipeline</li> <li>Experiment with different art styles and prompt variations</li> <li>Consider caching generated prompts for similar blog posts</li> <li>Add error handling and retry logic for production use</li> </ul>"},{"location":"how-to/generate-blog-banner-images/#interactive-style-customization-madlib-ui","title":"Interactive Style Customization (Madlib UI)","text":"<p>Now let's create an interactive interface that lets you experiment with different artistic styles for the same blog post. Use the controls below to customize the watercolor technique, artistic style, color palette, composition, and symbolic elements, then generate a new banner image with your selected styles.</p> <pre><code>watercolor_techniques = [\n    \"washes\",\n    \"wet-on-wet\",\n    \"dry brush\",\n    \"salt effects\",\n    \"splattering\",\n    \"layered glazes\",\n]\n\nartistic_styles = [\n    \"impressionistic\",\n    \"expressionistic\",\n    \"minimalist\",\n    \"detailed botanical\",\n    \"atmospheric\",\n    \"abstract\",\n]\n\ncolor_palettes = [\n    \"warm tones\",\n    \"cool tones\",\n    \"monochromatic\",\n    \"complementary\",\n    \"muted\",\n    \"vibrant\",\n    \"seasonal color scheme\",\n]\n\ncompositional_approaches = [\n    \"centered focal points\",\n    \"rule of thirds\",\n    \"diagonal compositions\",\n    \"asymmetrical balance\",\n]\n\nsymbolic_elements = [\n    \"natural objects\",\n    \"architectural forms\",\n    \"organic shapes\",\n    \"geometric patterns\",\n    \"conceptual representations\",\n]\n</code></pre> <pre><code>mo.md(\"### Customize Your Banner Style\")\n\ntechnique_selector = mo.ui.dropdown(\n    options=watercolor_techniques,\n    value=\"wet-on-wet\",\n    label=\"Watercolor Technique\",\n)\n\nstyle_selector = mo.ui.dropdown(\n    options=artistic_styles,\n    value=\"atmospheric\",\n    label=\"Artistic Style\",\n)\n\npalette_selector = mo.ui.dropdown(\n    options=color_palettes,\n    value=\"complementary\",\n    label=\"Color Palette\",\n)\n\ncomposition_selector = mo.ui.dropdown(\n    options=compositional_approaches,\n    value=\"rule of thirds\",\n    label=\"Compositional Approach\",\n)\n\nsymbol_selector = mo.ui.dropdown(\n    options=symbolic_elements,\n    value=\"conceptual representations\",\n    label=\"Symbolic Elements\",\n)\n\nmo.vstack(\n    [\n        technique_selector,\n        style_selector,\n        palette_selector,\n        composition_selector,\n        symbol_selector,\n    ]\n)\n</code></pre> <pre><code>@prompt(\"system\")\ndef styled_bannerbot_dalle_prompter_sysprompt(\n    technique: str,\n    style: str,\n    palette: str,\n    composition: str,\n    symbols: str,\n) -&gt; str:\n    \"\"\"You are a prompt designer for DALL-E image generation.\n\n    Your role is to create highly detailed and imaginative prompts for DALL-E,\n    designed to generate banner images for blog posts in a watercolor style,\n    with a 16:4 aspect ratio.\n\n    You will be given a chunk of text or a summary that comes from the blog post.\n    Your task is to translate the key concepts, ideas, and themes from the text\n    into an image prompt.\n\n    **Style Requirements:**\n    - Watercolor technique: {{ technique }}\n    - Artistic style: {{ style }}\n    - Color palette: {{ palette }}\n    - Compositional approach: {{ composition }}\n    - Symbolic elements: {{ symbols }}\n\n    **Guidelines for creating the prompt:**\n    - Use vivid and descriptive language to specify the image's mood, colors,\n      composition, and style.\n    - Incorporate the specified watercolor technique ({{ technique }}) prominently\n      in your description.\n    - Apply the {{ style }} artistic style throughout the image.\n    - Use a {{ palette }} color palette as the foundation for the image.\n    - Structure the composition using {{ composition }}.\n    - Feature {{ symbols }} as the primary symbolic elements.\n    - Focus on maximizing the use of imagery and symbols to represent ideas,\n      avoiding any inclusion of text or character symbols in the image.\n    - If the text is vague or lacks detail, make thoughtful and creative assumptions\n      to create a compelling visual representation.\n\n    The prompt should be suitable for a variety of blog topics,\n    evoking an emotional or intellectual connection to the content.\n    Ensure the description specifies the watercolor art style,\n    the wide 16:4 banner aspect ratio,\n    and incorporates all the specified style elements.\n\n    Do **NOT** include any text or character symbols in the image description.\n    \"\"\"\n\nstyled_dalle_prompt_bot = StructuredBot(\n    system_prompt=styled_bannerbot_dalle_prompter_sysprompt(\n        technique=technique_selector.value,\n        style=style_selector.value,\n        palette=palette_selector.value,\n        composition=composition_selector.value,\n        symbols=symbol_selector.value,\n    ),\n    pydantic_model=DallEImagePrompt,\n    model_name=\"gpt-4o\",\n)\n</code></pre> <pre><code># Generate the DALL-E prompt with custom styles\nstyled_dalle_prompt = styled_dalle_prompt_bot(blog_post_text)\nstyled_dalle_prompt.content\n</code></pre> <pre><code># Generate the banner image with custom styles\nstyled_banner_url = bannerbot(styled_dalle_prompt.content, return_url=True)\nstyled_banner_url\n</code></pre> <pre><code># Display the generated banner image with custom styles\nmo.image(styled_banner_url)\n</code></pre>"},{"location":"how-to/receipt-processing/","title":"Receipt Processing","text":"<p>To run this notebook, click on the molab shield above or run the following command at the terminal:</p> <pre><code>uvx marimo edit --sandbox --mcp --no-token --watch https://github.com/ericmjl/llamabot/blob/main/docs/how-to/receipt-processing.py\n</code></pre> <pre><code>import marimo as mo\n</code></pre>"},{"location":"how-to/receipt-processing/#how-to-process-receipts-with-llm-agents","title":"How to Process Receipts with LLM Agents","text":"<p>Learn how to extract structured data from receipt PDFs and images using a two-step OCR and structuring pattern with llamabot's SimpleBot and StructuredBot.</p>"},{"location":"how-to/receipt-processing/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Ollama installed and running locally: Visit ollama.ai to install</li> <li>Required Ollama models:</li> <li><code>ollama pull deepseek-ocr</code> (for OCR text extraction)</li> <li><code>ollama pull gemma3n:latest</code> (for structured output, or another model that supports structured outputs)</li> <li>Python 3.10+ with llamabot and pdf2image installed</li> <li>A receipt PDF or image to process (or use the example provided)</li> </ul> <p>All llamabot models in this guide use the <code>ollama/</code> or <code>ollama_chat/</code> prefix for local execution.</p>"},{"location":"how-to/receipt-processing/#goal","title":"Goal","text":"<p>By the end of this guide, you'll have built a receipt processing system that:</p> <ul> <li>Converts receipt PDFs to images</li> <li>Extracts text using vision models (OCR)</li> <li>Structures the extracted data into a validated Pydantic model</li> <li>Provides observability through spans</li> </ul> <pre><code>from pathlib import Path\nimport tempfile\n\nfrom pdf2image import convert_from_path\nfrom pydantic import BaseModel, Field\n\nimport llamabot as lmb\nfrom llamabot import get_current_span, span\nfrom llamabot.bot.structuredbot import StructuredBot\nfrom llamabot.components.messages import user\nfrom llamabot.prompt_manager import prompt\n</code></pre>"},{"location":"how-to/receipt-processing/#step-1-define-your-receipt-data-schema","title":"Step 1: Define Your Receipt Data Schema","text":"<p>First, define the Pydantic model that represents the structured receipt data. This schema must be defined before building the extraction agent.</p> <pre><code>class ReceiptData(BaseModel):\n    \"\"\"Receipt data schema - must be defined BEFORE building extraction agent.\"\"\"\n\n    vendor: str = Field(..., description=\"The name of the vendor/merchant\")\n    date: str = Field(..., description=\"The transaction date in YYYY-MM-DD format\")\n    amount: float = Field(\n        ..., description=\"The total amount as a number (without currency symbols)\"\n    )\n    category: str = Field(\n        ...,\n        description=\"Business category (e.g., 'Office Supplies', 'Travel', 'Meals', 'Software', 'Equipment')\",\n    )\n    description: str = Field(\n        ..., description=\"Brief description of what was purchased\"\n    )\n</code></pre>"},{"location":"how-to/receipt-processing/#step-2-create-the-two-step-processing-bots","title":"Step 2: Create the Two-Step Processing Bots","text":"<p>We use a two-step pattern because vision models like DeepSeek-OCR excel at OCR but don't necessarily support structured outputs. The solution:</p> <ol> <li>OCR Step (SimpleBot): Extract text from images using vision models</li> <li>Structuring Step (StructuredBot): Convert unstructured text to validated Pydantic models</li> </ol> <pre><code>@prompt(\"system\")\ndef receipt_extraction_sysprompt() -&gt; str:\n    \"\"\"You are an expert at extracting financial information from receipt and invoice documents.\n\n    Extract the following information accurately:\n\n    - vendor: The name of the vendor/merchant\n    - date: The transaction date in YYYY-MM-DD format\n    - amount: The total amount as a number (without currency symbols)\n    - category: Business category (e.g., \"Office Supplies\", \"Travel\", \"Meals\", \"Software\", \"Equipment\")\n    - description: Brief description of what was purchased\n\n    If any field is unclear or missing, use your best judgment based on the context.\n    For dates, convert any format to YYYY-MM-DD. For amounts, extract only the numerical value.\n    \"\"\"\n\n# Step 1: OCR extraction with DeepSeek-OCR (SimpleBot)\n# DeepSeek-OCR doesn't support structured outputs, so we use SimpleBot\nocr_bot = lmb.SimpleBot(\n    system_prompt=\"Extract all text from receipts accurately. \"\n    \"Preserve the structure and include all numbers, dates, and vendor names.\",\n    model_name=\"ollama/deepseek-ocr\",\n    stream_target=\"none\",\n)\n\n# Step 2: Structure the data (using a model that supports structured outputs)\nreceipt_structuring_bot = StructuredBot(\n    system_prompt=receipt_extraction_sysprompt(),\n    pydantic_model=ReceiptData,\n    model_name=\"ollama_chat/gemma3n:latest\",\n    stream_target=\"none\",\n)\n</code></pre>"},{"location":"how-to/receipt-processing/#step-3-create-pdf-to-image-converter-with-spans","title":"Step 3: Create PDF to Image Converter with Spans","text":"<p>Let's create a function that converts PDFs to images, using spans for observability.</p> <pre><code>@span\ndef convert_pdf_to_images(file_path: str):\n    \"\"\"Convert PDF to list of image paths.\"\"\"\n    s = get_current_span()\n    s[\"file_path\"] = file_path\n    file_extension = Path(file_path).suffix.lower()\n    s[\"file_extension\"] = file_extension\n\n    if file_extension == \".pdf\":\n        images = convert_from_path(file_path, dpi=200)\n        image_paths = []\n        for i, image in enumerate(images):\n            with tempfile.NamedTemporaryFile(\n                delete=False, suffix=f\"_page_{i + 1}.png\"\n            ) as temp_img:\n                image.save(temp_img.name, \"PNG\")\n                image_paths.append(temp_img.name)\n        s[\"page_count\"] = len(image_paths)\n        s[\"conversion_success\"] = True\n        return image_paths\n    elif file_extension in [\".png\", \".jpg\", \".jpeg\"]:\n        s[\"page_count\"] = 1\n        s[\"conversion_success\"] = True\n        return [file_path]\n    else:\n        s[\"conversion_success\"] = False\n        raise ValueError(f\"Unsupported file type: {file_extension}\")\n</code></pre>"},{"location":"how-to/receipt-processing/#step-4-process-a-receipt","title":"Step 4: Process a Receipt","text":"<p>Now let's process a receipt through the complete workflow:</p> <ol> <li>Convert PDF to images</li> <li>Extract text with OCR</li> <li>Structure the data</li> </ol> <pre><code># Example: Process a receipt\n# Replace with your own receipt file path\nreceipt_path = \"./receipt_lunch.pdf\"  # Or use your own: \"/path/to/your/receipt.pdf\"\n\n# Step 1: Convert PDF to images\nimage_paths = convert_pdf_to_images(receipt_path)\nprint(f\"Converted to {len(image_paths)} image(s)\")\n\n# Step 2: Extract text with OCR\nocr_texts = []\nfor image_path in image_paths:\n    ocr_response = ocr_bot(\n        user(\"Extract all text from this receipt image.\", image_path)\n    )\n    ocr_texts.append(ocr_response.content)\nprint(f\"Extracted text from {len(ocr_texts)} page(s)\")\n\n# Step 3: Structure the extracted text\ncombined_ocr_text = \"\\n\\n--- Page Break ---\\n\\n\".join(ocr_texts)\nreceipt_data = receipt_structuring_bot(combined_ocr_text)\nprint(f\"Structured data: {receipt_data.model_dump_json(indent=2)}\")\nreceipt_data\n</code></pre>"},{"location":"how-to/receipt-processing/#step-5-view-observability-with-spans","title":"Step 5: View Observability with Spans","text":"<p>Both bots automatically create spans for observability. Let's see what information is tracked.</p> <pre><code># Display spans from both bots\nprint(\"OCR Bot Spans:\")\nocr_bot.spans\n\nprint(\"\\n\\nReceipt Structuring Bot Spans:\")\nreceipt_structuring_bot.spans\n</code></pre> <p>The spans show:</p> <ul> <li>OCR Bot: query, model, input_message_count, duration_ms</li> <li>Structuring Bot: query, model, validation_attempts, validation_success, schema_fields, duration_ms</li> </ul> <p>You can also see nested spans from the <code>convert_pdf_to_images</code> function showing:</p> <ul> <li>file_path, file_extension, page_count, conversion_success</li> </ul> <p>This observability helps you debug issues and understand the workflow execution.</p>"},{"location":"how-to/receipt-processing/#step-6-create-a-complete-receipt-processing-function","title":"Step 6: Create a Complete Receipt Processing Function","text":"<p>Let's combine everything into a single function that can be used as a tool.</p> <pre><code>from llamabot.components.tools import tool\n</code></pre> <pre><code>@tool\ndef process_receipt(file_path: str, _globals_dict: dict = None) -&gt; str:\n    \"\"\"Process a receipt PDF or image and extract structured data.\n\n    This tool demonstrates that agents can have read access to the local file system.\n    Simply provide a file path and the tool will read it from disk.\n\n    :param file_path: Path to the receipt file (PDF, PNG, JPG, or JPEG)\n    :param _globals_dict: Internal parameter - automatically injected by AgentBot\n    :return: JSON string of extracted receipt data\n    \"\"\"\n    # Access current span to add attributes\n    s = get_current_span()\n    s[\"file_path\"] = file_path\n\n    # Verify the file exists\n    if not Path(file_path).exists():\n        raise FileNotFoundError(f\"Receipt file not found: {file_path}\")\n\n    # PDF to image conversion\n    image_paths = convert_pdf_to_images(file_path)\n    s[\"page_count\"] = len(image_paths)\n\n    if len(image_paths) == 1:\n        prompt_text = \"Extract all text from this receipt image.\"\n    else:\n        prompt_text = (\n            f\"Extract all text from this {len(image_paths)}-page receipt document.\"\n        )\n\n    # Step 1: OCR extraction - extract text from images\n    ocr_texts = []\n    for image_path in image_paths:\n        ocr_response = ocr_bot(user(prompt_text, image_path))\n        ocr_texts.append(ocr_response.content)\n    s.log(\"ocr_completed\", pages=len(image_paths))\n\n    # Combine OCR results from all pages\n    combined_ocr_text = \"\\n\\n--- Page Break ---\\n\\n\".join(ocr_texts)\n\n    # Step 2: Structure the extracted text according to ReceiptData schema\n    result = receipt_structuring_bot(combined_ocr_text)\n    s.log(\"structuring_completed\")\n    s[\"vendor\"] = result.vendor\n    s[\"amount\"] = result.amount\n\n    # Store ReceiptData object in globals for returning to user\n    if _globals_dict is not None:\n        _globals_dict[\"receipt_data\"] = result\n\n    return result.model_dump_json()\n</code></pre>"},{"location":"how-to/receipt-processing/#summary","title":"Summary","text":"<p>You've built a receipt processing system that:</p> <ul> <li>Uses a two-step OCR + structuring pattern</li> <li>Leverages vision models for text extraction</li> <li>Validates output with Pydantic schemas</li> <li>Provides observability through spans</li> <li>Can be used as a tool in agent workflows</li> </ul> <p>Key Takeaways:</p> <ul> <li>Define your Pydantic schema first</li> <li>Use SimpleBot for vision/OCR tasks</li> <li>Use StructuredBot for validated structured outputs</li> <li>Use <code>@span</code> decorator and <code>get_current_span()</code> for manual observability</li> <li>Spans automatically track bot operations</li> <li>The <code>@tool</code> decorator makes functions agent-callable</li> </ul>"},{"location":"reference/bots/agentbot/","title":"AgentBot API Reference","text":"<p>AgentBot is a graph-based agent that uses PocketFlow for tool orchestration. It automatically executes tools in a multi-step workflow based on user requests.</p>"},{"location":"reference/bots/agentbot/#class-definition","title":"Class Definition","text":"<pre><code>class AgentBot:\n    \"\"\"An AgentBot that uses PocketFlow for tool orchestration.\n\n    This bot requires user-provided tools to be decorated with @tool. It creates\n    a decision node that uses ToolBot to select tools and executes them through\n    a PocketFlow graph.\n    \"\"\"\n</code></pre>"},{"location":"reference/bots/agentbot/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    tools: List[Callable],\n    decide_node: Optional[DecideNode] = None,\n    system_prompt: Optional[str] = None,\n    model_name: str = \"gpt-4o-mini\",\n    chat_memory: Optional[ChatMemory] = None,\n    **completion_kwargs,\n)\n</code></pre>"},{"location":"reference/bots/agentbot/#parameters","title":"Parameters","text":"<ul> <li> <p>tools (<code>List[Callable]</code>): List of tools decorated with <code>@tool</code>. These tools will be automatically wrapped as PocketFlow nodes and connected to the decision node.</p> </li> <li> <p>decide_node (<code>Optional[DecideNode]</code>, default: <code>None</code>): Optional custom decision node. If provided, overrides <code>system_prompt</code> parameter. If not provided, a default <code>DecideNode</code> is created.</p> </li> <li> <p>system_prompt (<code>Optional[str]</code>, default: <code>None</code>): System prompt string for decision-making. Only used if <code>decide_node</code> is not provided. Ignored if <code>decide_node</code> is provided.</p> </li> <li> <p>model_name (<code>str</code>, default: <code>\"gpt-4o-mini\"</code>): The name of the model to use for decision-making and tool selection.</p> </li> <li> <p>chat_memory (<code>Optional[ChatMemory]</code>, default: <code>None</code>): Chat memory component for maintaining conversation context.</p> </li> <li> <p>completion_kwargs: Additional keyword arguments to pass to the completion function.</p> </li> </ul>"},{"location":"reference/bots/agentbot/#tool-requirements","title":"Tool Requirements","text":"<p>Tools must be decorated with <code>@tool</code> before being passed to AgentBot:</p> <pre><code>from llamabot.components.tools import tool\n\n@tool\ndef my_tool(arg: str) -&gt; str:\n    \"\"\"Tool description.\"\"\"\n    return arg\n\nbot = AgentBot(tools=[my_tool])\n</code></pre> <p>For terminal tools (like <code>respond_to_user</code>), use <code>@tool(loopback_name=None)</code>:</p> <pre><code>@tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to the user.\"\"\"\n    return response\n</code></pre>"},{"location":"reference/bots/agentbot/#methods","title":"Methods","text":""},{"location":"reference/bots/agentbot/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    *messages: Union[str, BaseMessage],\n) -&gt; Any\n</code></pre> <p>Execute the agent workflow with the given messages.</p>"},{"location":"reference/bots/agentbot/#parameters_1","title":"Parameters","text":"<ul> <li>messages: One or more messages to process. Can be strings or <code>BaseMessage</code> objects.</li> </ul>"},{"location":"reference/bots/agentbot/#returns","title":"Returns","text":"<ul> <li>Any: The result from the terminal node (usually a string response or object).</li> </ul>"},{"location":"reference/bots/agentbot/#example","title":"Example","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Implementation here\n    return results\n\n@lmb.tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to the user.\"\"\"\n    return response\n\nbot = lmb.AgentBot(\n    tools=[search_web, respond_to_user],\n    model_name=\"gpt-4o-mini\"\n)\n\nresult = bot(\"Research the latest developments in AI and summarize them\")\n</code></pre>"},{"location":"reference/bots/agentbot/#visualize","title":"<code>visualize</code>","text":"<pre><code>def visualize(self) -&gt; str\n</code></pre> <p>Generate a Mermaid diagram representation of the agent's flow graph.</p>"},{"location":"reference/bots/agentbot/#returns_1","title":"Returns","text":"<ul> <li>str: Mermaid diagram code that can be rendered.</li> </ul>"},{"location":"reference/bots/agentbot/#example_1","title":"Example","text":"<pre><code>bot = lmb.AgentBot(tools=[my_tool1, my_tool2])\nmermaid_diagram = bot.visualize()\nprint(mermaid_diagram)\n</code></pre>"},{"location":"reference/bots/agentbot/#default-tools","title":"Default Tools","text":"<p>AgentBot automatically includes these default tools:</p> <ul> <li><code>today_date()</code>: Returns the current date (loops back to decide node)</li> <li><code>respond_to_user(response: str)</code>: Sends a text response to the user (terminal node, no loopback)</li> <li><code>return_object_to_user(variable_name: str)</code>: Returns an object from the calling context's globals (terminal node, no loopback)</li> <li><code>inspect_globals()</code>: Inspects available global variables (loops back to decide node)</li> </ul>"},{"location":"reference/bots/agentbot/#flow-graph-structure","title":"Flow Graph Structure","text":"<p>AgentBot creates a flow graph where:</p> <ol> <li>Decision Node (<code>DecideNode</code>): Analyzes the conversation and selects which tool to execute</li> <li>Tool Nodes: Execute the selected tools</li> <li>Loopback: Tools can loop back to the decision node (except terminal tools)</li> <li>Terminal Nodes: Tools with <code>loopback_name=None</code> end the workflow</li> </ol>"},{"location":"reference/bots/agentbot/#attributes","title":"Attributes","text":"<ul> <li>flow (<code>Flow</code>): The PocketFlow flow graph</li> <li>decide_node (<code>DecideNode</code>): The decision node</li> <li>tools (<code>List[Callable]</code>): List of tool functions</li> </ul>"},{"location":"reference/bots/agentbot/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/bots/agentbot/#basic-multi-step-workflow","title":"Basic Multi-Step Workflow","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the current weather for a city.\n\n    :param city: The name of the city\n    :return: Weather information\n    \"\"\"\n    return f\"The weather in {city} is sunny, 72\u00b0F\"\n\n@lmb.tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to the user.\"\"\"\n    return response\n\nagent = lmb.AgentBot(\n    tools=[get_weather, respond_to_user],\n    model_name=\"gpt-4o-mini\"\n)\n\nresult = agent(\"What's the weather in New York?\")\n</code></pre>"},{"location":"reference/bots/agentbot/#with-custom-system-prompt","title":"With Custom System Prompt","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef analyze_data(data: str) -&gt; str:\n    \"\"\"Analyze data.\"\"\"\n    return \"Analysis complete\"\n\nagent = lmb.AgentBot(\n    tools=[analyze_data],\n    system_prompt=\"You are a data analysis expert.\",\n    model_name=\"gpt-4o-mini\"\n)\n</code></pre>"},{"location":"reference/bots/agentbot/#with-chat-memory","title":"With Chat Memory","text":"<pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory()\n\n@lmb.tool\ndef my_tool(arg: str) -&gt; str:\n    return arg\n\nagent = lmb.AgentBot(\n    tools=[my_tool],\n    chat_memory=memory,\n    model_name=\"gpt-4o-mini\"\n)\n</code></pre>"},{"location":"reference/bots/agentbot/#visualizing-the-flow-graph","title":"Visualizing the Flow Graph","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef step1(data: str) -&gt; str:\n    return \"Step 1 complete\"\n\n@lmb.tool\ndef step2(data: str) -&gt; str:\n    return \"Step 2 complete\"\n\nagent = lmb.AgentBot(tools=[step1, step2])\nmermaid_diagram = agent.visualize()\nprint(mermaid_diagram)\n</code></pre>"},{"location":"reference/bots/agentbot/#differences-from-toolbot","title":"Differences from ToolBot","text":"<ul> <li>AgentBot: Multi-turn planning, automatically executes tools in a graph</li> <li>ToolBot: Single-turn execution, returns tool calls for you to execute</li> </ul>"},{"location":"reference/bots/agentbot/#observability","title":"Observability","text":"<p>Span-based logging is enabled by default for all tools decorated with <code>@tool</code>. You can customize span attributes:</p> <pre><code>@lmb.tool(exclude_args=[\"api_key\"], operation_name=\"custom_name\")\ndef my_tool(api_key: str, data: str) -&gt; str:\n    \"\"\"Tool with custom span configuration.\"\"\"\n    return result\n</code></pre>"},{"location":"reference/bots/agentbot/#related-classes","title":"Related Classes","text":"<ul> <li>ToolBot: Single-turn tool execution bot</li> <li>DecideNode: Decision-making node component</li> <li>Tools Module: Tool decorator and utilities</li> </ul>"},{"location":"reference/bots/agentbot/#see-also","title":"See Also","text":"<ul> <li>AgentBot Tutorial</li> <li>Which Bot Should I Use?</li> <li>Tools Component</li> </ul>"},{"location":"reference/bots/imagebot/","title":"ImageBot API Reference","text":"<p>ImageBot is a specialized bot for generating images using DALL-E.</p>"},{"location":"reference/bots/imagebot/#class-definition","title":"Class Definition","text":"<pre><code>class ImageBot:\n    \"\"\"ImageBot class for generating images.\n\n    :param model: The model to use. Defaults to \"dall-e-3\".\n    :param size: The size of the image to generate. Defaults to \"1024x1024\".\n    :param quality: The quality of the image to generate. Defaults to \"standard\".\n    :param n: The number of images to generate. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"reference/bots/imagebot/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    model: str = \"dall-e-3\",\n    size: str = \"1024x1024\",\n    quality: str = \"hd\",\n    n: int = 1,\n)\n</code></pre>"},{"location":"reference/bots/imagebot/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li> <p>model (<code>str</code>, default: <code>\"dall-e-3\"</code>): The DALL-E model to use.   Currently supports <code>\"dall-e-3\"</code> and <code>\"dall-e-2\"</code>.</p> </li> <li> <p>size (<code>str</code>, default: <code>\"1024x1024\"</code>): The size of the image to generate.   For DALL-E 3, valid sizes are <code>\"1024x1024\"</code>, <code>\"1792x1024\"</code>, and   <code>\"1024x1792\"</code>. For DALL-E 2, valid sizes are <code>\"256x256\"</code>, <code>\"512x512\"</code>, and   <code>\"1024x1024\"</code>.</p> </li> <li> <p>quality (<code>str</code>, default: <code>\"hd\"</code>): The quality of the image.   For DALL-E 3, valid values are <code>\"standard\"</code> and <code>\"hd\"</code>.   For DALL-E 2, this parameter is not used.</p> </li> <li> <p>n (<code>int</code>, default: <code>1</code>): The number of images to generate.   For DALL-E 3, this must be 1. For DALL-E 2, can be 1-10.</p> </li> </ul>"},{"location":"reference/bots/imagebot/#methods","title":"Methods","text":""},{"location":"reference/bots/imagebot/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    prompt: str,\n    return_url: bool = False,\n    save_path: Optional[Path] = None,\n) -&gt; Union[str, Path]\n</code></pre> <p>Generate an image from a text prompt.</p>"},{"location":"reference/bots/imagebot/#parameters","title":"Parameters","text":"<ul> <li> <p>prompt (<code>str</code>): The text prompt describing the image to generate.</p> </li> <li> <p>return_url (<code>bool</code>, default: <code>False</code>): Whether to return the URL of the   generated image. If <code>True</code>, overrides <code>save_path</code> parameter.   Useful for Jupyter notebooks.</p> </li> <li> <p>save_path (<code>Optional[Path]</code>, default: <code>None</code>): The path to save the   generated image to. If <code>None</code>, a filename will be generated from the prompt.</p> </li> </ul>"},{"location":"reference/bots/imagebot/#returns","title":"Returns","text":"<ul> <li>Union[str, Path]:</li> <li>If <code>return_url=True</code>: Returns the image URL as a string     (useful for Jupyter notebooks)</li> <li>Otherwise: Returns a <code>Path</code> object pointing to the saved image file</li> </ul>"},{"location":"reference/bots/imagebot/#raises","title":"Raises","text":"<ul> <li>Exception: If no image URL is found in the response.</li> </ul>"},{"location":"reference/bots/imagebot/#example","title":"Example","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot()\n\n# In a Jupyter notebook (returns URL)\nurl = bot(\"A painting of a sunset over mountains\", return_url=True)\n\n# In a Python script (saves to file)\nimage_path = bot(\"A painting of a sunset over mountains\")\nprint(image_path)  # Path object\n</code></pre>"},{"location":"reference/bots/imagebot/#attributes","title":"Attributes","text":"<ul> <li>model (<code>str</code>): The DALL-E model being used</li> <li>size (<code>str</code>): The image size</li> <li>quality (<code>str</code>): The image quality setting</li> <li>n (<code>int</code>): The number of images to generate</li> </ul>"},{"location":"reference/bots/imagebot/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/bots/imagebot/#basic-image-generation","title":"Basic Image Generation","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot()\nimage_path = bot(\"A futuristic cityscape at night\")\n</code></pre>"},{"location":"reference/bots/imagebot/#custom-size-and-quality","title":"Custom Size and Quality","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot(\n    model=\"dall-e-3\",\n    size=\"1792x1024\",\n    quality=\"hd\"\n)\n\nimage_path = bot(\"A detailed landscape painting\")\n</code></pre>"},{"location":"reference/bots/imagebot/#in-jupyter-notebooks","title":"In Jupyter Notebooks","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot()\n\n# Returns URL for display in notebook\nurl = bot(\"A beautiful sunset\", return_url=True)\n# Image automatically displays in notebook\n</code></pre>"},{"location":"reference/bots/imagebot/#save-to-specific-path","title":"Save to Specific Path","text":"<pre><code>import llamabot as lmb\nfrom pathlib import Path\n\nbot = lmb.ImageBot()\n\nimage_path = bot(\n    \"A painting of a dog\",\n    save_path=Path(\"output/dog_painting.png\")\n)\n</code></pre>"},{"location":"reference/bots/imagebot/#using-dall-e-2","title":"Using DALL-E 2","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.ImageBot(\n    model=\"dall-e-2\",\n    size=\"512x512\",\n    n=3  # Can generate multiple images with DALL-E 2\n)\n\nimage_paths = [bot(\"A cat wearing sunglasses\") for _ in range(3)]\n</code></pre>"},{"location":"reference/bots/imagebot/#requirements","title":"Requirements","text":"<ul> <li>OpenAI API key must be set in environment variable <code>OPENAI_API_KEY</code></li> <li>For DALL-E 3, requires OpenAI API access</li> <li>For DALL-E 2, requires OpenAI API access</li> </ul>"},{"location":"reference/bots/imagebot/#best-practices","title":"Best Practices","text":"<ol> <li>Be descriptive: More detailed prompts produce better results</li> <li>Specify style: Include artistic style, mood, or composition details</li> <li>Use DALL-E 3 for quality: DALL-E 3 produces higher quality images</li> <li>Save important images: Always save generated images if you need them later</li> </ol>"},{"location":"reference/bots/imagebot/#related-classes","title":"Related Classes","text":"<ul> <li>SimpleBot: General-purpose chatbot (does not generate images)</li> </ul>"},{"location":"reference/bots/imagebot/#see-also","title":"See Also","text":"<ul> <li>Which Bot Should I Use?</li> <li>OpenAI DALL-E Documentation</li> </ul>"},{"location":"reference/bots/querybot/","title":"QueryBot API Reference","text":"<p>QueryBot is a bot that can answer questions based on a set of documents. It uses a document store to retrieve relevant documents for a given query.</p>"},{"location":"reference/bots/querybot/#class-definition","title":"Class Definition","text":"<pre><code>class QueryBot(SimpleBot):\n    \"\"\"Initialize QueryBot.\n\n    QueryBot is a bot that can answer questions based on a set of documents.\n    It uses a document store to retrieve relevant documents for a given query.\n    \"\"\"\n</code></pre>"},{"location":"reference/bots/querybot/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    system_prompt: str,\n    docstore: AbstractDocumentStore,\n    memory: Optional[AbstractDocumentStore] = None,\n    mock_response: str | None = None,\n    temperature: float = 0.0,\n    model_name: str = default_language_model(),\n    stream_target: str = \"stdout\",\n    **kwargs,\n)\n</code></pre>"},{"location":"reference/bots/querybot/#parameters","title":"Parameters","text":"<ul> <li> <p>system_prompt (<code>str</code>): The system prompt to use for the bot. This defines how the bot interprets and answers questions based on retrieved documents.</p> </li> <li> <p>docstore (<code>AbstractDocumentStore</code>): The document store to use for document retrieval. Must be an instance of <code>AbstractDocumentStore</code> (e.g., <code>LanceDBDocStore</code>, <code>BM25DocStore</code>).</p> </li> <li> <p>memory (<code>Optional[AbstractDocumentStore]</code>, default: <code>None</code>): Optional chat memory component. For conversational memory, use <code>ChatMemory</code> (e.g., <code>lmb.ChatMemory()</code>). This is separate from the document store used for RAG.</p> </li> <li> <p>mock_response (<code>str | None</code>, default: <code>None</code>): Optional mock response for testing purposes.</p> </li> <li> <p>temperature (<code>float</code>, default: <code>0.0</code>): The model temperature to use. Controls randomness in responses.</p> </li> <li> <p>model_name (<code>str</code>, default: <code>default_language_model()</code>): The name of the model to use. Supports all models from LiteLLM.</p> </li> <li> <p>stream_target (<code>str</code>, default: <code>\"stdout\"</code>): The target to stream the response to. Should be one of <code>\"stdout\"</code>, <code>\"panel\"</code>, <code>\"api\"</code>, or <code>\"none\"</code>.</p> </li> <li> <p>kwargs: Additional keyword arguments passed to <code>SimpleBot</code>.</p> </li> </ul>"},{"location":"reference/bots/querybot/#methods","title":"Methods","text":""},{"location":"reference/bots/querybot/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    query: Union[str, HumanMessage, BaseMessage],\n    n_results: int = 20,\n) -&gt; AIMessage\n</code></pre> <p>Query documents within QueryBot's document store and return an answer.</p>"},{"location":"reference/bots/querybot/#parameters_1","title":"Parameters","text":"<ul> <li> <p>query (<code>Union[str, HumanMessage, BaseMessage]</code>): The query to search for. Can be a string or a message object.</p> </li> <li> <p>n_results (<code>int</code>, default: <code>20</code>): The number of document results to retrieve and use for answering the query.</p> </li> </ul>"},{"location":"reference/bots/querybot/#returns","title":"Returns","text":"<ul> <li>AIMessage: The AI's response message containing:</li> <li><code>content</code>: The answer based on retrieved documents</li> <li><code>role</code>: <code>\"assistant\"</code></li> <li>Additional metadata</li> </ul>"},{"location":"reference/bots/querybot/#example","title":"Example","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(table_name=\"my_docs\")\ndocstore.extend([doc1, doc2, doc3])\n\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore\n)\n\nresponse = bot(\"What does the documentation say about authentication?\")\nprint(response.content)\n</code></pre>"},{"location":"reference/bots/querybot/#attributes","title":"Attributes","text":"<ul> <li>docstore (<code>AbstractDocumentStore</code>): The document store used for retrieval</li> <li>memory (<code>Optional[AbstractDocumentStore]</code>): The chat memory component (if any)</li> </ul>"},{"location":"reference/bots/querybot/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/bots/querybot/#basic-document-qa","title":"Basic Document Q&amp;A","text":"<pre><code>import llamabot as lmb\nfrom pathlib import Path\n\n# Create a document store\ndocstore = lmb.LanceDBDocStore(table_name=\"my_documents\")\n\n# Add documents\ndocs_paths = Path(\"docs\").rglob(\"*.md\")\ndocs_texts = [p.read_text() for p in docs_paths]\ndocstore.extend(docs_texts)\n\n# Create QueryBot\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore\n)\n\n# Query the documents\nresponse = bot(\"What is the main topic of these documents?\")\n</code></pre>"},{"location":"reference/bots/querybot/#with-chat-memory","title":"With Chat Memory","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(table_name=\"my_docs\")\ndocstore.extend(documents)\n\n# Create chat memory for conversation context\nmemory = lmb.ChatMemory()\n\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore,\n    memory=memory\n)\n\n# Bot remembers previous questions\nresponse1 = bot(\"What is authentication?\")\nresponse2 = bot(\"How does it work?\")  # Bot remembers context\n</code></pre>"},{"location":"reference/bots/querybot/#with-threaded-memory","title":"With Threaded Memory","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(table_name=\"my_docs\")\ndocstore.extend(documents)\n\n# Use intelligent threading for better context\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore,\n    memory=memory\n)\n</code></pre>"},{"location":"reference/bots/querybot/#custom-number-of-results","title":"Custom Number of Results","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(table_name=\"my_docs\")\ndocstore.extend(documents)\n\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore\n)\n\n# Retrieve more documents for comprehensive answers\nresponse = bot(\"Explain the architecture\", n_results=50)\n</code></pre>"},{"location":"reference/bots/querybot/#document-store-options","title":"Document Store Options","text":""},{"location":"reference/bots/querybot/#lancedbdocstore-default","title":"LanceDBDocStore (Default)","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(\n    table_name=\"my-documents\",\n    embedding_registry=\"sentence-transformers\",\n    embedding_model=\"minishlab/potion-base-8M\"\n)\n</code></pre>"},{"location":"reference/bots/querybot/#bm25docstore-keyword-based","title":"BM25DocStore (Keyword-based)","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.BM25DocStore()\ndocstore.extend(documents)\n</code></pre>"},{"location":"reference/bots/querybot/#differences-from-simplebot","title":"Differences from SimpleBot","text":"<ul> <li>QueryBot: Retrieves relevant documents before answering</li> <li>SimpleBot: Answers based on training data only (no document retrieval)</li> </ul>"},{"location":"reference/bots/querybot/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive system prompts: Help the bot understand how to use the documents</li> <li>Adjust n_results: More results = more context, but slower and more expensive</li> <li>Use chat memory: Maintain conversation context across queries</li> <li>Pre-process documents: Clean and structure documents before adding to the store</li> </ol>"},{"location":"reference/bots/querybot/#related-classes","title":"Related Classes","text":"<ul> <li>SimpleBot: Base class that QueryBot extends</li> <li>AbstractDocumentStore: Document store interface</li> <li>LanceDBDocStore: Default vector-based document store</li> <li>BM25DocStore: Keyword-based document store</li> <li>ChatMemory: Conversation memory component</li> </ul>"},{"location":"reference/bots/querybot/#see-also","title":"See Also","text":"<ul> <li>QueryBot Tutorial</li> <li>Which Bot Should I Use?</li> <li>DocStore Component</li> <li>Chat Memory Component</li> </ul>"},{"location":"reference/bots/simplebot/","title":"SimpleBot API Reference","text":"<p>SimpleBot is a stateless or stateful chatbot that can be primed with a system prompt and used for general-purpose conversations.</p>"},{"location":"reference/bots/simplebot/#class-definition","title":"Class Definition","text":"<pre><code>class SimpleBot:\n    \"\"\"Simple Bot that is primed with a system prompt, accepts a human message,\n    and sends back a single response.\n\n    This bot does not retain chat history by default, but can be configured\n    to do so by passing in a chat memory component.\n    \"\"\"\n</code></pre>"},{"location":"reference/bots/simplebot/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    system_prompt: str,\n    temperature: float = 0.0,\n    memory: Optional[AbstractDocumentStore] = None,\n    model_name: str = default_language_model(),\n    stream_target: str = \"stdout\",\n    json_mode: bool = False,\n    api_key: Optional[str] = None,\n    mock_response: Optional[str] = None,\n    **completion_kwargs,\n)\n</code></pre>"},{"location":"reference/bots/simplebot/#parameters","title":"Parameters","text":"<ul> <li> <p>system_prompt (<code>str</code>): The system prompt to use for the bot. This defines the bot's behavior and personality.</p> </li> <li> <p>temperature (<code>float</code>, default: <code>0.0</code>): The model temperature to use. Controls randomness in responses. Range: 0.0 (deterministic) to 2.0 (more creative). See OpenAI's temperature documentation for more information.</p> </li> <li> <p>memory (<code>Optional[AbstractDocumentStore]</code>, default: <code>None</code>): An optional chat memory component to use. If provided, the bot will retain chat history. For conversational memory, use <code>ChatMemory</code> (e.g., <code>lmb.ChatMemory()</code>). For RAG/document retrieval, use an <code>AbstractDocumentStore</code> (such as <code>BM25DocStore</code> or <code>LanceDBDocStore</code>).</p> </li> <li> <p>model_name (<code>str</code>, default: <code>default_language_model()</code>): The name of the model to use. Supports all models from LiteLLM. Examples: <code>\"gpt-4o-mini\"</code>, <code>\"ollama_chat/llama2:13b\"</code>, <code>\"anthropic/claude-3-5-sonnet\"</code>.</p> </li> <li> <p>stream_target (<code>str</code>, default: <code>\"stdout\"</code>): The target to stream the response to. Should be one of <code>\"stdout\"</code>, <code>\"panel\"</code>, <code>\"api\"</code>, or <code>\"none\"</code>.</p> </li> <li> <p>json_mode (<code>bool</code>, default: <code>False</code>): Whether to enable JSON mode for structured output. Note: This does not guarantee schema validation (use <code>StructuredBot</code> for that).</p> </li> <li> <p>api_key (<code>Optional[str]</code>, default: <code>None</code>): The API key to use. If not provided, will use environment variables (e.g., <code>OPENAI_API_KEY</code>).</p> </li> <li> <p>mock_response (<code>Optional[str]</code>, default: <code>None</code>): A mock response to use, for testing purposes only.</p> </li> <li> <p>completion_kwargs: Additional keyword arguments to pass to the completion function of <code>litellm</code>. See LiteLLM documentation for available options.</p> </li> </ul>"},{"location":"reference/bots/simplebot/#special-cases","title":"Special Cases","text":"<ul> <li>For <code>o1-preview</code> and <code>o1-mini</code> models, the system prompt is automatically converted to a human message (as required by these models).</li> </ul>"},{"location":"reference/bots/simplebot/#methods","title":"Methods","text":""},{"location":"reference/bots/simplebot/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    *messages: Union[str, BaseMessage, list[Union[str, BaseMessage]], Callable],\n) -&gt; AIMessage\n</code></pre> <p>Process messages and return an AI response.</p>"},{"location":"reference/bots/simplebot/#parameters_1","title":"Parameters","text":"<ul> <li>messages: One or more messages to process. Can be:</li> <li>Strings (converted to <code>HumanMessage</code>)</li> <li><code>BaseMessage</code> objects (<code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>)</li> <li>Lists of messages</li> <li>Callable functions that return strings</li> </ul>"},{"location":"reference/bots/simplebot/#returns","title":"Returns","text":"<ul> <li>AIMessage: The AI's response message containing:</li> <li><code>content</code>: The response text</li> <li><code>role</code>: <code>\"assistant\"</code></li> <li>Additional metadata</li> </ul>"},{"location":"reference/bots/simplebot/#example","title":"Example","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.SimpleBot(\"You are a helpful assistant.\")\n\n# Single string message\nresponse = bot(\"Hello!\")\nprint(response.content)\n\n# Multiple messages\nresponse = bot(\"What is Python?\", \"Tell me more about it.\")\n\n# Using message objects\nfrom llamabot.components.messages import HumanMessage\nresponse = bot(HumanMessage(content=\"Hello!\"))\n</code></pre>"},{"location":"reference/bots/simplebot/#generate_response","title":"<code>generate_response</code>","text":"<pre><code>def generate_response(\n    self,\n    messages: List[BaseMessage],\n    stream: bool = True,\n) -&gt; AIMessage\n</code></pre> <p>Generate a response from a list of messages. This is the internal method used by <code>__call__</code>.</p>"},{"location":"reference/bots/simplebot/#parameters_2","title":"Parameters","text":"<ul> <li>messages (<code>List[BaseMessage]</code>): List of message objects to process</li> <li>stream (<code>bool</code>, default: <code>True</code>): Whether to stream the response</li> </ul>"},{"location":"reference/bots/simplebot/#returns_1","title":"Returns","text":"<ul> <li>AIMessage: The AI's response message</li> </ul>"},{"location":"reference/bots/simplebot/#attributes","title":"Attributes","text":"<ul> <li>system_prompt (<code>SystemMessage</code>): The system prompt message</li> <li>model_name (<code>str</code>): The model name being used</li> <li>temperature (<code>float</code>): The temperature setting</li> <li>memory (<code>Optional[AbstractDocumentStore]</code>): The memory component (if any)</li> <li>stream_target (<code>str</code>): The streaming target</li> <li>json_mode (<code>bool</code>): Whether JSON mode is enabled</li> </ul>"},{"location":"reference/bots/simplebot/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/bots/simplebot/#basic-usage","title":"Basic Usage","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"gpt-4o-mini\"\n)\n\nresponse = bot(\"What is machine learning?\")\nprint(response.content)\n</code></pre>"},{"location":"reference/bots/simplebot/#with-chat-memory","title":"With Chat Memory","text":"<pre><code>import llamabot as lmb\n\n# Linear memory (fast, no LLM calls)\nmemory = lmb.ChatMemory()\n\n# Or threaded memory (intelligent conversation threading)\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    memory=memory,\n    model_name=\"gpt-4o-mini\"\n)\n\n# Bot remembers previous conversations\nresponse1 = bot(\"My name is Alice.\")\nresponse2 = bot(\"What's my name?\")  # Bot remembers!\n</code></pre>"},{"location":"reference/bots/simplebot/#with-custom-model","title":"With Custom Model","text":"<pre><code>import llamabot as lmb\n\n# Using Ollama local model\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"ollama_chat/llama2:13b\"\n)\n\n# Using Anthropic Claude\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    model_name=\"anthropic/claude-3-5-sonnet\"\n)\n</code></pre>"},{"location":"reference/bots/simplebot/#streaming-to-panel","title":"Streaming to Panel","text":"<pre><code>import llamabot as lmb\n\nbot = lmb.SimpleBot(\n    system_prompt=\"You are a helpful assistant.\",\n    stream_target=\"panel\",\n    model_name=\"gpt-4o-mini\"\n)\n\nresponse = bot(\"Tell me a long story.\")\n</code></pre>"},{"location":"reference/bots/simplebot/#related-classes","title":"Related Classes","text":"<ul> <li>ToolBot: Extends SimpleBot with tool execution capabilities</li> <li>QueryBot: Extends SimpleBot with document retrieval</li> <li>StructuredBot: Extends SimpleBot with Pydantic validation</li> <li>AgentBot: Uses SimpleBot internally for decision-making</li> </ul>"},{"location":"reference/bots/simplebot/#see-also","title":"See Also","text":"<ul> <li>SimpleBot Tutorial</li> <li>Which Bot Should I Use?</li> <li>Chat Memory Component</li> </ul>"},{"location":"reference/bots/structuredbot/","title":"StructuredBot API Reference","text":"<p>StructuredBot is designed for getting structured, validated outputs from LLMs. It enforces Pydantic schema validation and provides automatic retry logic when the LLM doesn't produce valid output.</p>"},{"location":"reference/bots/structuredbot/#class-definition","title":"Class Definition","text":"<pre><code>class StructuredBot(SimpleBot):\n    \"\"\"StructuredBot is given a Pydantic Model and expects the LLM to return\n    a JSON structure that conforms to the model schema.\n    It will validate the returned json against the pydantic model,\n    prompting the LLM to fix any of the validation errors if it does not validate,\n    and then explicitly return an instance of that model.\n    \"\"\"\n</code></pre>"},{"location":"reference/bots/structuredbot/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    system_prompt: Union[str, SystemMessage],\n    pydantic_model: BaseModel,\n    model_name: str = default_language_model(),\n    stream_target: str = \"stdout\",\n    allow_failed_validation: bool = False,\n    **completion_kwargs,\n)\n</code></pre>"},{"location":"reference/bots/structuredbot/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li> <p>system_prompt (<code>Union[str, SystemMessage]</code>): The system prompt to use   for the bot. Should instruct the LLM on how to extract or generate   structured data.</p> </li> <li> <p>pydantic_model (<code>BaseModel</code>): The Pydantic model that defines the   expected output schema. The LLM must return JSON that validates against   this model.</p> </li> <li> <p>model_name (<code>str</code>, default: <code>default_language_model()</code>): The name of   the model to use. Must support structured outputs (e.g., <code>gpt-4o</code>,   <code>anthropic/claude-3-5-sonnet</code>, <code>gemini/gemini-1.5-pro-latest</code>). See   model support for details.</p> </li> <li> <p>stream_target (<code>str</code>, default: <code>\"stdout\"</code>): The target to stream the   response to. StructuredBot streams only to stdout; other modes may not   work correctly.</p> </li> <li> <p>allow_failed_validation (<code>bool</code>, default: <code>False</code>): Whether to allow   returning invalid data if validation fails after retries. If <code>False</code>,   raises <code>ValidationError</code> on failure.</p> </li> <li> <p>completion_kwargs: Additional keyword arguments to pass to the completion function.</p> </li> </ul>"},{"location":"reference/bots/structuredbot/#model-support","title":"Model Support","text":"<p>StructuredBot requires models that support both <code>response_format</code> and <code>response_schema</code> parameters. Supported models include:</p> <ul> <li><code>gpt-4o</code>, <code>gpt-4-turbo</code>, <code>gpt-4</code></li> <li><code>anthropic/claude-3-5-sonnet</code>, <code>anthropic/claude-3-opus</code></li> <li><code>gemini/gemini-1.5-pro-latest</code></li> <li><code>ollama_chat/*</code> (with structured output support)</li> </ul> <p>If a model doesn't support structured outputs, <code>StructuredBot</code> will raise a <code>ValueError</code> at initialization.</p>"},{"location":"reference/bots/structuredbot/#methods","title":"Methods","text":""},{"location":"reference/bots/structuredbot/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    *messages: Union[str, BaseMessage],\n) -&gt; BaseModel\n</code></pre> <p>Process messages and return a validated Pydantic model instance.</p>"},{"location":"reference/bots/structuredbot/#parameters","title":"Parameters","text":"<ul> <li>messages: One or more messages to process. Can be strings or <code>BaseMessage</code> objects.</li> </ul>"},{"location":"reference/bots/structuredbot/#returns","title":"Returns","text":"<ul> <li>BaseModel: An instance of the provided <code>pydantic_model</code> with validated data.</li> </ul>"},{"location":"reference/bots/structuredbot/#raises","title":"Raises","text":"<ul> <li>ValidationError: If validation fails after retries and <code>allow_failed_validation=False</code>.</li> </ul>"},{"location":"reference/bots/structuredbot/#example","title":"Example","text":"<pre><code>import llamabot as lmb\nfrom pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information from text.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n\nperson = bot(\"John is 25 years old. Email: john@example.com\")\nprint(person.name)  # \"John\"\nprint(person.age)   # 25\n</code></pre>"},{"location":"reference/bots/structuredbot/#retry-logic","title":"Retry Logic","text":"<p>StructuredBot automatically retries when validation fails:</p> <ol> <li>LLM generates JSON response</li> <li>JSON is validated against Pydantic model</li> <li>If validation fails, error details are sent back to LLM</li> <li>LLM attempts to fix the JSON</li> <li>Process repeats up to a maximum number of retries</li> <li>If still invalid and <code>allow_failed_validation=False</code>, raises <code>ValidationError</code></li> </ol>"},{"location":"reference/bots/structuredbot/#attributes","title":"Attributes","text":"<ul> <li>pydantic_model (<code>BaseModel</code>): The Pydantic model for validation</li> <li>allow_failed_validation (<code>bool</code>): Whether to allow failed validation</li> </ul>"},{"location":"reference/bots/structuredbot/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/bots/structuredbot/#basic-data-extraction","title":"Basic Data Extraction","text":"<pre><code>import llamabot as lmb\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    hobbies: List[str]\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information from text.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n\nperson = bot(\"John is 25 years old and enjoys hiking and photography.\")\nprint(person.name)      # \"John\"\nprint(person.age)       # 25\nprint(person.hobbies)   # [\"hiking\", \"photography\"]\n</code></pre>"},{"location":"reference/bots/structuredbot/#complex-nested-models","title":"Complex Nested Models","text":"<pre><code>import llamabot as lmb\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    zip_code: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    address: Address\n    created_at: datetime\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information with address.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n\nperson = bot(\"John, 25, lives at 123 Main St, New York, 10001\")\nprint(person.address.city)  # \"New York\"\n</code></pre>"},{"location":"reference/bots/structuredbot/#with-optional-fields","title":"With Optional Fields","text":"<pre><code>import llamabot as lmb\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: Optional[str] = None\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n\n# Works even if email is missing\nperson = bot(\"John is 25 years old\")\nprint(person.email)  # None\n</code></pre>"},{"location":"reference/bots/structuredbot/#allowing-failed-validation","title":"Allowing Failed Validation","text":"<pre><code>import llamabot as lmb\nfrom pydantic import BaseModel, ValidationError\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\",\n    allow_failed_validation=True\n)\n\n# If validation fails, returns partial data instead of raising\ntry:\n    person = bot(\"Invalid text\")\nexcept ValidationError:\n    # Handle validation error\n    pass\n</code></pre>"},{"location":"reference/bots/structuredbot/#differences-from-simplebot-json-mode","title":"Differences from SimpleBot JSON Mode","text":"<ul> <li>StructuredBot: Guarantees schema validation, returns Pydantic objects, automatic retries</li> <li>SimpleBot JSON Mode: Ensures valid JSON only, no schema validation, returns strings</li> </ul>"},{"location":"reference/bots/structuredbot/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive system prompts: Clearly explain what data to extract</li> <li>Define clear schemas: Use Pydantic's validation features (validators, constraints)</li> <li>Handle optional fields: Use <code>Optional</code> for fields that may be missing</li> <li>Test with edge cases: Ensure your schema handles various input formats</li> </ol>"},{"location":"reference/bots/structuredbot/#related-classes","title":"Related Classes","text":"<ul> <li>SimpleBot: Base class that StructuredBot extends</li> <li>Pydantic BaseModel: Schema definition class</li> </ul>"},{"location":"reference/bots/structuredbot/#see-also","title":"See Also","text":"<ul> <li>StructuredBot Tutorial</li> <li>Which Bot Should I Use?</li> <li>Pydantic Documentation</li> </ul>"},{"location":"reference/bots/toolbot/","title":"ToolBot API Reference","text":"<p>ToolBot is a single-turn bot designed for tool execution and function calling. It analyzes user requests and selects the most appropriate tool to execute.</p>"},{"location":"reference/bots/toolbot/#class-definition","title":"Class Definition","text":"<pre><code>class ToolBot(SimpleBot):\n    \"\"\"A single-turn bot that can execute tools.\n\n    This bot is designed to analyze user requests and determine the most appropriate\n    tool or function to execute. It's a generalization of other bot types, focusing\n    on tool selection and execution rather than multi-turn conversation.\n    \"\"\"\n</code></pre>"},{"location":"reference/bots/toolbot/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    system_prompt: str,\n    model_name: str,\n    tools: Optional[List[Callable]] = None,\n    chat_memory: Optional[ChatMemory] = None,\n    **completion_kwargs,\n)\n</code></pre>"},{"location":"reference/bots/toolbot/#parameters","title":"Parameters","text":"<ul> <li> <p>system_prompt (<code>str</code>): The system prompt to use for the bot. This defines the bot's behavior and tool selection strategy.</p> </li> <li> <p>model_name (<code>str</code>): The name of the model to use. Must support function calling. Examples: <code>\"gpt-4o-mini\"</code>, <code>\"gpt-4o\"</code>, <code>\"anthropic/claude-3-5-sonnet\"</code>.</p> </li> <li> <p>tools (<code>Optional[List[Callable]]</code>, default: <code>None</code>): Optional list of additional tools to include. Tools should be decorated with <code>@tool</code> or be callable functions. Default tools (<code>today_date</code>, <code>respond_to_user</code>, <code>return_object_to_user</code>, <code>inspect_globals</code>) are automatically included.</p> </li> <li> <p>chat_memory (<code>Optional[ChatMemory]</code>, default: <code>None</code>): Chat memory component for context retrieval. If not provided, a new <code>ChatMemory()</code> instance is created.</p> </li> <li> <p>completion_kwargs: Additional keyword arguments to pass to the completion function of <code>litellm</code>.</p> </li> </ul>"},{"location":"reference/bots/toolbot/#methods","title":"Methods","text":""},{"location":"reference/bots/toolbot/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    *messages: Union[str, BaseMessage, list[Union[str, BaseMessage]], Callable],\n    execution_history: Optional[List[Dict]] = None,\n) -&gt; List[ChatCompletionMessageToolCall]\n</code></pre> <p>Process messages and return tool calls to execute.</p>"},{"location":"reference/bots/toolbot/#parameters_1","title":"Parameters","text":"<ul> <li> <p>messages: One or more messages to process. Can be strings, <code>BaseMessage</code> objects, lists of messages, or callable functions that return strings.</p> </li> <li> <p>execution_history (<code>Optional[List[Dict]]</code>, default: <code>None</code>): Optional list of previously executed tool calls for context. Each dict should contain:</p> </li> <li><code>tool_name</code>: Name of the tool</li> <li><code>args</code>: Arguments passed to the tool</li> <li><code>result</code>: Result of the tool execution</li> <li><code>was_cached</code>: Whether the result was cached</li> </ul>"},{"location":"reference/bots/toolbot/#returns","title":"Returns","text":"<ul> <li>List[ChatCompletionMessageToolCall]: List of tool calls to execute. Each tool call contains:</li> <li><code>function.name</code>: Name of the function to call</li> <li><code>function.arguments</code>: JSON string of arguments</li> </ul>"},{"location":"reference/bots/toolbot/#example","title":"Example","text":"<pre><code>import llamabot as lmb\nfrom llamabot.components.tools import write_and_execute_code\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a data analyst.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[write_and_execute_code(globals_dict=globals())]\n)\n\ntool_calls = bot(\"Calculate the mean of the sales_data DataFrame\")\n# Returns list of tool calls to execute\n</code></pre>"},{"location":"reference/bots/toolbot/#default-tools","title":"Default Tools","text":"<p>ToolBot automatically includes these default tools:</p> <ul> <li><code>today_date()</code>: Returns the current date in YYYY-MM-DD format</li> <li><code>respond_to_user(response: str)</code>: Responds to the user with a text message</li> <li><code>return_object_to_user(variable_name: str)</code>: Returns an object from the calling context's globals dictionary</li> <li><code>inspect_globals()</code>: Inspects available global variables</li> </ul>"},{"location":"reference/bots/toolbot/#attributes","title":"Attributes","text":"<ul> <li>tools (<code>List[Dict]</code>): List of tool JSON schemas</li> <li>name_to_tool_map (<code>Dict[str, Callable]</code>): Mapping from tool names to tool functions</li> <li>chat_memory (<code>ChatMemory</code>): The chat memory component</li> </ul>"},{"location":"reference/bots/toolbot/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/bots/toolbot/#basic-tool-execution","title":"Basic Tool Execution","text":"<pre><code>import llamabot as lmb\nfrom llamabot.components.tools import write_and_execute_code\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a helpful assistant that can execute Python code.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[write_and_execute_code(globals_dict=globals())]\n)\n\ntool_calls = bot(\"Calculate 2 + 2\")\n</code></pre>"},{"location":"reference/bots/toolbot/#with-custom-tools","title":"With Custom Tools","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef calculate_fibonacci(n: int) -&gt; int:\n    \"\"\"Calculate the nth Fibonacci number.\n\n    :param n: The position in the Fibonacci sequence\n    :return: The nth Fibonacci number\n    \"\"\"\n    if n &lt;= 1:\n        return n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    return b\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a mathematical assistant.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[calculate_fibonacci]\n)\n\ntool_calls = bot(\"Calculate the 10th Fibonacci number\")\n</code></pre>"},{"location":"reference/bots/toolbot/#with-global-variables","title":"With Global Variables","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport llamabot as lmb\nfrom llamabot.components.tools import write_and_execute_code\n\n# Create some data\ndata = pd.DataFrame({\n    'x': np.random.randn(100),\n    'y': np.random.randn(100)\n})\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a data analyst.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[write_and_execute_code(globals_dict=globals())]\n)\n\ntool_calls = bot(\"Calculate the correlation between x and y in the data DataFrame\")\n</code></pre>"},{"location":"reference/bots/toolbot/#with-chat-memory","title":"With Chat Memory","text":"<pre><code>import llamabot as lmb\nfrom llamabot.components.tools import write_and_execute_code\n\nmemory = lmb.ChatMemory()\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a data analysis assistant.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[write_and_execute_code(globals_dict=globals())],\n    chat_memory=memory\n)\n\n# Bot remembers previous interactions\ntool_calls1 = bot(\"Create a DataFrame with sample data\")\ntool_calls2 = bot(\"Now analyze the data you just created\")\n</code></pre>"},{"location":"reference/bots/toolbot/#differences-from-agentbot","title":"Differences from AgentBot","text":"<ul> <li>ToolBot: Single-turn execution, returns tool calls for you to execute</li> <li>AgentBot: Multi-turn planning, executes tools automatically in a graph-based flow</li> </ul>"},{"location":"reference/bots/toolbot/#related-classes","title":"Related Classes","text":"<ul> <li>SimpleBot: Base class that ToolBot extends</li> <li>AgentBot: Multi-turn agent with automatic tool execution</li> <li>Tools Module: Tool decorator and utilities</li> </ul>"},{"location":"reference/bots/toolbot/#see-also","title":"See Also","text":"<ul> <li>ToolBot Tutorial</li> <li>Which Bot Should I Use?</li> <li>Tools Component</li> </ul>"},{"location":"reference/components/chat_memory/","title":"ChatMemory API Reference","text":"<p>ChatMemory provides intelligent conversation memory with configurable threading and retrieval strategies.</p>"},{"location":"reference/components/chat_memory/#class-definition","title":"Class Definition","text":"<pre><code>class ChatMemory:\n    \"\"\"Unified chat memory system with configurable threading and retrieval.\n\n    :param node_selector: Strategy for selecting parent nodes (None = LinearNodeSelector)\n    :param summarizer: Optional summarization strategy (None = no summarization)\n    :param context_depth: Default depth for context retrieval\n    \"\"\"\n</code></pre>"},{"location":"reference/components/chat_memory/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    node_selector: Optional[NodeSelector] = None,\n    summarizer: Optional[Summarizer] = None,\n    context_depth: int = 5,\n)\n</code></pre>"},{"location":"reference/components/chat_memory/#parameters","title":"Parameters","text":"<ul> <li> <p>node_selector (<code>Optional[NodeSelector]</code>, default: <code>None</code>): Strategy for selecting parent nodes when adding messages. If <code>None</code>, uses <code>LinearNodeSelector</code> (linear memory). For intelligent threading, use <code>LLMNodeSelector</code>.</p> </li> <li> <p>summarizer (<code>Optional[Summarizer]</code>, default: <code>None</code>): Optional summarization strategy. If <code>None</code>, no summarization is performed. For threaded memory, use <code>LLMSummarizer</code>.</p> </li> <li> <p>context_depth (<code>int</code>, default: <code>5</code>): Default depth for context retrieval when traversing the conversation graph.</p> </li> </ul>"},{"location":"reference/components/chat_memory/#class-methods","title":"Class Methods","text":""},{"location":"reference/components/chat_memory/#threaded","title":"<code>threaded</code>","text":"<pre><code>@classmethod\ndef threaded(cls, model: str = \"gpt-4o-mini\", **kwargs) -&gt; \"ChatMemory\"\n</code></pre> <p>Create ChatMemory with LLM-based threading.</p>"},{"location":"reference/components/chat_memory/#parameters_1","title":"Parameters","text":"<ul> <li>model (<code>str</code>, default: <code>\"gpt-4o-mini\"</code>): LLM model name for node selection and summarization</li> <li>kwargs: Additional arguments passed to ChatMemory constructor</li> </ul>"},{"location":"reference/components/chat_memory/#returns","title":"Returns","text":"<ul> <li>ChatMemory: A ChatMemory instance with LLM-based threading enabled</li> </ul>"},{"location":"reference/components/chat_memory/#example","title":"Example","text":"<pre><code>import llamabot as lmb\n\n# Create threaded memory\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n</code></pre>"},{"location":"reference/components/chat_memory/#methods","title":"Methods","text":""},{"location":"reference/components/chat_memory/#append","title":"<code>append</code>","text":"<pre><code>def append(self, message: BaseMessage) -&gt; None\n</code></pre> <p>Append a message to the memory.</p>"},{"location":"reference/components/chat_memory/#parameters_2","title":"Parameters","text":"<ul> <li>message (<code>BaseMessage</code>): The message to append (HumanMessage, AIMessage, etc.)</li> </ul>"},{"location":"reference/components/chat_memory/#example_1","title":"Example","text":"<pre><code>import llamabot as lmb\nfrom llamabot.components.messages import HumanMessage, AIMessage\n\nmemory = lmb.ChatMemory()\nmemory.append(HumanMessage(content=\"Hello\"))\nmemory.append(AIMessage(content=\"Hi there!\"))\n</code></pre>"},{"location":"reference/components/chat_memory/#retrieve","title":"<code>retrieve</code>","text":"<pre><code>def retrieve(\n    self,\n    query: str,\n    n_results: int = 10,\n    context_depth: Optional[int] = None,\n) -&gt; List[BaseMessage]\n</code></pre> <p>Smart retrieval that adapts based on memory configuration.</p>"},{"location":"reference/components/chat_memory/#parameters_3","title":"Parameters","text":"<ul> <li>query (<code>str</code>): The search query</li> <li>n_results (<code>int</code>, default: <code>10</code>): Number of results to return</li> <li>context_depth (<code>Optional[int]</code>, default: <code>None</code>): Context depth (uses default if None)</li> </ul>"},{"location":"reference/components/chat_memory/#returns_1","title":"Returns","text":"<ul> <li>List[BaseMessage]: List of relevant messages</li> </ul>"},{"location":"reference/components/chat_memory/#behavior","title":"Behavior","text":"<ul> <li>Linear memory: Returns the most recent <code>n_results</code> messages</li> <li>Threaded memory: Performs semantic search with context traversal up to <code>context_depth</code> levels</li> </ul>"},{"location":"reference/components/chat_memory/#example_2","title":"Example","text":"<pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory()\n\n# Add some messages\nmemory.append(HumanMessage(content=\"What is Python?\"))\nmemory.append(AIMessage(content=\"Python is a programming language.\"))\nmemory.append(HumanMessage(content=\"Tell me more about it.\"))\n\n# Retrieve relevant messages\nrelevant = memory.retrieve(\"programming language\", n_results=5)\n</code></pre>"},{"location":"reference/components/chat_memory/#reset","title":"<code>reset</code>","text":"<pre><code>def reset(self) -&gt; None\n</code></pre> <p>Reset the memory, clearing all stored messages.</p>"},{"location":"reference/components/chat_memory/#example_3","title":"Example","text":"<pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory()\nmemory.append(HumanMessage(content=\"Hello\"))\nmemory.reset()  # Clear all messages\n</code></pre>"},{"location":"reference/components/chat_memory/#to_mermaid","title":"<code>to_mermaid</code>","text":"<pre><code>def to_mermaid(self) -&gt; str\n</code></pre> <p>Generate a Mermaid diagram representation of the conversation graph.</p>"},{"location":"reference/components/chat_memory/#returns_2","title":"Returns","text":"<ul> <li>str: Mermaid diagram code</li> </ul>"},{"location":"reference/components/chat_memory/#example_4","title":"Example","text":"<pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory.threaded()\n# ... add messages ...\nmermaid_diagram = memory.to_mermaid()\nprint(mermaid_diagram)\n</code></pre>"},{"location":"reference/components/chat_memory/#attributes","title":"Attributes","text":"<ul> <li>graph (<code>networkx.DiGraph</code>): The NetworkX graph storing conversation structure</li> <li>node_selector (<code>NodeSelector</code>): The node selection strategy</li> <li>summarizer (<code>Optional[Summarizer]</code>): The summarization strategy (if any)</li> <li>context_depth (<code>int</code>): Default context depth for retrieval</li> </ul>"},{"location":"reference/components/chat_memory/#memory-types","title":"Memory Types","text":""},{"location":"reference/components/chat_memory/#linear-memory","title":"Linear Memory","text":"<p>Simple, fast memory that stores messages in order:</p> <pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory()  # Linear by default\n</code></pre> <p>Characteristics:</p> <ul> <li>Fast (no LLM calls)</li> <li>Stores messages sequentially</li> <li>Retrieves most recent messages</li> <li>Best for simple conversations</li> </ul>"},{"location":"reference/components/chat_memory/#threaded-memory","title":"Threaded Memory","text":"<p>Intelligent memory that connects related conversation topics:</p> <pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n</code></pre> <p>Characteristics:</p> <ul> <li>Uses LLM to connect related topics</li> <li>Creates conversation threads</li> <li>Semantic search for retrieval</li> <li>Best for complex, multi-topic conversations</li> </ul>"},{"location":"reference/components/chat_memory/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/components/chat_memory/#basic-linear-memory","title":"Basic Linear Memory","text":"<pre><code>import llamabot as lmb\nfrom llamabot.components.messages import HumanMessage, AIMessage\n\nmemory = lmb.ChatMemory()\n\nmemory.append(HumanMessage(content=\"What is Python?\"))\nmemory.append(AIMessage(content=\"Python is a programming language.\"))\n\n# Retrieve recent messages\nrecent = memory.retrieve(\"\", n_results=5)\n</code></pre>"},{"location":"reference/components/chat_memory/#threaded-memory_1","title":"Threaded Memory","text":"<pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n\nmemory.append(HumanMessage(content=\"What is Python?\"))\nmemory.append(AIMessage(content=\"Python is a programming language.\"))\nmemory.append(HumanMessage(content=\"How do I install it?\"))\n\n# Semantic search finds related messages\nrelevant = memory.retrieve(\"programming language\", n_results=5)\n</code></pre>"},{"location":"reference/components/chat_memory/#with-bots","title":"With Bots","text":"<pre><code>import llamabot as lmb\n\n# Create memory\nmemory = lmb.ChatMemory()\n\n# Use with SimpleBot\nbot = lmb.SimpleBot(\n    system_prompt=\"You are helpful.\",\n    memory=memory,\n    model_name=\"gpt-4o-mini\"\n)\n\n# Bot automatically uses memory\nresponse1 = bot(\"My name is Alice\")\nresponse2 = bot(\"What's my name?\")  # Bot remembers!\n</code></pre>"},{"location":"reference/components/chat_memory/#visualizing-conversation-graph","title":"Visualizing Conversation Graph","text":"<pre><code>import llamabot as lmb\n\nmemory = lmb.ChatMemory.threaded()\n# ... add messages ...\n\n# Generate Mermaid diagram\ndiagram = memory.to_mermaid()\nprint(diagram)\n</code></pre>"},{"location":"reference/components/chat_memory/#related-classes","title":"Related Classes","text":"<ul> <li>NodeSelector: Strategy for selecting parent nodes</li> <li>Summarizer: Strategy for summarizing messages</li> <li>LinearNodeSelector: Linear memory selector</li> <li>LLMNodeSelector: LLM-based intelligent selector</li> <li>LLMSummarizer: LLM-based summarizer</li> </ul>"},{"location":"reference/components/chat_memory/#see-also","title":"See Also","text":"<ul> <li>Which Bot Should I Use?</li> <li>SimpleBot Reference</li> </ul>"},{"location":"reference/components/docstore/","title":"Document Store API Reference","text":"<p>Document stores provide persistent storage and retrieval for documents in RAG (Retrieval-Augmented Generation) applications.</p>"},{"location":"reference/components/docstore/#abstractdocumentstore","title":"AbstractDocumentStore","text":"<p>Abstract base class defining the document store interface.</p> <pre><code>class AbstractDocumentStore:\n    \"\"\"Abstract document store for LlamaBot.\"\"\"\n</code></pre>"},{"location":"reference/components/docstore/#methods","title":"Methods","text":"<p>All document stores implement these methods:</p>"},{"location":"reference/components/docstore/#append","title":"<code>append</code>","text":"<pre><code>def append(self, document: str) -&gt; None\n</code></pre> <p>Append a single document to the store.</p>"},{"location":"reference/components/docstore/#append-parameters","title":"Append Parameters","text":"<ul> <li>document (<code>str</code>): The document text to append</li> </ul>"},{"location":"reference/components/docstore/#extend","title":"<code>extend</code>","text":"<pre><code>def extend(self, documents: list[str]) -&gt; None\n</code></pre> <p>Extend the store with multiple documents (bulk operation).</p>"},{"location":"reference/components/docstore/#extend-parameters","title":"Extend Parameters","text":"<ul> <li>documents (<code>list[str]</code>): List of document texts to add</li> </ul>"},{"location":"reference/components/docstore/#retrieve","title":"<code>retrieve</code>","text":"<pre><code>def retrieve(self, query: str, n_results: int = 10) -&gt; list[str]\n</code></pre> <p>Retrieve relevant documents for a query.</p>"},{"location":"reference/components/docstore/#retrieve-parameters","title":"Retrieve Parameters","text":"<ul> <li>query (<code>str</code>): The search query</li> <li>n_results (<code>int</code>, default: <code>10</code>): Number of results to return</li> </ul>"},{"location":"reference/components/docstore/#returns","title":"Returns","text":"<ul> <li>list[str]: List of retrieved document texts</li> </ul>"},{"location":"reference/components/docstore/#reset","title":"<code>reset</code>","text":"<pre><code>def reset(self) -&gt; None\n</code></pre> <p>Reset the document store, clearing all documents.</p>"},{"location":"reference/components/docstore/#lancedbdocstore","title":"LanceDBDocStore","text":"<p>Vector-based document store using LanceDB for semantic search.</p> <pre><code>class LanceDBDocStore(AbstractDocumentStore):\n    \"\"\"A document store for LlamaBot that wraps around LanceDB.\n\n    Supports optional partitioning to organize documents into logical groups.\n    \"\"\"\n</code></pre>"},{"location":"reference/components/docstore/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    table_name: str,\n    storage_path: Path = Path.home() / \".llamabot\" / \"lancedb\",\n    embedding_registry: str = \"sentence-transformers\",\n    embedding_model: str = \"minishlab/potion-base-8M\",\n    auto_create_fts_index: bool = True,\n    enable_partitioning: bool = False,\n    default_partition: str = \"default\",\n)\n</code></pre>"},{"location":"reference/components/docstore/#parameters","title":"Parameters","text":"<ul> <li> <p>table_name (<code>str</code>): Name of the table to create or open. Will be automatically slugified.</p> </li> <li> <p>storage_path (<code>Path</code>, default: <code>~/.llamabot/lancedb</code>): Path to the LanceDB storage directory.</p> </li> <li> <p>embedding_registry (<code>str</code>, default: <code>\"sentence-transformers\"</code>): Registry name for the embedding function.</p> </li> <li> <p>embedding_model (<code>str</code>, default: <code>\"minishlab/potion-base-8M\"</code>): Model name for the embedding function.</p> </li> <li> <p>auto_create_fts_index (<code>bool</code>, default: <code>True</code>): Whether to automatically create a full-text search index on the document field.</p> </li> <li> <p>enable_partitioning (<code>bool</code>, default: <code>False</code>): If <code>True</code>, enables partitioning support. When enabled, documents must be assigned to partitions and retrieval can be filtered by partition.</p> </li> <li> <p>default_partition (<code>str</code>, default: <code>\"default\"</code>): Default partition name to use when partition is not specified and partitioning is enabled.</p> </li> </ul>"},{"location":"reference/components/docstore/#methods_1","title":"Methods","text":""},{"location":"reference/components/docstore/#append_1","title":"<code>append</code>","text":"<pre><code>def append(\n    self,\n    document: str,\n    partition: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Append a document to the store.</p>"},{"location":"reference/components/docstore/#lancedb-append-parameters","title":"LanceDB Append Parameters","text":"<ul> <li>document (<code>str</code>): The document text to append</li> <li>partition (<code>Optional[str]</code>, default: <code>None</code>): Partition name (required if partitioning is enabled)</li> </ul>"},{"location":"reference/components/docstore/#extend_1","title":"<code>extend</code>","text":"<pre><code>def extend(\n    self,\n    documents: list[str],\n    partitions: Optional[list[str]] = None,\n) -&gt; None\n</code></pre> <p>Extend the store with multiple documents.</p>"},{"location":"reference/components/docstore/#lancedb-extend-parameters","title":"LanceDB Extend Parameters","text":"<ul> <li>documents (<code>list[str]</code>): List of document texts</li> <li>partitions (<code>Optional[list[str]]</code>, default: <code>None</code>): List of partition names (required if partitioning is enabled)</li> </ul>"},{"location":"reference/components/docstore/#retrieve_1","title":"<code>retrieve</code>","text":"<pre><code>def retrieve(\n    self,\n    query: str,\n    n_results: int = 10,\n    partition: Optional[str] = None,\n) -&gt; list[str]\n</code></pre> <p>Retrieve relevant documents using semantic search.</p>"},{"location":"reference/components/docstore/#lancedb-retrieve-parameters","title":"LanceDB Retrieve Parameters","text":"<ul> <li>query (<code>str</code>): The search query</li> <li>n_results (<code>int</code>, default: <code>10</code>): Number of results to return</li> <li>partition (<code>Optional[str]</code>, default: <code>None</code>): Filter results by partition (if partitioning is enabled)</li> </ul>"},{"location":"reference/components/docstore/#returns_1","title":"Returns","text":"<ul> <li>list[str]: List of retrieved document texts</li> </ul>"},{"location":"reference/components/docstore/#reset_1","title":"<code>reset</code>","text":"<pre><code>def reset(self) -&gt; None\n</code></pre> <p>Reset the document store, clearing all documents.</p>"},{"location":"reference/components/docstore/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/components/docstore/#basic-usage","title":"Basic Usage","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(table_name=\"my_documents\")\n\n# Add documents\ndocstore.append(\"Document 1 text\")\ndocstore.extend([\"Document 2 text\", \"Document 3 text\"])\n\n# Retrieve documents\nresults = docstore.retrieve(\"search query\", n_results=5)\n</code></pre>"},{"location":"reference/components/docstore/#with-custom-embeddings","title":"With Custom Embeddings","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(\n    table_name=\"my_documents\",\n    embedding_registry=\"sentence-transformers\",\n    embedding_model=\"all-MiniLM-L6-v2\"\n)\n</code></pre>"},{"location":"reference/components/docstore/#with-partitioning","title":"With Partitioning","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.LanceDBDocStore(\n    table_name=\"my_documents\",\n    enable_partitioning=True\n)\n\n# Add documents to specific partitions\ndocstore.append(\"Document 1\", partition=\"category_a\")\ndocstore.extend(\n    [\"Doc 2\", \"Doc 3\"],\n    partitions=[\"category_b\", \"category_b\"]\n)\n\n# Retrieve from specific partition\nresults = docstore.retrieve(\"query\", partition=\"category_a\")\n</code></pre>"},{"location":"reference/components/docstore/#bm25docstore","title":"BM25DocStore","text":"<p>Keyword-based document store using BM25 for full-text search.</p> <pre><code>class BM25DocStore(AbstractDocumentStore):\n    \"\"\"A document store for LlamaBot that uses BM25 for keyword-based search.\"\"\"\n</code></pre>"},{"location":"reference/components/docstore/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self)\n</code></pre> <p>BM25DocStore has no configuration parameters.</p>"},{"location":"reference/components/docstore/#methods_2","title":"Methods","text":"<p>Implements all methods from <code>AbstractDocumentStore</code>:</p> <ul> <li><code>append(document: str) -&gt; None</code></li> <li><code>extend(documents: list[str]) -&gt; None</code></li> <li><code>retrieve(query: str, n_results: int = 10) -&gt; list[str]</code></li> <li><code>reset() -&gt; None</code></li> </ul>"},{"location":"reference/components/docstore/#usage-examples_1","title":"Usage Examples","text":""},{"location":"reference/components/docstore/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import llamabot as lmb\n\ndocstore = lmb.BM25DocStore()\n\n# Add documents\ndocstore.append(\"Document 1 text\")\ndocstore.extend([\"Document 2 text\", \"Document 3 text\"])\n\n# Retrieve using keyword search\nresults = docstore.retrieve(\"keyword search\", n_results=5)\n</code></pre>"},{"location":"reference/components/docstore/#comparison","title":"Comparison","text":"Feature LanceDBDocStore BM25DocStore Search Type Semantic (vector) Keyword-based Embeddings Required Not needed Best For Semantic similarity Exact keyword matching Speed Fast (indexed) Fast (in-memory) Persistence Yes (disk) No (in-memory) Partitioning Supported Not supported"},{"location":"reference/components/docstore/#usage-with-querybot","title":"Usage with QueryBot","text":"<pre><code>import llamabot as lmb\nfrom pathlib import Path\n\n# Create document store\ndocstore = lmb.LanceDBDocStore(table_name=\"my_docs\")\n\n# Add documents\ndocs_paths = Path(\"docs\").rglob(\"*.md\")\ndocs_texts = [p.read_text() for p in docs_paths]\ndocstore.extend(docs_texts)\n\n# Use with QueryBot\nbot = lmb.QueryBot(\n    system_prompt=\"You are an expert on these documents.\",\n    docstore=docstore\n)\n\nresponse = bot(\"What is the main topic?\")\n</code></pre>"},{"location":"reference/components/docstore/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right store: Use <code>LanceDBDocStore</code> for semantic search, <code>BM25DocStore</code> for keyword search</li> <li>Batch operations: Use <code>extend()</code> for adding multiple documents (faster than multiple <code>append()</code> calls)</li> <li>Partitioning: Use partitioning in <code>LanceDBDocStore</code> to organize large document collections</li> <li>Reset when needed: Use <code>reset()</code> to clear and rebuild indexes when documents change significantly</li> </ol>"},{"location":"reference/components/docstore/#related-classes","title":"Related Classes","text":"<ul> <li>QueryBot: Bot that uses document stores for RAG</li> <li>AbstractDocumentStore: Base interface for document stores</li> </ul>"},{"location":"reference/components/docstore/#see-also","title":"See Also","text":"<ul> <li>QueryBot Reference</li> <li>QueryBot Tutorial</li> <li>Which Bot Should I Use?</li> </ul>"},{"location":"reference/components/messages/","title":"Messages API Reference","text":"<p>The messages system provides unified message types for LLM communication.</p>"},{"location":"reference/components/messages/#basemessage","title":"BaseMessage","text":"<p>Base class for all message types.</p> <pre><code>class BaseMessage(BaseModel):\n    \"\"\"A base message class.\"\"\"\n\n    role: str\n    content: str\n    prompt_hash: str | None = None\n    tool_calls: list = []\n</code></pre>"},{"location":"reference/components/messages/#attributes","title":"Attributes","text":"<ul> <li>role (<code>str</code>): The role of the message sender (<code>\"system\"</code>, <code>\"user\"</code>, <code>\"assistant\"</code>, <code>\"tool\"</code>)</li> <li>content (<code>str</code>): The message content</li> <li>prompt_hash (<code>str | None</code>, default: <code>None</code>): Optional hash of the prompt</li> <li>tool_calls (<code>list</code>, default: <code>[]</code>): List of tool calls associated with the message</li> </ul>"},{"location":"reference/components/messages/#methods","title":"Methods","text":""},{"location":"reference/components/messages/#__len__","title":"<code>__len__</code>","text":"<pre><code>def __len__(self) -&gt; int\n</code></pre> <p>Get the length of the message content.</p>"},{"location":"reference/components/messages/#__getitem__","title":"<code>__getitem__</code>","text":"<pre><code>def __getitem__(self, index) -&gt; BaseMessage\n</code></pre> <p>Get a slice of the message content.</p>"},{"location":"reference/components/messages/#__add__-__radd__","title":"<code>__add__</code> / <code>__radd__</code>","text":"<pre><code>def __add__(self, other: str) -&gt; BaseMessage\ndef __radd__(self, other: str) -&gt; BaseMessage\n</code></pre> <p>Concatenate strings with message content.</p>"},{"location":"reference/components/messages/#message-types","title":"Message Types","text":""},{"location":"reference/components/messages/#systemmessage","title":"SystemMessage","text":"<p>Message from the system (system prompt).</p> <pre><code>class SystemMessage(BaseMessage):\n    \"\"\"A message from the system.\"\"\"\n\n    content: str\n    role: str = \"system\"\n</code></pre>"},{"location":"reference/components/messages/#example","title":"Example","text":"<pre><code>from llamabot.components.messages import SystemMessage\n\nmsg = SystemMessage(content=\"You are a helpful assistant.\")\n</code></pre>"},{"location":"reference/components/messages/#humanmessage","title":"HumanMessage","text":"<p>Message from a human user.</p> <pre><code>class HumanMessage(BaseMessage):\n    \"\"\"A message from a human.\"\"\"\n\n    content: str\n    role: str = \"user\"\n</code></pre>"},{"location":"reference/components/messages/#example_1","title":"Example","text":"<pre><code>from llamabot.components.messages import HumanMessage\n\nmsg = HumanMessage(content=\"What is Python?\")\n</code></pre>"},{"location":"reference/components/messages/#aimessage","title":"AIMessage","text":"<p>Message from the AI assistant.</p> <pre><code>class AIMessage(BaseMessage):\n    \"\"\"A message from the AI.\"\"\"\n\n    content: str\n    role: str = \"assistant\"\n</code></pre>"},{"location":"reference/components/messages/#example_2","title":"Example","text":"<pre><code>from llamabot.components.messages import AIMessage\n\nmsg = AIMessage(content=\"Python is a programming language.\")\n</code></pre>"},{"location":"reference/components/messages/#developermessage","title":"DeveloperMessage","text":"<p>Message from the developer (for development context).</p> <pre><code>class DeveloperMessage(BaseMessage):\n    \"\"\"A message from the developer.\"\"\"\n\n    content: str\n    role: str = \"developer\"\n</code></pre>"},{"location":"reference/components/messages/#example_3","title":"Example","text":"<pre><code>from llamabot.components.messages import DeveloperMessage\n\nmsg = DeveloperMessage(content=\"Add error handling to this function.\")\n</code></pre>"},{"location":"reference/components/messages/#toolmessage","title":"ToolMessage","text":"<p>Message representing tool execution results.</p> <pre><code>class ToolMessage(BaseMessage):\n    \"\"\"A message from a tool.\"\"\"\n\n    content: str\n    role: str = \"tool\"\n</code></pre>"},{"location":"reference/components/messages/#example_4","title":"Example","text":"<pre><code>from llamabot.components.messages import ToolMessage\n\nmsg = ToolMessage(content=\"Tool execution result\")\n</code></pre>"},{"location":"reference/components/messages/#retrievedmessage","title":"RetrievedMessage","text":"<p>Message retrieved from document store or memory.</p> <pre><code>class RetrievedMessage(BaseMessage):\n    \"\"\"A message retrieved from the history.\"\"\"\n\n    content: str\n    role: str = \"system\"\n</code></pre>"},{"location":"reference/components/messages/#example_5","title":"Example","text":"<pre><code>from llamabot.components.messages import RetrievedMessage\n\nmsg = RetrievedMessage(content=\"Retrieved document content\")\n</code></pre>"},{"location":"reference/components/messages/#imagemessage","title":"ImageMessage","text":"<p>Message containing an image.</p> <pre><code>class ImageMessage(BaseMessage):\n    \"\"\"A message containing an image.\"\"\"\n\n    content: str  # Base64-encoded image or file path\n    role: str = \"user\"\n</code></pre>"},{"location":"reference/components/messages/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    content: Union[str, Path],\n    role: str = \"user\",\n    prompt_hash: str | None = None,\n    tool_calls: list = None,\n)\n</code></pre>"},{"location":"reference/components/messages/#parameters","title":"Parameters","text":"<ul> <li>content (<code>Union[str, Path]</code>): Path to image file, URL of image, or base64-encoded image</li> <li>role (<code>str</code>, default: <code>\"user\"</code>): Role of the message sender</li> <li>prompt_hash (<code>str | None</code>, default: <code>None</code>): Optional prompt hash</li> <li>tool_calls (<code>list</code>, default: <code>None</code>): Optional tool calls</li> </ul>"},{"location":"reference/components/messages/#example_6","title":"Example","text":"<pre><code>from llamabot.components.messages import ImageMessage\nfrom pathlib import Path\n\n# From file path\nmsg = ImageMessage(content=Path(\"image.jpg\"))\n\n# From URL\nmsg = ImageMessage(content=\"https://example.com/image.jpg\")\n</code></pre>"},{"location":"reference/components/messages/#helper-functions","title":"Helper Functions","text":""},{"location":"reference/components/messages/#user","title":"<code>user</code>","text":"<pre><code>def user(*content: Union[str, Path]) -&gt; Union[HumanMessage, ImageMessage, list]\n</code></pre> <p>Create user messages, automatically detecting images.</p>"},{"location":"reference/components/messages/#parameters_1","title":"Parameters","text":"<ul> <li>content: One or more strings or image paths</li> </ul>"},{"location":"reference/components/messages/#returns","title":"Returns","text":"<ul> <li>Union[HumanMessage, ImageMessage, list]: Message(s) created from content</li> </ul>"},{"location":"reference/components/messages/#example_7","title":"Example","text":"<pre><code>from llamabot.components.messages import user\n\n# Text message\nmsg = user(\"What is this?\")\n\n# Image message\nmsg = user(\"/path/to/image.jpg\")\n\n# Text and image\nmsgs = user(\"What is this?\", \"/path/to/image.jpg\")\n</code></pre>"},{"location":"reference/components/messages/#dev","title":"<code>dev</code>","text":"<pre><code>def dev(*content: str) -&gt; Union[DeveloperMessage, list[DeveloperMessage]]\n</code></pre> <p>Create developer messages for development context.</p>"},{"location":"reference/components/messages/#parameters_2","title":"Parameters","text":"<ul> <li>content: One or more strings</li> </ul>"},{"location":"reference/components/messages/#returns_1","title":"Returns","text":"<ul> <li>Union[DeveloperMessage, list[DeveloperMessage]]: Developer message(s)</li> </ul>"},{"location":"reference/components/messages/#example_8","title":"Example","text":"<pre><code>from llamabot.components.messages import dev\n\nmsg = dev(\"Add error handling\")\nmsgs = dev(\"Refactor code\", \"Add tests\")\n</code></pre>"},{"location":"reference/components/messages/#system","title":"<code>system</code>","text":"<pre><code>def system(content: str) -&gt; SystemMessage\n</code></pre> <p>Create a system message.</p>"},{"location":"reference/components/messages/#parameters_3","title":"Parameters","text":"<ul> <li>content (<code>str</code>): System prompt content</li> </ul>"},{"location":"reference/components/messages/#returns_2","title":"Returns","text":"<ul> <li>SystemMessage: System message</li> </ul>"},{"location":"reference/components/messages/#example_9","title":"Example","text":"<pre><code>from llamabot.components.messages import system\n\nmsg = system(\"You are a helpful assistant.\")\n</code></pre>"},{"location":"reference/components/messages/#to_basemessage","title":"<code>to_basemessage</code>","text":"<pre><code>def to_basemessage(\n    messages: Union[\n        str,\n        BaseMessage,\n        list[Union[str, BaseMessage]],\n        Callable,\n    ]\n) -&gt; list[BaseMessage]\n</code></pre> <p>Convert various input types to a list of BaseMessage objects.</p>"},{"location":"reference/components/messages/#parameters_4","title":"Parameters","text":"<ul> <li>messages: Can be:</li> <li>String (converted to HumanMessage)</li> <li>BaseMessage object</li> <li>List of strings or BaseMessage objects</li> <li>Callable function (called and result converted)</li> </ul>"},{"location":"reference/components/messages/#returns_3","title":"Returns","text":"<ul> <li>list[BaseMessage]: List of BaseMessage objects</li> </ul>"},{"location":"reference/components/messages/#example_10","title":"Example","text":"<pre><code>from llamabot.components.messages import to_basemessage, HumanMessage\n\n# String\nmsgs = to_basemessage(\"Hello\")\n\n# Message object\nmsgs = to_basemessage(HumanMessage(content=\"Hello\"))\n\n# List\nmsgs = to_basemessage([\"Hello\", \"World\"])\n\n# Mixed\nmsgs = to_basemessage([\n    \"Hello\",\n    HumanMessage(content=\"World\")\n])\n</code></pre>"},{"location":"reference/components/messages/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/components/messages/#creating-messages","title":"Creating Messages","text":"<pre><code>from llamabot.components.messages import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage,\n    user,\n    dev,\n    system\n)\n\n# System prompt\nsys_msg = system(\"You are a helpful assistant.\")\n\n# User message\nuser_msg = user(\"What is Python?\")\n\n# AI response\nai_msg = AIMessage(content=\"Python is a programming language.\")\n</code></pre>"},{"location":"reference/components/messages/#with-images","title":"With Images","text":"<pre><code>from llamabot.components.messages import user, ImageMessage\nfrom pathlib import Path\n\n# Using user() helper\nmsg = user(\"/path/to/image.jpg\")\n\n# Direct ImageMessage\nmsg = ImageMessage(content=Path(\"image.jpg\"))\n\n# Text and image\nmsgs = user(\"What is this?\", \"/path/to/image.jpg\")\n</code></pre>"},{"location":"reference/components/messages/#with-bots","title":"With Bots","text":"<pre><code>import llamabot as lmb\nfrom llamabot.components.messages import HumanMessage\n\nbot = lmb.SimpleBot(\"You are helpful.\")\n\n# String (automatically converted)\nresponse = bot(\"Hello\")\n\n# Message object\nresponse = bot(HumanMessage(content=\"Hello\"))\n\n# Multiple messages\nresponse = bot(\"First message\", \"Second message\")\n</code></pre>"},{"location":"reference/components/messages/#message-hierarchy","title":"Message Hierarchy","text":"<pre><code>BaseMessage (base class)\n\u251c\u2500\u2500 SystemMessage (system prompts)\n\u251c\u2500\u2500 HumanMessage (user input)\n\u251c\u2500\u2500 DeveloperMessage (development context)\n\u251c\u2500\u2500 AIMessage (AI responses)\n\u2502   \u251c\u2500\u2500 ThoughtMessage (agent reasoning)\n\u2502   \u2514\u2500\u2500 ObservationMessage (tool observations)\n\u251c\u2500\u2500 ToolMessage (tool execution results)\n\u251c\u2500\u2500 RetrievedMessage (retrieved documents)\n\u2514\u2500\u2500 ImageMessage (image content)\n</code></pre>"},{"location":"reference/components/messages/#related-classes","title":"Related Classes","text":"<ul> <li>SimpleBot: Uses messages for LLM communication</li> <li>ChatMemory: Stores and retrieves messages</li> <li>QueryBot: Uses RetrievedMessage for document retrieval</li> </ul>"},{"location":"reference/components/messages/#see-also","title":"See Also","text":"<ul> <li>SimpleBot Reference</li> <li>Chat Memory Reference</li> <li>Which Bot Should I Use?</li> </ul>"},{"location":"reference/components/tools/","title":"Tools API Reference","text":"<p>The tools system provides function calling capabilities for bots, with built-in observability and AgentBot integration.</p>"},{"location":"reference/components/tools/#tool-decorator","title":"Tool Decorator","text":"<pre><code>@tool(\n    *,\n    loopback_name: Optional[str] = \"decide\",\n    span: bool = True,\n    exclude_args: Optional[List[str]] = None,\n    operation_name: Optional[str] = None,\n    **span_attributes,\n)\n</code></pre> <p>Decorator to create a tool from a function. Provides built-in AgentBot integration and observability.</p>"},{"location":"reference/components/tools/#parameters","title":"Parameters","text":"<ul> <li> <p>loopback_name (<code>Optional[str]</code>, default: <code>\"decide\"</code>): Controls whether execution continues after this tool. Set to <code>None</code> for terminal tools that end the workflow. Defaults to <code>\"decide\"</code> to loop back to the decision node.</p> </li> <li> <p>span (<code>bool</code>, default: <code>True</code>): Whether to apply <code>@span</code> decorator for observability. Enables automatic logging of tool calls.</p> </li> <li> <p>exclude_args (<code>Optional[List[str]]</code>, default: <code>None</code>): Span parameter: exclude specific arguments from logging. Useful for sensitive data like API keys or passwords.</p> </li> <li> <p>operation_name (<code>Optional[str]</code>, default: <code>None</code>): Span parameter: custom operation name for logging. If not provided, uses the function name.</p> </li> <li> <p>span_attributes: Additional span attributes passed to <code>@span</code> decorator.</p> </li> </ul>"},{"location":"reference/components/tools/#returns","title":"Returns","text":"<ul> <li>Callable: The decorated function with:</li> <li><code>json_schema</code> attribute: JSON schema for function calling</li> <li><code>func</code> attribute: AgentBot integration function</li> <li><code>loopback_name</code> attribute: Loopback configuration</li> </ul>"},{"location":"reference/components/tools/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/components/tools/#basic-tool","title":"Basic Tool","text":"<pre><code>from llamabot.components.tools import tool\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the current weather for a city.\n\n    :param city: The name of the city\n    :return: Weather information\n    \"\"\"\n    return f\"The weather in {city} is sunny, 72\u00b0F\"\n</code></pre>"},{"location":"reference/components/tools/#terminal-tool","title":"Terminal Tool","text":"<pre><code>from llamabot.components.tools import tool\n\n@tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to the user with a message.\n\n    :param response: The message to send\n    :return: The response message\n    \"\"\"\n    return response\n</code></pre>"},{"location":"reference/components/tools/#tool-with-excluded-arguments","title":"Tool with Excluded Arguments","text":"<pre><code>from llamabot.components.tools import tool\n\n@tool(exclude_args=[\"api_key\", \"password\"])\ndef secure_tool(api_key: str, password: str, data: str) -&gt; str:\n    \"\"\"Process data securely.\n\n    :param api_key: API key (excluded from logs)\n    :param password: Password (excluded from logs)\n    :param data: Data to process\n    :return: Processed data\n    \"\"\"\n    return f\"Processed: {data}\"\n</code></pre>"},{"location":"reference/components/tools/#custom-operation-name","title":"Custom Operation Name","text":"<pre><code>from llamabot.components.tools import tool\n\n@tool(operation_name=\"custom_weather_lookup\")\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather.\"\"\"\n    return f\"Weather in {city}\"\n</code></pre>"},{"location":"reference/components/tools/#tool-requirements","title":"Tool Requirements","text":""},{"location":"reference/components/tools/#function-signature","title":"Function Signature","text":"<p>Tools must have:</p> <ul> <li>Type annotations for all parameters</li> <li>Return type annotation</li> <li>Docstring with parameter descriptions</li> </ul>"},{"location":"reference/components/tools/#docstring-format","title":"Docstring Format","text":"<p>Use Sphinx-style docstrings:</p> <pre><code>@tool\ndef my_tool(param1: str, param2: int) -&gt; str:\n    \"\"\"Tool description.\n\n    :param param1: Description of param1\n    :param param2: Description of param2\n    :return: Description of return value\n    \"\"\"\n    return result\n</code></pre>"},{"location":"reference/components/tools/#default-tools","title":"Default Tools","text":"<p>The following tools are automatically available in AgentBot and ToolBot:</p>"},{"location":"reference/components/tools/#today_date","title":"<code>today_date</code>","text":"<pre><code>@tool\ndef today_date() -&gt; str:\n    \"\"\"Returns the current date in YYYY-MM-DD format.\"\"\"\n</code></pre> <p>Returns the current date. Loops back to decision node.</p>"},{"location":"reference/components/tools/#respond_to_user","title":"<code>respond_to_user</code>","text":"<pre><code>@tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to the user with a message.\"\"\"\n</code></pre> <p>Sends a text response to the user. Terminal tool (no loopback).</p>"},{"location":"reference/components/tools/#return_object_to_user","title":"<code>return_object_to_user</code>","text":"<pre><code>@tool(loopback_name=None)\ndef return_object_to_user(variable_name: str, _globals_dict: dict = None) -&gt; Any:\n    \"\"\"Return an object from the calling context's globals.\"\"\"\n</code></pre> <p>Returns an object from the calling context's globals dictionary. Terminal tool (no loopback).</p>"},{"location":"reference/components/tools/#inspect_globals","title":"<code>inspect_globals</code>","text":"<pre><code>@tool\ndef inspect_globals(_globals_dict: dict = None) -&gt; str:\n    \"\"\"Inspect available global variables.\"\"\"\n</code></pre> <p>Inspects available global variables. Loops back to decision node.</p>"},{"location":"reference/components/tools/#tool-functions","title":"Tool Functions","text":""},{"location":"reference/components/tools/#write_and_execute_code","title":"<code>write_and_execute_code</code>","text":"<pre><code>def write_and_execute_code(globals_dict: dict) -&gt; Callable\n</code></pre> <p>Creates a tool for executing Python code with access to global variables.</p>"},{"location":"reference/components/tools/#parameters_1","title":"Parameters","text":"<ul> <li>globals_dict (<code>dict</code>): Dictionary of global variables to make available to executed code (typically <code>globals()</code>)</li> </ul>"},{"location":"reference/components/tools/#returns_1","title":"Returns","text":"<ul> <li>Callable: A tool function that can execute Python code</li> </ul>"},{"location":"reference/components/tools/#example","title":"Example","text":"<pre><code>from llamabot.components.tools import write_and_execute_code\n\ncode_tool = write_and_execute_code(globals_dict=globals())\n\n# Use with ToolBot or AgentBot\nbot = lmb.ToolBot(\n    system_prompt=\"You are a code executor.\",\n    tools=[code_tool]\n)\n</code></pre>"},{"location":"reference/components/tools/#write_and_execute_script","title":"<code>write_and_execute_script</code>","text":"<pre><code>def write_and_execute_script(\n    code: str,\n    dependencies_str: str,\n    python_version: str,\n) -&gt; str\n</code></pre> <p>Executes Python scripts in a secure sandbox.</p>"},{"location":"reference/components/tools/#parameters_2","title":"Parameters","text":"<ul> <li>code (<code>str</code>): Python code to execute</li> <li>dependencies_str (<code>str</code>): Comma-separated list of dependencies</li> <li>python_version (<code>str</code>): Python version to use</li> </ul>"},{"location":"reference/components/tools/#returns_2","title":"Returns","text":"<ul> <li>str: Execution result</li> </ul>"},{"location":"reference/components/tools/#tool-schema","title":"Tool Schema","text":"<p>Tools automatically generate a JSON schema for function calling:</p> <pre><code>@tool\ndef my_tool(arg: str) -&gt; str:\n    return arg\n\n# Access the schema\nschema = my_tool.json_schema\n# {\n#     \"type\": \"function\",\n#     \"function\": {\n#         \"name\": \"my_tool\",\n#         \"description\": \"...\",\n#         \"parameters\": {...}\n#     }\n# }\n</code></pre>"},{"location":"reference/components/tools/#usage-with-bots","title":"Usage with Bots","text":""},{"location":"reference/components/tools/#toolbot","title":"ToolBot","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef calculate_sum(a: int, b: int) -&gt; int:\n    \"\"\"Calculate the sum of two numbers.\"\"\"\n    return a + b\n\nbot = lmb.ToolBot(\n    system_prompt=\"You are a calculator.\",\n    model_name=\"gpt-4o-mini\",\n    tools=[calculate_sum]\n)\n\ntool_calls = bot(\"Calculate 5 + 3\")\n</code></pre>"},{"location":"reference/components/tools/#agentbot","title":"AgentBot","text":"<pre><code>import llamabot as lmb\n\n@lmb.tool\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return results\n\n@lmb.tool(loopback_name=None)\ndef respond_to_user(response: str) -&gt; str:\n    \"\"\"Respond to user.\"\"\"\n    return response\n\nbot = lmb.AgentBot(\n    tools=[search_web, respond_to_user],\n    model_name=\"gpt-4o-mini\"\n)\n\nresult = bot(\"Search for AI news and summarize\")\n</code></pre>"},{"location":"reference/components/tools/#observability","title":"Observability","text":"<p>Tools decorated with <code>@tool</code> automatically include span-based logging:</p> <ul> <li>Tool calls are logged with parameters</li> <li>Results are logged</li> <li>Execution time is tracked</li> <li>Sensitive arguments can be excluded</li> </ul> <p>View logs using the log viewer:</p> <pre><code>llamabot logviewer\n</code></pre>"},{"location":"reference/components/tools/#best-practices","title":"Best Practices","text":"<ol> <li>Clear docstrings: Provide detailed descriptions of what tools do</li> <li>Type annotations: Always include type hints for all parameters</li> <li>Exclude sensitive data: Use <code>exclude_args</code> for API keys, passwords, etc.</li> <li>Terminal tools: Use <code>loopback_name=None</code> for tools that end workflows</li> <li>Error handling: Include proper error handling in tool implementations</li> </ol>"},{"location":"reference/components/tools/#related-classes","title":"Related Classes","text":"<ul> <li>ToolBot: Single-turn tool execution bot</li> <li>AgentBot: Multi-turn agent with tool orchestration</li> <li>DecideNode: Decision-making node for AgentBot</li> </ul>"},{"location":"reference/components/tools/#see-also","title":"See Also","text":"<ul> <li>ToolBot Reference</li> <li>AgentBot Reference</li> <li>ToolBot Tutorial</li> <li>AgentBot Tutorial</li> </ul>"},{"location":"releases/v0.0.10/","title":"V0.0.10","text":""},{"location":"releases/v0.0.10/#0010","title":"0.0.10","text":"<p>This new version introduces experiments with Llamahub document loaders, updates workspace settings, and makes Panel an official dependency. It also includes a new Panel example and a correction in the docstring for accuracy.</p>"},{"location":"releases/v0.0.10/#new-features","title":"New Features","text":"<ul> <li>Experiments with Llamahub document loaders have been added to enhance functionality (8faea4) (Eric Ma)</li> <li>Workspace settings have been updated to improve user experience (3b5522) (Eric Ma)</li> <li>Panel has been made an official dependency to streamline the software's requirements (781906) (Eric Ma)</li> <li>A new Panel example has been added to provide users with more comprehensive usage scenarios (2209cd) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The docstring has been edited for correctness to ensure accurate documentation (0ee1ca) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.10/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.11/","title":"V0.0.11","text":""},{"location":"releases/v0.0.11/#version-0011","title":"Version 0.0.11","text":"<p>This version introduces automatic recording of prompts, improves the recording process, and verifies its functionality. It also includes a cleanup of notebooks and adds loguru as a dependency.</p>"},{"location":"releases/v0.0.11/#new-features","title":"New Features","text":"<ul> <li>Added automatic recording of prompts (2c956f) (Eric Ma)</li> <li>Improved automatic recording of prompts (50779c) (Eric Ma)</li> <li>Verified that the recorder works (aa428a) (Eric Ma)</li> <li>Added loguru as a dependency (23ee02) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed return type (3986b7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#other-changes","title":"Other Changes","text":"<ul> <li>Cleaned up notebooks (3c5a74) (Eric Ma)</li> <li>Reorganized notebook structure (4bee78) (Eric Ma)</li> <li>Enabled context manager for recording prompt-response pairs (e6a8b4) (Eric Ma)</li> <li>Settled on a stuff-the-text-into-prompt pattern rather than synthesizing and refining response. This makes things faster (7020d0) (Eric Ma)</li> <li>Enabled arbitrary loading of documents, not just text files (e4223c) (Eric Ma)</li> <li>Switched to using servable for feynman example (e50e45) (Eric Ma)</li> <li>Disabled test mode. A different way to make mock API calls work will be found (7a7beb) (Eric Ma)</li> <li>More experiments with llamahub loaders (4b2871) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.11/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.12/","title":"V0.0.12","text":""},{"location":"releases/v0.0.12/#version-0012","title":"Version 0.0.12","text":"<p>This new version introduces a panel representation of a recorder class and includes a version bump.</p>"},{"location":"releases/v0.0.12/#new-features","title":"New Features","text":"<ul> <li>Added a panel representation of a recorder class to enhance the user interface and functionality (18ae007) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.12/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.12/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.13/","title":"V0.0.13","text":""},{"location":"releases/v0.0.13/#version-0013","title":"Version 0.0.13","text":"<p>This new version includes an update to the example about querying PDFs and a bug fix related to query nodes. The version has been bumped from 0.0.12 to 0.0.13.</p>"},{"location":"releases/v0.0.13/#new-features","title":"New Features","text":"<ul> <li>The example about querying PDFs has been updated to provide more clarity and better understanding (8169ce) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.13/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed a bug where query nodes were hard-coded to 3, which limited the flexibility of the system. Now, the number of query nodes is dynamic (6802ac) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.13/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.14/","title":"V0.0.14","text":""},{"location":"releases/v0.0.14/#version-0014","title":"Version 0.0.14","text":"<p>This new version includes several enhancements and bug fixes to improve the overall performance and user experience of the Llamabot. The Python environment and llama_index versions have been pinned for stability. The chatbot panel app now uses a width of 600 pixels for better UI. The Querybot system message now applies SEMBR for improved readability.</p>"},{"location":"releases/v0.0.14/#new-features","title":"New Features","text":"<ul> <li>The chatbot panel app now uses a width of 600 pixels for a more user-friendly interface (7e2f05) (Eric Ma)</li> <li>Faux chat history of length 6000 tokens is now used as context for further responses in chatbot, enhancing the chatbot's response accuracy (02ef9d) (Eric Ma)</li> <li>SEMBR has been applied on the Querybot system message for improved readability (b3c53c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.14/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed UI for a smoother user experience (733759) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.14/#deprecations","title":"Deprecations","text":"<ul> <li>The version of the Python environment has been pinned to 3.9 to ensure compatibility and stability (5db1ae) (Eric Ma)</li> <li>The version of llama_index has been pinned for stability (930cbb) (Eric Ma)</li> <li>Temporarily settled on an older version of langchain for the time being (af0938) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.14/#refactors","title":"Refactors","text":"<ul> <li>Refactored Querybot to allow loading of documents later, enhancing the flexibility of the system (3103d9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.15/","title":"V0.0.15","text":""},{"location":"releases/v0.0.15/#0015","title":"0.0.15","text":"<p>This new version includes an update to the environment to fix bokeh versions and a version bump.</p>"},{"location":"releases/v0.0.15/#new-features","title":"New Features","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.15/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed bokeh versions in the environment to ensure compatibility and smooth operation (8c48f7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.15/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.16/","title":"V0.0.16","text":""},{"location":"releases/v0.0.16/#0016","title":"0.0.16","text":"<p>This new version includes an update to the pyproject.toml file and a version bump from 0.0.15 to 0.0.16.</p>"},{"location":"releases/v0.0.16/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.0.16/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes were made in this version.</li> </ul>"},{"location":"releases/v0.0.16/#updates","title":"Updates","text":"<ul> <li>The pyproject.toml file was updated (3773a5) (Eric Ma)</li> <li>The version was bumped from 0.0.15 to 0.0.16 (b55951) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.16/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were made in this version.</li> </ul>"},{"location":"releases/v0.0.17/","title":"V0.0.17","text":""},{"location":"releases/v0.0.17/#version-0017","title":"Version 0.0.17","text":"<p>This new version includes updates to the versions of bokeh used in the project. The bokeh version has been pinned to ensure compatibility and stability of the project.</p>"},{"location":"releases/v0.0.17/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.0.17/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The versions of bokeh used in the project have been updated to ensure compatibility and stability. (96a89e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.17/#deprecations","title":"Deprecations","text":"<ul> <li>The bokeh version has been pinned to less than or equal to 3.1.0 to prevent potential issues with future versions. (2a93d9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.18/","title":"V0.0.18","text":""},{"location":"releases/v0.0.18/#0018","title":"0.0.18","text":"<p>This version does not include any significant changes, as the commit was a work-in-progress save.</p>"},{"location":"releases/v0.0.18/#other-changes","title":"Other Changes","text":"<ul> <li>Work-in-progress save (1ef5bf1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.19/","title":"V0.0.19","text":""},{"location":"releases/v0.0.19/#0019","title":"0.0.19","text":"<p>This version includes a bug fix related to the insertion of documents into the index.</p>"},{"location":"releases/v0.0.19/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed bug with insert_documents_into_index (d788ed9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.20/","title":"V0.0.20","text":""},{"location":"releases/v0.0.20/#0020","title":"0.0.20","text":"<p>This version includes the deletion of a codepath where the index was set to None.</p>"},{"location":"releases/v0.0.20/#deprecations","title":"Deprecations","text":"<ul> <li>Deleted codepath where index was set to None (f0443e9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.21/","title":"V0.0.21","text":""},{"location":"releases/v0.0.21/#0021","title":"0.0.21","text":"<p>This version includes the addition of a new scratch notebook on querying JMLR paper and two default processors.</p>"},{"location":"releases/v0.0.21/#new-features","title":"New Features","text":"<ul> <li>New scratch notebook on querying JMLR paper added (d05c539) (Eric Ma)</li> <li>Two default processors added (408f0d0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.22/","title":"V0.0.22","text":""},{"location":"releases/v0.0.22/#0022","title":"0.0.22","text":"<p>This version includes the addition of outlines to the pyproject.toml file and the completion of an example notebook on doing a paper review.</p>"},{"location":"releases/v0.0.22/#new-features","title":"New Features","text":"<ul> <li>Outlines added to pyproject.toml (d1ff517) (Eric Ma)</li> <li>Completed example notebook on doing a paper review (324d353) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.23/","title":"V0.0.23","text":""},{"location":"releases/v0.0.23/#0023","title":"0.0.23","text":"<p>This new version includes a variety of enhancements and new features, including the addition of a coding bot panel app, a refactor of the CLI, and updates to the notebooks and tutorial materials. The version also includes the integration of pyzotero to dependencies and the beginning of a prototype for coding and diffbots in a library of bots.</p>"},{"location":"releases/v0.0.23/#new-features","title":"New Features","text":"<ul> <li>Added a coding bot panel app (a139420) (Eric Ma)</li> <li>Refactored the CLI for better performance and usability (b067193) (Eric Ma)</li> <li>Updated the scratch notebook (c0a4e2b) (Eric Ma)</li> <li>Added a code tutorial notebook to help users understand how to use the application (ae42c64) (Eric Ma)</li> <li>Added instructions in the codingbot source file to provide more context for the code tutorial bot (79236f) (Eric Ma)</li> <li>Added more notebooks to save work and enhance the user experience (891e570) (Eric Ma)</li> <li>Added pyzotero to dependencies to enhance the functionality of the application (1d55eb2) (Eric Ma)</li> <li>Began the prototype of coding and diffbots in a library of bots to expand the application's capabilities (f26c789) (Eric Ma)</li> <li>Updated the blog text demo for better user understanding (ae9c539) (Eric Ma)</li> <li>Updated base bots to use the updated llamaindex for improved performance (baad16c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.23/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the placement of noqa DAR101 for better code quality (d73b14f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.23/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.24/","title":"V0.0.24","text":""},{"location":"releases/v0.0.24/#0024","title":"0.0.24","text":"<p>This new version introduces the ability to configure the port and address, providing more flexibility and control to the users.</p>"},{"location":"releases/v0.0.24/#new-features","title":"New Features","text":"<ul> <li>Made port and address configurable, allowing users to set these parameters as per their requirements (0920e4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.24/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.24/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.25/","title":"V0.0.25","text":""},{"location":"releases/v0.0.25/#0025","title":"0.0.25","text":"<p>This new version includes improvements to the basecallbackmanager and the pinned version of langchain.</p>"},{"location":"releases/v0.0.25/#new-features","title":"New Features","text":"<ul> <li>The basecallbackmanager now requires an explicit handlers argument, enhancing clarity and reducing potential for errors (169f72) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.25/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.25/#deprecations","title":"Deprecations","text":"<ul> <li>The version of langchain used in the project has been pinned, ensuring stability and compatibility with this version of the software (631c29) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.26/","title":"V0.0.26","text":""},{"location":"releases/v0.0.26/#0026","title":"0.0.26","text":"<p>This new version includes updates to the dependency management of the project. The version of the project has been bumped up from 0.0.25 to 0.0.26.</p>"},{"location":"releases/v0.0.26/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.0.26/#updates","title":"Updates","text":"<ul> <li>The version of the project has been updated from 0.0.25 to 0.0.26 (894c25) (Eric Ma)</li> <li>The versions of the dependencies have been unpinned, allowing for the latest versions to be used (bc8d64) (Eric Ma)</li> <li>The 'langchain' dependency has been pinned to an exact version to ensure compatibility and stability (2b69d0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.26/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes were made in this version.</li> </ul>"},{"location":"releases/v0.0.26/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were made in this version.</li> </ul>"},{"location":"releases/v0.0.27/","title":"V0.0.27","text":""},{"location":"releases/v0.0.27/#0027","title":"0.0.27","text":"<p>This new version includes several important updates to improve the functionality and security of the Llamabot application.</p>"},{"location":"releases/v0.0.27/#new-features","title":"New Features","text":"<ul> <li>All websocket origins are now allowed, enhancing the connectivity and compatibility of the application (a685f0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.27/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the issue with the Querybot index retriever by updating it with the refactored Llamaindex code. This should improve the accuracy and efficiency of the index retriever (12a87b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.27/#deprecations","title":"Deprecations","text":"<ul> <li>The opening of the app has been disabled. This is a temporary measure for security reasons and will be addressed in future updates (e6e1a4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.28/","title":"V0.0.28","text":""},{"location":"releases/v0.0.28/#version-0028","title":"Version 0.0.28","text":"<p>This version includes several enhancements and new features, including the addition of new modules and functions, improvements to documentation, and minor updates to notebooks.</p>"},{"location":"releases/v0.0.28/#new-features","title":"New Features","text":"<ul> <li>Added <code>pyperclip</code> to environment spec and requirements (e57f7f) (Eric Ma)</li> <li>Added additional config path for llamabot's OpenAI API key (713579) (Eric Ma)</li> <li>Added function for getting function source from a .py file (07e402) (Eric Ma)</li> <li>Added <code>get_valid_input</code> for user prompts (b001d2) (Eric Ma)</li> <li>Initial commit of llamabot's python bot (e5c67d) (Eric Ma)</li> <li>Added <code>read_file</code> to the <code>file_finder.py</code> source file (5b44d5) (Eric Ma)</li> <li>Added <code>PromptRecorder</code> module documentation (2f208e) (Eric Ma)</li> <li>Added explanation of where <code>autorecord</code> function is supposed to be used (b7a02c) (Eric Ma)</li> <li>Added doc processor documentation (be1bf8) (Eric Ma)</li> <li>Added in tutorials ghostwritten by GPT4 (ace0cd) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.28/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Minor update to scratch notebooks (dc1f76) (Eric Ma)</li> <li>Minor changes on doc processor (4601eb) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.28/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.28/#documentation","title":"Documentation","text":"<ul> <li>Updated module-level docstring for CLI init (ef812f) (Eric Ma)</li> <li>Added module-level docstring prompt (7be6ad) (Eric Ma)</li> <li>Updated docstring based on Llamabot (6d9829) (Eric Ma)</li> <li>Improved <code>doc_processor</code> docstring (fb98de) (Eric Ma)</li> <li>Moved notebooks to examples under docs (998b2d) (Eric Ma)</li> <li>Added note on doc processor documentation that it was written by GPT4 (83abc9) (Eric Ma)</li> <li>Added notes about tutorial being written by GPT in an official modal (4bb2a4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.29/","title":"V0.0.29","text":""},{"location":"releases/v0.0.29/#version-0029","title":"Version 0.0.29","text":"<p>This version includes a number of new features, improvements, and bug fixes. The main focus of this release was to enhance the testing environment, improve code generation, and add new functionalities.</p>"},{"location":"releases/v0.0.29/#new-features","title":"New Features","text":"<ul> <li>Added float_to_top = true for isort in pyproject.toml config (0c58f8) (Eric Ma)</li> <li>Added tests for doc_processor (d679a0) (Eric Ma)</li> <li>Modified prompt to ensure that docstring indentation is done correctly (1b1ade) (Eric Ma)</li> <li>Added functions replace_object_in_file and insert_docstring (adf44a) (Eric Ma)</li> <li>Added dummy module for experimentation purposes (f27a71) (Eric Ma)</li> <li>Added validation of chunk_overlap value (9d2070) (Eric Ma)</li> <li>Added tests for get_valid_input (0a984b) (Eric Ma)</li> <li>Added comment (511953) (Eric Ma)</li> <li>Added tests for recorder (f38341) (Eric Ma)</li> <li>Added tests for file_finder.py (cfc296) (Eric Ma)</li> <li>Added testwriting functionality (435aa4) (Eric Ma)</li> <li>Added python dotenv to pyproject.toml (0339c2) (Eric Ma)</li> <li>Added more testing deps into the environment (b6bd32) (Eric Ma)</li> <li>Added pytest to gh bare testing env (bef9a6) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.29/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed type hint based on test (8864e0) (Eric Ma)</li> <li>Changed python test version to 3.11 (b4bb40) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.29/#deprecations","title":"Deprecations","text":"<ul> <li>Got rid of typer-cli (b7a187) (Eric Ma)</li> <li>Removed requirements.txt build step (27eea3) (Eric Ma)</li> <li>Removed call test (0578cf) (Eric Ma)</li> <li>Removed model testing (28e8bd) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.29/#improvements","title":"Improvements","text":"<ul> <li>Bumped version from 0.0.28 to 0.0.29 (ad955c) (Eric Ma)</li> <li>Minor changes to Python code generation CLI (3663f8) (Eric Ma)</li> <li>Manually installed testing dependencies (44b3de) (Eric Ma)</li> <li>Ensured bare environment has all optional dependencies installed (8118c1) (Eric Ma)</li> <li>Ensured typer version minimum 0.4.0 in pyproject.toml (8403ad) (Eric Ma)</li> <li>Updated environment.yml to pin typer to greater than 0.4.0 (0804a8) (Eric Ma)</li> <li>Ensured test ghostwriter has access to bigger file context (5ae1e6) (Eric Ma)</li> <li>Modified tests prompt (db9e19) (Eric Ma)</li> <li>Changed test prompt (41a46e) (Eric Ma)</li> <li>Upgraded docstring (e83505) (Eric Ma)</li> <li>Added more explicit validation checks on the presence of the openai API key (d1c508) (Eric Ma)</li> <li>Specified object type for markdown_object (4d3782) (Eric Ma)</li> <li>Upgraded python to 3.11 (55e457) (Eric Ma)</li> <li>Tried out github actions matrix (bc5360) (Eric Ma)</li> <li>Ensured tests run on all pushes to main (23c506) (Eric Ma)</li> <li>Changed version of panel and bokeh (bfeac4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.3/","title":"V0.0.3","text":""},{"location":"releases/v0.0.3/#003","title":"0.0.3","text":"<p>This new version includes some important changes to the project structure and setup.</p>"},{"location":"releases/v0.0.3/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.0.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes were made in this version.</li> </ul>"},{"location":"releases/v0.0.3/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the setup.py file, indicating a change in the project setup process (796163)</li> </ul>"},{"location":"releases/v0.0.30/","title":"V0.0.30","text":""},{"location":"releases/v0.0.30/#0030","title":"0.0.30","text":"<p>This new version includes an improved error handling mechanism, expanded test coverage, and a minor version bump.</p>"},{"location":"releases/v0.0.30/#new-features","title":"New Features","text":"<ul> <li>Expanded test coverage with the addition of ghostwriting tests (cdb0fe) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.30/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Improved error handling by changing from raising an error to raising a warning (30cc86) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.30/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.30/#other","title":"Other","text":"<ul> <li>Version bumped from 0.0.29 to 0.0.30 (73c1f0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.31/","title":"V0.0.31","text":""},{"location":"releases/v0.0.31/#0031","title":"0.0.31","text":"<p>This new version includes enhancements to the directory tree functionality and some minor changes.</p>"},{"location":"releases/v0.0.31/#new-features","title":"New Features","text":"<ul> <li>Added functionality to display the directory tree (e8248d) (Eric Ma)</li> <li>Ensured that the directory tree context is passed into the prompt (df93e5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.31/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.31/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.31/#other-changes","title":"Other Changes","text":"<ul> <li>More scratch work done (96b60b) (Eric Ma)</li> <li>Version bumped from 0.0.30 to 0.0.31 (36e55b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.32/","title":"V0.0.32","text":""},{"location":"releases/v0.0.32/#0032","title":"0.0.32","text":"<p>This new version includes some internal code reorganization for better encapsulation and security.</p>"},{"location":"releases/v0.0.32/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.0.32/#code-improvements","title":"Code Improvements","text":"<ul> <li>Moved 'codebot' and 'diffbot' into protected functions to enhance code security and maintainability (ce59a0c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.32/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes were included in this version.</li> </ul>"},{"location":"releases/v0.0.32/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were made in this version.</li> </ul>"},{"location":"releases/v0.0.33/","title":"V0.0.33","text":""},{"location":"releases/v0.0.33/#version-0033","title":"Version 0.0.33","text":"<p>This new version includes a significant change in the underlying parsing library, moving from astunparse to astor. This change is expected to improve the performance and reliability of the Llamabot.</p>"},{"location":"releases/v0.0.33/#new-features","title":"New Features","text":"<ul> <li>The version of Llamabot has been updated from 0.0.32 to 0.0.33 (2e4c61) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.33/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.33/#deprecations","title":"Deprecations","text":"<ul> <li>The use of astunparse has been deprecated and replaced with astor for better performance and reliability (326c59) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.34/","title":"V0.0.34","text":""},{"location":"releases/v0.0.34/#0034","title":"0.0.34","text":"<p>This version introduces several new features and improvements to the LlamaBot CLI, including the addition of git diff display and commit message generation, a repr method for the Dummy class, and handling for no staged changes in commit_message. It also includes several refactors and a documentation update.</p>"},{"location":"releases/v0.0.34/#new-features","title":"New Features","text":"<ul> <li>Added git diff display and commit message generation functionality to the LlamaBot CLI. This feature imports the get_git_diff function from llamabot.code_manipulation, creates a SimpleBot instance for commit message generation, defines a commit_message function with a text.prompt decorator, and calls commitbot with the generated commit message. (1a6104) (Eric Ma)</li> <li>Added a repr method to the Dummy class in dummy.py. This provides a string representation of the object, making it easier to inspect and debug instances of the Dummy class. (ae3e7c) (Eric Ma)</li> <li>Updated commit_message function in cli/git.py to check for staged changes before generating a commit message. If no staged changes are found, a message is printed and the function returns. The get_git_diff function in code_manipulation.py was also updated to return an empty string if there are no staged changes. (ed7a3d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.34/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed typos in the llamabot CLI git module. Changes include renaming <code>git.app</code> to <code>git.gitapp</code> in llamabot/cli/init.py, adding missing parentheses to decorators in llamabot/cli/git.py, and replacing \"docstring\" with \"commit message\" in the user prompt. (860930) (Eric Ma)</li> <li>Refactored Typer app and command decorators in git.py. The <code>app</code> was renamed to <code>gitapp</code> for better context, and decorators were updated to use the new <code>gitapp</code> variable. (f7af8b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.34/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the unnecessary hello command from the git.py file in the llamabot CLI. This simplifies the codebase and focuses on the core functionality. (8f0b9d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.34/#documentation","title":"Documentation","text":"<ul> <li>Added a detailed explanation of the Conventional Commits specification to the git.py file. This outlines the various commit types, scopes, and footers, as well as their correlation with Semantic Versioning. This information will help users understand the importance of following the Conventional Commits specification when crafting their commit messages. (ca9b1c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.35/","title":"V0.0.35","text":""},{"location":"releases/v0.0.35/#version-0035","title":"Version 0.0.35","text":"<p>This new version introduces an autocommit option to the commit_message function in llamabot/cli/git.py. This feature allows for automatic committing of changes using the generated commit message when the autocommit parameter is set to True.</p>"},{"location":"releases/v0.0.35/#new-features","title":"New Features","text":"<ul> <li>Added an autocommit option to the commit_message function in llamabot/cli/git.py. When set to True, changes are automatically committed using the generated commit message. (5c202a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.35/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.35/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.36/","title":"V0.0.36","text":""},{"location":"releases/v0.0.36/#0036","title":"0.0.36","text":"<p>This new version includes a refactor of the CLI to improve code readability and understanding.</p>"},{"location":"releases/v0.0.36/#new-features","title":"New Features","text":"<p>No new features were added in this version.</p>"},{"location":"releases/v0.0.36/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes were made in this version.</p>"},{"location":"releases/v0.0.36/#refactors","title":"Refactors","text":"<ul> <li>The <code>commit_message</code> function in <code>llamabot/cli/git.py</code> has been renamed to <code>commit</code> to better reflect its purpose of committing staged changes (ae7b10) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.36/#deprecations","title":"Deprecations","text":"<p>No deprecations were made in this version.</p>"},{"location":"releases/v0.0.37/","title":"V0.0.37","text":""},{"location":"releases/v0.0.37/#0037","title":"0.0.37","text":"<p>This new version introduces the GitPython dependency to the project, allowing for Git operations within the project.</p>"},{"location":"releases/v0.0.37/#new-features","title":"New Features","text":"<ul> <li>GitPython dependency added to both environment.yml and pyproject.toml files to support Git operations within the project (aa6140) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.37/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.37/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.38/","title":"V0.0.38","text":""},{"location":"releases/v0.0.38/#version-0038","title":"Version 0.0.38","text":"<p>This new version introduces a new dependency to the project, enhancing the project root path management.</p>"},{"location":"releases/v0.0.38/#new-features","title":"New Features","text":"<ul> <li>Added pyprojroot to the list of dependencies for better project root path management (69444d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.38/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.38/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.39/","title":"V0.0.39","text":""},{"location":"releases/v0.0.39/#version-0039","title":"Version 0.0.39","text":"<p>This new version introduces automatic push to origin after commit and adds pytest-mock to the pr-tests workflow.</p>"},{"location":"releases/v0.0.39/#new-features","title":"New Features","text":"<ul> <li>Automatic push to origin after commit has been added. This feature simplifies the workflow and ensures that changes are synced with the remote repository. (7a0151) (Eric Ma)</li> <li>Pytest-mock has been added to the pr-tests workflow. This enables mocking in test cases. (ab1cb0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.39/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.39/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.4/","title":"V0.0.4","text":""},{"location":"releases/v0.0.4/#version-004","title":"Version 0.0.4","text":"<p>This new version includes several updates to the documentation and build process, as well as the addition of an all-contributors section. The version also includes a fix to the build command.</p>"},{"location":"releases/v0.0.4/#new-features","title":"New Features","text":"<ul> <li>Added an all-contributors section to the documentation (9a65f2) (Eric Ma)</li> <li>Added an all-contributors badge to the project (71be05) (Eric Ma)</li> <li>Added all-contributors configuration to the project (7ae0d2) (Eric Ma)</li> <li>Updated the all-contributors specification to correct the project name and owner (3e841c) (Eric Ma)</li> <li>Switched to using docs/index.md for all-contributors (0aa0fe) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the build command for the project (6e8e65) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/#deprecations","title":"Deprecations","text":"<ul> <li>Temporarily removed the badge from the project (4bef48) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.4/#other-changes","title":"Other Changes","text":"<ul> <li>Removed unnecessary whitespace from the project (943ba4) (Eric Ma)</li> <li>Reformatted the allcontributors.rc file (8ae972) (Eric Ma)</li> <li>Updated the badge for the project (6c0850) (Eric Ma)</li> <li>Updated the table in the documentation (047481) (Eric Ma)</li> <li>Bumped the project version from 0.0.3 to 0.0.4 (943e5a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.40/","title":"V0.0.40","text":""},{"location":"releases/v0.0.40/#0040","title":"0.0.40","text":"<p>This new version introduces the addition of a new dependency to the project, enhancing its functionality.</p>"},{"location":"releases/v0.0.40/#new-features","title":"New Features","text":"<ul> <li>Added the frozenlist package to the dependencies list in pyproject.toml to support immutable lists in the project (a7d069) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.40/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.40/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.41/","title":"V0.0.41","text":""},{"location":"releases/v0.0.41/#0041","title":"0.0.41","text":"<p>This new version introduces a handy <code>version</code> command to the llamabot CLI and updates the bumpversion configuration.</p>"},{"location":"releases/v0.0.41/#new-features","title":"New Features","text":"<ul> <li>A new <code>version</code> command has been added to the llamabot CLI. This command prints the current version of the application. (a88028) (Eric Ma)</li> <li>The <code>.bumpversion.cfg</code> file has been updated to include <code>llamabot/version.py</code> for version updates. This ensures that the version number is updated consistently across the application. (a88028) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.41/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.41/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.42/","title":"V0.0.42","text":""},{"location":"releases/v0.0.42/#version-0042","title":"Version 0.0.42","text":"<p>This new version introduces an enhancement to the get_valid_input function and a new feature that allows users to manually edit the generated commit message using their system's default text editor.</p>"},{"location":"releases/v0.0.42/#new-features","title":"New Features","text":"<ul> <li>Manual commit message editing option has been added. Users can now manually edit the generated commit message using their system's default text editor. This is done by creating a temporary file with the generated message, opening it in the editor, and reading the edited message back into the script. The 'm' option is added to the user input prompt to trigger manual editing. (37baea) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.42/#enhancements","title":"Enhancements","text":"<ul> <li>The get_valid_input function in cli/utils has been refactored for better input validation. A valid_inputs parameter has been added to the function, the input prompt has been updated to include valid_inputs, and the input validation now checks against valid_inputs. The error message has also been updated to display valid_inputs options. (b32986) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.42/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.42/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.43/","title":"V0.0.43","text":""},{"location":"releases/v0.0.43/#version-0043","title":"Version 0.0.43","text":"<p>This new version includes several enhancements to the code workflow and code manipulation features, as well as an update to the default model_name in various bot classes.</p>"},{"location":"releases/v0.0.43/#new-features","title":"New Features","text":"<ul> <li>Added new code cells and autoreload in code_workflow.ipynb. This includes the addition of new empty code cells for future implementation, a placeholder in one of the cells, autoreload magic commands for a better development experience, and the importation and demonstration of the get_dependencies function usage (5f6880) (Eric Ma)</li> <li>Introduced the get_dependencies function to retrieve a list of dependencies for a specified object in a source file. Also fixed the return type annotation for the get_git_diff function and added a test case for the get_dependencies function in test_code_manipulation.py (2d816f) (Eric Ma)</li> <li>Updated the default model_name parameter value from \"gpt-4\" to \"gpt-4-32k\" in the constructors of ChatBot, QueryBot, and SimpleBot classes (c93ba3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.43/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.43/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.43/#refactors","title":"Refactors","text":"<ul> <li>Reorganized imports and improved test generation. This includes moving the <code>get_valid_input</code> import to the top of <code>llamabot/cli/git.py</code>, adding the <code>get_dependencies</code> import to <code>llamabot/cli/python.py</code>, and updating the <code>tests</code> function in <code>llamabot/prompt_library/coding.py</code> to include dependent source files for better test generation (f75202) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.44/","title":"V0.0.44","text":""},{"location":"releases/v0.0.44/#0044","title":"0.0.44","text":"<p>This new version includes a variety of enhancements and new features, including the addition of new notebooks, improvements to the Zotero and QueryBot functionalities, and the integration of Google Calendar API.</p>"},{"location":"releases/v0.0.44/#new-features","title":"New Features","text":"<ul> <li>Added blogging assistant and gcal notebooks for blog tagger, summarizer, and Google Calendar related tasks. Also, updated existing notebooks for cache and Zotero with new features and improvements (2378760) (Eric Ma)</li> <li>Implemented updates to all attendees on event creation and update in Google Calendar (57f80de) (Eric Ma)</li> <li>Refactored Zotero library handling and improved chat_paper functionality (b160982) (Eric Ma)</li> <li>Improved index handling and document processing in QueryBot (ea47ec9) (Eric Ma)</li> <li>Refactored sync function in Zotero and added chat_paper command (237631e) (Eric Ma)</li> <li>Added sync command to sync Zotero items to local JSON file (b1ccf350) (Eric Ma)</li> <li>Created .llamabot directory and updated config path (6e1b457) (Eric Ma)</li> <li>Added zotero integration and refactored configure function in CLI (0add77a) (Eric Ma)</li> <li>Added rich library to dependencies for better terminal output and formatting (d4c21d1) (Eric Ma)</li> <li>Added environment variable configuration in cli/utils (3ba93ef) (Eric Ma)</li> <li>Added initial CLI configuration for Zotero integration (8a3deb2) (Eric Ma)</li> <li>Added progress display for commit and push operations (9d9c86e) (Eric Ma)</li> <li>Added capture_errors decorator in google/utility_functions (12a7a3c) (Eric Ma)</li> <li>Added Google Calendar API integration (5c6ba9f) (Eric Ma)</li> <li>Added hashing for scopes and credentials in token file name in Google API (24d774b) (Eric Ma)</li> <li>Added llamabot_config_dir variable in config (2622ef3) (Eric Ma)</li> <li>Added convenience wrappers for Google API (0ea502d) (Eric Ma)</li> <li>Added tutorial bot and prompts in prompt_library (fe0d717) (Eric Ma)</li> <li>Added tutorial module to CLI (436374b) (Eric Ma)</li> <li>Added file_finder module documentation (dff0c71) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.44/#refactor","title":"Refactor","text":"<ul> <li>Simplified chat_paper function and improved document loading in Zotero (7f3e2d0) (Eric Ma)</li> <li>Updated index with a new document and set default values for chunk_size and chunk_overlap in QueryBot (02e21f8) (Eric Ma)</li> <li>Restructured Google API wrapper (88464cc) (Eric Ma)</li> <li>Separated credential loading from calendar service creation in Google API (39c038d) (Eric Ma)</li> <li>Moved code block in tests function in prompt_library (a2e5453) (Eric Ma)</li> <li>Improved commit message handling and progress display in cli/git (55e6aa8) (Eric Ma)</li> <li>Renamed test_coding.py to test_coding_prompt_library.py in tests (260c989) (Eric Ma)</li> <li>Centralized llamabotrc_paths and updated imports in config (f3af30d) (Eric Ma)</li> <li>Removed unused tutorial_writer function in cli (407f687) (Eric Ma)</li> <li>Updated commit message guidelines in prompt_library/git (185649c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.44/#docs","title":"Docs","text":"<ul> <li>Updated pip install command in python.md (84b079a) (Eric Ma)</li> <li>Added Llamabot Python CLI tutorial (d85d039) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.45/","title":"V0.0.45","text":""},{"location":"releases/v0.0.45/#0045","title":"0.0.45","text":"<p>This new version includes a refactor of the <code>get_git_diff</code> function in the <code>code_manipulation</code> module. The default value for the <code>repo_path</code> parameter has been changed to improve the function's usability.</p>"},{"location":"releases/v0.0.45/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.0.45/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes were made in this version.</li> </ul>"},{"location":"releases/v0.0.45/#refactors","title":"Refactors","text":"<ul> <li>The default value of the <code>repo_path</code> parameter in the <code>get_git_diff</code> function has been changed from <code>here()</code> to <code>None</code>. Additionally, a conditional check has been added to set <code>repo_path</code> to <code>here()</code> if it is <code>None</code>. This change makes the function more flexible and easier to use. (96e69b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.45/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were made in this version.</li> </ul>"},{"location":"releases/v0.0.46/","title":"V0.0.46","text":""},{"location":"releases/v0.0.46/#0046","title":"0.0.46","text":"<p>This new version includes a significant refactor of the tutorialbot, improving its flexibility and maintainability.</p>"},{"location":"releases/v0.0.46/#new-features","title":"New Features","text":"<ul> <li>The tutorialbot has been refactored from a SimpleBot instance to a function that returns a SimpleBot instance. This change enhances the flexibility of the bot, allowing for more diverse use cases. (d85426) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.46/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.46/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.47/","title":"V0.0.47","text":""},{"location":"releases/v0.0.47/#0047","title":"0.0.47","text":"<p>This new version includes an additional dependency to enhance the functionality of the software.</p>"},{"location":"releases/v0.0.47/#new-features","title":"New Features","text":"<ul> <li>Added pyzotero as a new dependency to the project (d2214e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.47/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.47/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.48/","title":"V0.0.48","text":""},{"location":"releases/v0.0.48/#version-0048","title":"Version 0.0.48","text":"<p>This new version introduces improvements to the progress reporting in the chat_paper function of the Zotero feature.</p>"},{"location":"releases/v0.0.48/#new-features","title":"New Features","text":"<ul> <li>Improved progress reporting in the chat_paper function of the Zotero feature. The changes include moving the retrieverbot response and paper_key retrieval outside of the progress context, adding progress tasks for embedding Zotero library, downloading paper, and initializing docbot, and wrapping relevant sections of code with progress context (da0fc0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.48/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.48/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.49/","title":"V0.0.49","text":""},{"location":"releases/v0.0.49/#0049","title":"0.0.49","text":"<p>This new version introduces a tutorial for the Zotero CLI feature of Llamabot and refactors the tutorial generation process for improved code readability and maintainability.</p>"},{"location":"releases/v0.0.49/#new-features","title":"New Features","text":"<ul> <li>A comprehensive tutorial for using the Llamabot Zotero CLI has been added. This tutorial includes sections on prerequisites, configuration, syncing Zotero items, and chatting with a paper, with examples and explanations provided for each step. (711011) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.49/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.49/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.49/#refactors","title":"Refactors","text":"<ul> <li>The tutorial generation process has been updated. Now, the tutorialbot is instantiated before calling the module_tutorial_writer, which improves code readability and maintainability. (99f487) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.5/","title":"V0.0.5","text":""},{"location":"releases/v0.0.5/#005","title":"0.0.5","text":"<p>This new version introduces the QueryBot prototype and its corresponding tests. It also includes improvements in documentation and example notebooks. The version also includes some housekeeping changes like ignoring certain files and directories.</p>"},{"location":"releases/v0.0.5/#new-features","title":"New Features","text":"<ul> <li>QueryBot prototype added to the project. This is a new feature that allows users to interact with the bot using queries. (c190e03) (Eric Ma)</li> <li>Tests for QueryBot have been added to ensure its proper functioning. (78a791d) (Eric Ma)</li> <li>A new example on how to build a simple panel app has been added. This will help users understand how to create their own apps. (7e928b7) (Eric Ma)</li> <li>A notebook chatbot example has been added to provide a practical example of how to use the chatbot in a notebook environment. (7e96304) (Eric Ma)</li> <li>A simplebot notebook has been added to the project. This notebook provides a simple example of a bot. (0121db5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The chat notebook example is now properly executed. This fix ensures that the example runs as expected. (60803dd) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.5/#deprecations","title":"Deprecations","text":"<ul> <li>Notebook execution has been disabled. This change is made to prevent automatic execution of notebooks. (89c39c1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.5/#other-changes","title":"Other Changes","text":"<ul> <li>The project version has been bumped from 0.0.4 to 0.0.5. (e94a28) (Eric Ma)</li> <li>Docstrings have been added to the project for better code understanding and readability. (2eb8c62) (Eric Ma)</li> <li>The directory 'data/' is now ignored by Git. This prevents unnecessary tracking of changes in this directory. (4252cd4) (Eric Ma)</li> <li>The 'mknotebooks' has been moved to the pip section. (e5f0e9d) (Eric Ma)</li> <li>Temporary markdown files created by 'mknotebooks' are now ignored by Git. This prevents unnecessary tracking of these temporary files. (1e4821d) (Eric Ma)</li> <li>The README file has been updated twice to provide the latest information about the project. (b3e02e2, 32f32db) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.50/","title":"V0.0.50","text":""},{"location":"releases/v0.0.50/#0050","title":"0.0.50","text":"<p>This new version introduces enhanced functionality to the chat_paper function and the get_key prompt in zotero.py, adds a streaming option to the QueryBot class in querybot.py, and removes a debugging print statement in doc_processor.py.</p>"},{"location":"releases/v0.0.50/#new-features","title":"New Features","text":"<ul> <li>The chat_paper function in zotero.py now supports multiple paper keys, provides a list of paper titles for the user to choose from, and displays a summary of the selected paper (1c47a8) (Eric Ma)</li> <li>The get_key prompt in zotero.py has been updated to return a list of keys instead of a single key, improving the user experience (1c47a8) (Eric Ma)</li> <li>A new 'stream' parameter has been added to the QueryBot class in querybot.py, allowing users to choose whether to stream the chatbot or not. By default, 'stream' is set to True (01ada0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.50/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>A print statement used for debugging purposes has been removed from the doc_processor.py file (796ac2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.50/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.51/","title":"V0.0.51","text":""},{"location":"releases/v0.0.51/#0051","title":"0.0.51","text":"<p>This new version introduces several enhancements to the Zotero integration in the llamabot project, improving performance, user interaction, and error handling. It also includes important bug fixes and documentation updates.</p>"},{"location":"releases/v0.0.51/#new-features","title":"New Features","text":"<ul> <li>Added a sync option to the ZoteroLibrary class, improving performance by reducing unnecessary queries to Zotero when the library can be loaded from a local file (a3ea1b) (Eric Ma)</li> <li>Integrated the standalone sync command from zotero.py into the chat command and refactored ZoteroLibrary and ZoteroItem classes to handle synchronization and downloading of Zotero items (a75308) (Eric Ma)</li> <li>Updated the guidelines for writing commit messages in the <code>git.py</code> file (a98ba93) (Eric Ma)</li> <li>Added support for accessing nested keys in the ZoteroItem class (216abc) (Eric Ma)</li> <li>Improved task progress visibility and command help in the Zotero integration (895079) (Eric Ma)</li> <li>Enhanced the chat function in zotero.py with an interactive prompt and an exit command (bf043b) (Eric Ma)</li> <li>Updated file handling in ZoteroItem class, including a fallback to write an abstract.txt file when no PDF is available (8b9fa4) (Eric Ma)</li> <li>Simplified progress task handling and improved output formatting in the Zotero integration (26dc67) (Eric Ma)</li> <li>Improved user interaction and error handling in Zotero integration, including persistent progress display, better progress tracking, real-time streaming, and continuous interaction (347a08) (Eric Ma)</li> <li>Ensured that the get_key function in zotero.py strictly returns JSON format (34b82d) (Eric Ma)</li> <li>Enhanced Zotero library and item classes, including faster lookup, better PDF handling, and improved functionality and usability (a813c5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.51/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected file writing in ZoteroItem class, ensuring that the abstractNote data is correctly written to the file (42e6a5) (Eric Ma)</li> <li>Fixed a typo in the file open method in the ZoteroItem class that was causing a runtime error (0a20e9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.51/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.52/","title":"V0.0.52","text":""},{"location":"releases/v0.0.52/#0052","title":"0.0.52","text":"<p>This new version includes an important bug fix and updates to the tutorial content for the Llamabot Zotero CLI.</p>"},{"location":"releases/v0.0.52/#new-features","title":"New Features","text":"<ul> <li>The tutorial content for the Llamabot Zotero CLI has been updated to provide a more accurate and user-friendly guide. Changes include rewording the introduction, updating the prerequisites section, removing the section on syncing Zotero items, and adding sections on various topics such as chatting with a paper, retrieving keys, downloading papers, and asking questions (fab7d3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.52/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The field declaration for 'zot' in ZoteroLibrary class has been changed to use default_factory instead of default. This ensures that the load_zotero function is called when a new instance of ZoteroLibrary is created, rather than at import time (c65618) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.52/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.53/","title":"V0.0.53","text":""},{"location":"releases/v0.0.53/#version-0053","title":"Version 0.0.53","text":"<p>This new version introduces significant improvements to the chat recording and saving mechanism of the Llamabot. It also includes a minor refactor in the Zotero module.</p>"},{"location":"releases/v0.0.53/#new-features","title":"New Features","text":"<ul> <li>Added chat recording and saving functionality. This feature includes the addition of <code>case-converter</code> to the project dependencies, the importation of <code>date</code> and <code>snakecase</code> from <code>datetime</code> and <code>caseconverter</code> respectively, the addition of <code>PromptRecorder</code> to record the chat, modification of the <code>chat</code> function to record and save the chat with a filename in snakecase format prefixed with the current date, and the addition of a <code>save</code> method in <code>PromptRecorder</code> to save the recorded chat to a specified path (22738e) (Eric Ma)</li> <li>Improved the chat recording and saving mechanism. The creation of the save path was moved to the beginning of the chat function, the save path now includes the date and the snakecased user choice, the save path is printed to the console when the user exits the chat, the save function now coerces the path argument to a pathlib.Path object for compatibility, and the save function is now called with the save path instead of a string for flexibility and ease of use (c44562) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.53/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.53/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the temperature parameter from the QueryBot instantiation in the chat function of the Zotero module. This simplifies the QueryBot configuration and does not affect the functionality of the bot (663594) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.54/","title":"V0.0.54","text":""},{"location":"releases/v0.0.54/#0054","title":"0.0.54","text":"<p>This new version includes code refactoring for improved efficiency and cleaner console output.</p>"},{"location":"releases/v0.0.54/#new-features","title":"New Features","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.54/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.54/#refactoring","title":"Refactoring","text":"<ul> <li>Removed unnecessary progress task related to summarizing the paper in <code>zotero.py</code> (62c35b) (Eric Ma)</li> <li>Removed print statement indicating the completion of recording in <code>recorder.py</code> for cleaner console output (62c35b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.54/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.55/","title":"V0.0.55","text":""},{"location":"releases/v0.0.55/#0055","title":"0.0.55","text":"<p>This new version introduces a chat command to LlamaBot CLI, adds a logging option to the ChatBot class, and updates the documentation with new usage examples and a CLI demos section.</p>"},{"location":"releases/v0.0.55/#new-features","title":"New Features","text":"<ul> <li>Added a chat command to LlamaBot CLI. This new command allows users to interact with the ChatBot and includes an option to save the chat to a markdown file. The filename for the saved chat is generated based on the current date and time. The chat command will exit if the user types \"exit\" or \"quit\". (baa4d64) (Eric Ma)</li> <li>Added a logging option to the ChatBot class. This new parameter is a boolean that determines whether to log the chat history and token budget. This feature provides more flexibility for users who want to monitor the chat history and token budget during the bot operation. (6550cf3) (Eric Ma)</li> <li>Updated the documentation's index file with new usage examples. These include a new example of exposing a chatbot directly at the command line using <code>llamabot chat</code>, an updated description and command for using <code>llamabot</code> as part of the backend of a CLI app to chat with Zotero library, and a new example of using <code>llamabot</code>'s <code>SimpleBot</code> to create a bot that automatically writes commit messages. (274a779) (Eric Ma)</li> <li>Introduced a new section in the documentation, specifically in the index.md file. The section is titled \"CLI Demos\" and provides examples of what can be built with Llamabot and some supporting code. It also includes an embedded asciicast for a more interactive demonstration. (ce7e734) (Eric Ma)</li> <li>Added an asciicast script to the documentation index file. This will provide users with a visual guide or tutorial. (e332f0a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.55/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.55/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.56/","title":"V0.0.56","text":""},{"location":"releases/v0.0.56/#0056","title":"0.0.56","text":"<p>This new version brings a number of improvements to the user interface, streamlines the handling of user prompts and Zotero library, and introduces new features such as a document chat bot functionality. It also includes several bug fixes and refactoring of the code for better performance and readability.</p>"},{"location":"releases/v0.0.56/#new-features","title":"New Features","text":"<ul> <li>Document chat bot functionality has been added. This feature allows users to chat with a document by providing a path to the document (005a10) (Eric Ma)</li> <li>The 'textual' package has been added to the dependencies, enhancing the functionality of the codebase (9b53aa) (Eric Ma)</li> <li>A new Jupyter notebook, patreon_ghostwriter.ipynb, has been introduced in the scratch_notebooks directory. The notebook includes code for a bot that can generate Patreon posts based on provided talking points (849497) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.56/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed a bug in <code>ZoteroLibrary</code> where items were not being loaded from JSONL file (7e9ea4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.56/#refactors","title":"Refactors","text":"<ul> <li>User prompts have been streamlined for consistency across modules, and Zotero library handling has been improved (7e9ea4) (Eric Ma)</li> <li>CLI prompts and exit handling have been streamlined (3c4cc3) (Eric Ma)</li> <li>Instructions for writing commit messages in git.py have been improved for clarity and user-friendliness (942005) (Eric Ma)</li> <li>A function has been renamed to <code>ensure_work_email_on_calendly_events</code> to make it more generic (841c78) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.56/#environment-and-dependencies","title":"Environment and Dependencies","text":"<ul> <li>Python version has been updated from 3.9 to 3.11, and pre-commit has been removed from dependencies (8f880f) (Eric Ma)</li> <li>Python version has been downgraded from 3.11 to 3.9 to ensure compatibility with existing libraries, and version constraint on bokeh has been removed to use the latest version (0e8bff) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.57/","title":"V0.0.57","text":""},{"location":"releases/v0.0.57/#0057","title":"0.0.57","text":"<p>This new version introduces several enhancements to the QueryBot class, adds a language inference function to the <code>embed_repo.ipynb</code> notebook, and provides a command line interface for interacting with a code repository. It also includes progress bars for file hashing and document splitting processes, an option to ignore directories when displaying the directory tree, and support for multiple documents for indexing. Lastly, a comprehensive tutorial on how to install, configure, and use LlamaBot is added.</p>"},{"location":"releases/v0.0.57/#new-features","title":"New Features","text":"<ul> <li>Added caching option and improved document handling in QueryBot. This includes changes to the <code>make_or_load_index</code> function, <code>exit_if_asked</code> function, <code>ZOTERO_JSON_DIR</code>, <code>ZoteroLibrary</code> class, and <code>magic_load_doc</code> function. Also, updates were made to the <code>zotero.ipynb</code> notebook to reflect these changes (579f162) (Eric Ma)</li> <li>Added language inference function and updated execution counts in <code>embed_repo.ipynb</code> notebook. This enhances the functionality of the notebook by allowing it to infer the programming languages used in a repository and providing a more detailed view of the repository's structure (b795e72) (Eric Ma)</li> <li>Added CLI for interacting with code repository. This is part of ongoing efforts to improve the usability of the LlamaBot project (042ae26) (Eric Ma)</li> <li>Added progress bars to file hashing and document splitting in the QueryBot module. This provides a visual indication of progress when processing large numbers of documents, improving user experience (4634185) (Eric Ma)</li> <li>Added directory ignore option to <code>show_directory_tree</code>. This allows specifying a list of directory names to ignore when displaying the directory tree (271ccde) (Eric Ma)</li> <li>Added support for multiple documents for indexing in QueryBot. This includes changes to the <code>doc_paths</code> parameter and the <code>make_or_load_index</code> function (c813522) (Eric Ma)</li> <li>Added LlamaBot tutorial documentation. This provides a comprehensive tutorial on how to install, configure, and use LlamaBot (9e25fb5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.57/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.57/#deprecations","title":"Deprecations","text":"<ul> <li>The change in how Zotero library data is stored and handled may break existing code that relies on the old JSONL format (579f162) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.58/","title":"V0.0.58","text":""},{"location":"releases/v0.0.58/#version-0058","title":"Version 0.0.58","text":"<p>This new version includes an important bug fix that improves the compatibility of the ZoteroLibrary with other components. The output format of the ZoteroLibrary has been changed from JSONL to JSON.</p>"},{"location":"releases/v0.0.58/#new-features","title":"New Features","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.58/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Changed the output format of the ZoteroLibrary from JSONL to JSON for better compatibility with other components (9b7757) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.58/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.59/","title":"V0.0.59","text":""},{"location":"releases/v0.0.59/#0059","title":"0.0.59","text":"<p>This new version includes a critical bug fix related to the creation of the ZOTERO_JSON_DIR directory.</p>"},{"location":"releases/v0.0.59/#new-features","title":"New Features","text":"<p>No new features were added in this version.</p>"},{"location":"releases/v0.0.59/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the method of directory creation for ZOTERO_JSON_DIR to ensure correct creation (a6a0c7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.59/#deprecations","title":"Deprecations","text":"<p>No deprecations were made in this version.</p>"},{"location":"releases/v0.0.6/","title":"V0.0.6","text":""},{"location":"releases/v0.0.6/#006","title":"0.0.6","text":"<p>This new version includes an example notebook for QueryBot and a version bump.</p>"},{"location":"releases/v0.0.6/#new-features","title":"New Features","text":"<ul> <li>Added an example notebook for QueryBot (fa7182) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.6/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.60/","title":"V0.0.60","text":""},{"location":"releases/v0.0.60/#version-0060","title":"Version 0.0.60","text":"<p>This new version introduces a significant refactor of the retriever initialization and cache handling in the Llamabot application. It also includes minor changes in the Zotero chat function and the zotero notebook.</p>"},{"location":"releases/v0.0.60/#new-features","title":"New Features","text":"<ul> <li>Refactored the retriever initialization and cache handling in the Llamabot application. This includes the removal of direct import and usage of VectorIndexRetriever in querybot.py, the addition of a method to get the retriever from the index, and the definition of CACHE_DIR as a constant in querybot.py and init.py. The get_persist_dir has been refactored to use the CACHE_DIR constant, and a clear_cache command has been added in init.py to clear the Llamabot cache. The default value of the sync option in the zotero.py chat function has been changed, and the doc_paths argument in the retrieverbot initialization in zotero.py has been updated. Directory creation in zotero.ipynb has been commented out, and code has been added to list json files in the ZOTERO_JSON_DIR in zotero.ipynb. (49645b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.60/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.60/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.61/","title":"V0.0.61","text":""},{"location":"releases/v0.0.61/#version-0061","title":"Version 0.0.61","text":"<p>This new version introduces caching to improve performance and enhances the paper selection process for a more interactive experience.</p>"},{"location":"releases/v0.0.61/#new-features","title":"New Features","text":"<ul> <li>Enabled use of cache in the chat function to improve performance (013dae) (Eric Ma)</li> <li>Enhanced the paper selection process to handle single paper scenario and provide a more interactive selection process for multiple papers (013dae) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.61/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.61/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.62/","title":"V0.0.62","text":""},{"location":"releases/v0.0.62/#version-0062","title":"Version 0.0.62","text":"<p>This new version includes a significant refactor of the querybot's faux chat history construction for improved clarity and functionality.</p>"},{"location":"releases/v0.0.62/#new-features","title":"New Features","text":"<ul> <li>The faux chat history construction in querybot has been updated for better clarity and functionality. The VectorIndexRetriever has been replaced with the index.as_retriever method, a system message has been added to the faux chat history, the last four responses from the chat history are now included in the faux chat history, and the order of faux chat history construction has been adjusted for better clarity (47a35d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.62/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.62/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.63/","title":"V0.0.63","text":""},{"location":"releases/v0.0.63/#version-0063","title":"Version 0.0.63","text":"<p>This new version introduces a blog assistant functionality to llamabot and specifies a minimum Python version in the pyproject.toml file.</p>"},{"location":"releases/v0.0.63/#new-features","title":"New Features","text":"<ul> <li>Blog assistant functionality has been added to llamabot. This new feature can summarize and tag a blog post. It includes the addition of a new 'blog' module, a new 'blog' command to the CLI, and the creation of several new files in the CLI and prompt_library directories. This enhancement provides users with a tool to automatically summarize and tag their blog posts. (265962) (Eric Ma)</li> <li>The pyproject.toml file now requires a minimum Python version of 3.10. This change ensures compatibility with the latest features and security updates. (a664df) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.63/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.63/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.64/","title":"V0.0.64","text":""},{"location":"releases/v0.0.64/#0064","title":"0.0.64","text":"<p>This new version focuses on improving the configuration process of LlamaBot. It introduces a new feature that fetches the default language model from the configuration file. The LlamaBot tutorial has been updated to provide detailed instructions on how to set up the OpenAI API key and select the default model. Additionally, the configuration command has been moved to a separate module for better code organization.</p>"},{"location":"releases/v0.0.64/#new-features","title":"New Features","text":"<ul> <li>The LlamaBot tutorial now focuses on the configuration process, providing detailed instructions on how to set up the OpenAI API key and select the default model. The sections on installation, version checking, and chatting with LlamaBot have been removed. (87dfef) (Eric Ma)</li> <li>Introduced a new feature where the default language model is now fetched from the configuration file. This change affects the ChatBot, QueryBot, and SimpleBot classes where the model_name parameter in their constructors now defaults to the value returned by the default_language_model function from the config module. (d531cb) (Eric Ma)</li> <li>The configuration command has been moved from the main init.py file to a new configure.py module. This change improves the organization of the code and makes it easier to maintain. A new command for setting the default model has been added to the configure module. (2bffdaf) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.64/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.64/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.65/","title":"V0.0.65","text":""},{"location":"releases/v0.0.65/#0065","title":"0.0.65","text":"<p>This new version introduces several enhancements to the blog assistant CLI and blogging prompts, adds token budgeting for different models in the chatbot, and updates blogging and Patreon notebooks. A new notebook for semantic line breaks has also been added.</p>"},{"location":"releases/v0.0.65/#new-features","title":"New Features","text":"<ul> <li>Blogging and Patreon notebooks have been updated with new code cells and existing ones have been improved. A new notebook, sembr.ipynb, has been added with code for semantic line breaks. These changes improve the functionality and expand the capabilities of the notebooks (a34a02) (Eric Ma)</li> <li>Token budgeting for different models has been added to the chatbot. This feature allows for more flexible token budgeting depending on the model used (cc7ab8) (Eric Ma)</li> <li>Several enhancements have been made to the blog assistant CLI and blogging prompts. The <code>summarize_and_tag</code> function has been renamed to <code>summarize</code> and now also returns the blog title. A new <code>social_media</code> function has been added to generate social media posts for LinkedIn, Patreon, and Twitter. The <code>blog_tagger_and_summarizer</code> prompt has been renamed to <code>blog_title_tags_summary</code> and now also returns the blog title. New prompts <code>compose_linkedin_post</code>, <code>compose_patreon_post</code>, and <code>compose_twitter_post</code> have been added to generate social media posts. A new <code>BlogInformation</code> model has been added to represent blog information (453e5d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.65/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.65/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.66/","title":"V0.0.66","text":""},{"location":"releases/v0.0.66/#version-0066","title":"Version 0.0.66","text":"<p>This new version introduces the Semantic Line Breaks (SEMBR) functionality to the blog summary and a new command. It enhances the readability and maintainability of the blog posts by applying a consistent line break strategy.</p>"},{"location":"releases/v0.0.66/#new-features","title":"New Features","text":"<ul> <li>Added SEMBR functionality to blog summary in the <code>summarize</code> function (faa08e) (Eric Ma)</li> <li>Introduced a new command <code>sembr</code> that allows users to apply SEMBR to their blog posts (faa08e) (Eric Ma)</li> <li>Implemented SEMBR functionality using a new <code>sembr_bot</code> in the <code>prompt_library</code> (faa08e) (Eric Ma)</li> <li>Created a new file <code>prompt_library/sembr.py</code> to handle the SEMBR prompts and bot creation (faa08e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.66/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.66/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.0.67/","title":"V0.0.67","text":""},{"location":"releases/v0.0.67/#0067","title":"0.0.67","text":"<p>This new version introduces enhancements to the social media post generation, updates to the testing matrix for Python versions, and a new GitHub workflow for daily testing of PyPI packages.</p>"},{"location":"releases/v0.0.67/#new-features","title":"New Features","text":"<ul> <li>Enhanced social media post generation. The update refactors the social media content generation to handle different platforms more effectively, adds JSON schema to standardize the return format, improves the handling of Patreon posts, and copies the post text to the clipboard for platforms other than Patreon. (07f90e) (Eric Ma)</li> <li>Introduced a new GitHub workflow for daily testing of PyPI packages. The workflow runs on the main branch and uses a matrix strategy to test on Python versions 3.9, 3.10, and 3.11. (fce17c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.67/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the python versions used in the test-pypi-package workflow. The versions have been updated from 3.10 to 3.10.12 and from 3.11 to 3.11.4. This ensures that the package is tested against the latest patch versions of Python. (e9ec8d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.67/#deprecations","title":"Deprecations","text":"<ul> <li>Removed Python version 3.12 from the testing matrix in the GitHub Actions workflow for testing the PyPI package. This change is made to focus on the more stable and widely used versions of Python. (b90b8c) (Eric Ma)</li> <li>Updated the python versions used in the testing matrix of the test-pypi-package workflow. The version 3.9 has been removed and version 3.12 has been added. This ensures our package remains compatible with the latest python versions. (70e4dc) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.68/","title":"V0.0.68","text":""},{"location":"releases/v0.0.68/#0068","title":"0.0.68","text":"<p>This new version introduces several enhancements to the LLaMaBot project, including the addition of a 'prompts' section to the pyproject.toml file, improved error handling for missing packages, a new Jupyter notebook for LLaMaBot demo, and updates to the Google Calendar integration. The version also includes several code refactoring and documentation updates for better readability and maintainability.</p>"},{"location":"releases/v0.0.68/#new-features","title":"New Features","text":"<ul> <li>Added a 'prompts' section to the pyproject.toml file (82d9e8) (Eric Ma)</li> <li>Introduced error handling for the import of the <code>outlines</code> package in various modules of the llamabot prompt library (a569ca) (Eric Ma)</li> <li>Added a new Jupyter notebook demonstrating the usage of LLaMaBot (fdd17f) (Eric Ma)</li> <li>Updated Google Calendar integration with new features and improvements (170271) (Eric Ma)</li> <li>Added a tutorial for the Blog Assistant CLI in the documentation (620da7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.68/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Pinned the version of mkdocs to 1.4.3 in the environment.yml file to ensure consistent documentation builds across different environments (ee7e7e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.68/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the outlines package from the project dependencies in pyproject.toml file (25bccb) (Eric Ma)</li> <li>Removed all the files related to Google API, which are superseded by the gcsa package (eb0c50) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.68/#other-improvements","title":"Other Improvements","text":"<ul> <li>Improved type annotations and code organization in the llamabot module (a1b391) (Eric Ma)</li> <li>Updated cron schedule in test-pypi-package workflow for better server load distribution (efd390) (Eric Ma)</li> <li>Added explanation for stateless function in the documentation (9a6b4e) (Eric Ma)</li> <li>Improved readability of the documentation by applying semantic line breaks and changing code block to text block (604277, 3583ba) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.69/","title":"V0.0.69","text":""},{"location":"releases/v0.0.69/#version-0069","title":"Version 0.0.69","text":"<p>This new version introduces extended installation options for the llamabot package and adds two new Jupyter notebooks to the project. The installation now includes all optional dependencies, ensuring full feature availability during testing. The new notebooks provide code for language model configuration and OpenAI API setup.</p>"},{"location":"releases/v0.0.69/#new-features","title":"New Features","text":"<ul> <li>Extended llamabot installation to include all optional dependencies, improving the thoroughness of the testing process and ensuring all package features are working as expected (e6e1e3) (Eric Ma)</li> <li>Added two new Jupyter notebooks: multiscale_embeddings.ipynb and outlines_backend_prototype.ipynb. The first notebook provides code for loading and configuring language models, creating and loading indices, retrieving and scoring nodes, and building queries. The second notebook provides code for setting up the OpenAI API and generating completions (044439) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.69/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.69/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.7/","title":"V0.0.7","text":""},{"location":"releases/v0.0.7/#version-007","title":"Version 0.0.7","text":"<p>This new version includes several enhancements and updates to improve the functionality and consistency of the LlamaBot.</p>"},{"location":"releases/v0.0.7/#new-features","title":"New Features","text":"<ul> <li>Added 'llama-index' to the list of dependencies to enhance the functionality of the bot (196cdc) (Eric Ma)</li> <li>Updated the <code>__call__</code> method of QueryBot for better performance and efficiency (b1840c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Replaced a complex version with a simplified one to fix performance issues (f17655) (Eric Ma)</li> <li>Ensured the return of strings is consistent across all functions to fix inconsistency issues (88d62a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.7/#deprecations","title":"Deprecations","text":"<ul> <li>Changed the default argument of <code>return_sources</code> to True. This might affect the behavior of functions that rely on the previous default value (a03db6) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.70/","title":"V0.0.70","text":""},{"location":"releases/v0.0.70/#version-0070","title":"Version 0.0.70","text":"<p>This new version introduces a more streamlined and reliable process for releasing Python packages, with several enhancements to the GitHub Actions workflows. It also includes a new feature for similarity search in the QueryBot class and some minor bug fixes.</p>"},{"location":"releases/v0.0.70/#new-features","title":"New Features","text":"<ul> <li>Added a project description and linked the README.md file to the project configuration (92002ba) (Eric Ma)</li> <li>Updated the pypi-publish action used in the GitHub Actions workflow for releasing the Python package to ensure stability and reliability of the release process (b8ecf9f) (Eric Ma)</li> <li>Separated the installation of the 'build' and 'wheel' packages in the GitHub Actions workflow for releasing a Python package to make the installation steps more explicit and easier to understand (005280e) (Eric Ma)</li> <li>Added the 'build' package to the python setup step in the GitHub Actions workflow for releasing a python package (62af643) (Eric Ma)</li> <li>Simplified the python package build process in the GitHub workflow to use the build module instead of setup.py (321e282) (Eric Ma)</li> <li>Set the default release type to 'patch' in the release-python-package workflow to prevent accidental major or minor releases (b339f88) (Eric Ma)</li> <li>Added a new step in the GitHub Actions workflow for releasing the Python package that configures the Git user name and email (f8f6ab4) (Eric Ma)</li> <li>Changed the GitHub workflow from running tests on different Python versions to publishing the Python package to PyPI (628b91f) (Eric Ma)</li> <li>Introduced a new GitHub workflow for releasing Python packages that includes steps for running tests, bumping version numbers, building and publishing the package, and creating a release in the GitHub repository (2f28ab7) (Eric Ma)</li> <li>Added a new method 'retrieve' in the QueryBot class for retrieving source nodes associated with a query using similarity search (a08d0f0) (Eric Ma)</li> <li>Added the ability to manually trigger the test-pypi-package workflow from the GitHub Actions UI (7611052) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.70/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Disabled the deadline for the ghostwriter test in the Python prompt library to prevent Hypothesis from failing the test due to it taking too long to run (b960ced) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.70/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.71/","title":"V0.0.71","text":""},{"location":"releases/v0.0.71/#version-0071","title":"Version 0.0.71","text":"<p>This new version includes several updates to the GitHub Actions workflow for releasing the Python package. The git configuration has been updated for better readability and specific use by the GitHub Actions user. The secret used for the user password in the release workflow has been changed for correct deployment. The git configuration now includes the credential helper and GitHub token for authentication when pushing changes. The versions of actions/checkout and actions/setup-python have been upgraded for better performance and security.</p>"},{"location":"releases/v0.0.71/#new-features","title":"New Features","text":"<ul> <li>Added credential helper and GitHub token to git configuration for authentication when pushing changes (5ed538) (Eric Ma)</li> <li>Upgraded actions/checkout from v2 to v3 and actions/setup-python from v2 to v3 for better performance and security (8af512) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.71/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Changed the secret used for the user password in the GitHub Actions release workflow for correct deployment (f96f6d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.71/#chores","title":"Chores","text":"<ul> <li>Updated git configuration and push command in the GitHub Actions workflow for better readability (250e87) (Eric Ma)</li> </ul> <p>Please note that the publishing of the package was temporarily commented out in this version (ec6cb5) (Eric Ma).</p>"},{"location":"releases/v0.0.72/","title":"V0.0.72","text":""},{"location":"releases/v0.0.72/#version-0072","title":"Version 0.0.72","text":"<p>This new version includes several enhancements to the Zotero module, improvements to the QueryBot, and updates to the pre-commit hooks. It also introduces a new Jupyter notebook for outlines models and enables package publishing to PyPI.</p>"},{"location":"releases/v0.0.72/#new-features","title":"New Features","text":"<ul> <li>Added code to retrieve the title of a specific article from the Zotero library using the article's unique identifier (5921df) (Eric Ma)</li> <li>Added support for default similarity top ks in QueryBot based on the OPENAI_DEFAULT_MODEL environment variable (ae392f) (Eric Ma)</li> <li>Enhanced the ZoteroLibrary class by adding an <code>articles_only</code> filter and a <code>key_title_map</code> function (85a223) (Eric Ma)</li> <li>Improved the get_key function documentation in the Zotero module (89b6bc) (Eric Ma)</li> <li>Streamlined the paper selection process in the Zotero CLI by introducing a new PaperTitleCompleter for more efficient paper selection (1122e6) (Eric Ma)</li> <li>Improved handling of similarity_top_k in QueryBot and refactored index creation (acc6e8) (Eric Ma)</li> <li>Added 'sh' dependency to environment.yml and pyproject.toml files (5e23f9) (Eric Ma)</li> <li>Added execution of pre-commit hooks before committing changes (82979d) (Eric Ma)</li> <li>Added a new class, PaperTitleCompleter, to provide completion suggestions for paper titles in the Zotero module (3fac26) (Eric Ma)</li> <li>Updated pre-commit config and notebooks (b077aa) (Eric Ma)</li> <li>Extended the ruff pre-commit hook to also check python and jupyter files (4ae772) (Eric Ma)</li> <li>Added nltk as a transitive dependency via llama_index in the environment.yml file (2bd392) (Eric Ma)</li> <li>Introduced a new pre-commit hook, ruff, to the .pre-commit-config.yaml file (c7c5bc) (Eric Ma)</li> <li>Enabled package publishing to PyPI (baca5c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.72/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed assertion in test_magic_load_doc_txt function (ef4b3e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.72/#refactors","title":"Refactors","text":"<ul> <li>Simplified the docstring in the doc_processor module and modified the document loading (fab218) (Eric Ma)</li> <li>Replaced 'index' with 'vector_index' in QueryBot class and refactored related methods (cfb284) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.72/#dependencies","title":"Dependencies","text":"<ul> <li>Bumped version: 0.0.71 \u2192 0.0.72 (d37eab) (github-actions)</li> <li>Added \"pre-commit\" to the list of dependencies in pyproject.toml (687645) (Eric Ma)</li> <li>Updated dependencies in environment.yml (7cb9a6) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.72/#other","title":"Other","text":"<ul> <li>Updated the version of the black pre-commit hook and removed the flake8 and isort pre-commit hooks (9fca51) (Eric Ma)</li> <li>Added a comment to clarify that GH Actions is allowed to write to the repository in the release-python-package workflow (bcf534) (Eric Ma)</li> <li>Introduced a new Jupyter notebook 'outlines_models.ipynb' in the 'scratch_notebooks' directory (746273) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.73/","title":"V0.0.73","text":""},{"location":"releases/v0.0.73/#version-0073","title":"Version 0.0.73","text":"<p>This new version includes an update to the commitbot feature, which now uses a more efficient model for generating commit messages.</p>"},{"location":"releases/v0.0.73/#new-features","title":"New Features","text":"<ul> <li>The commitbot has been updated to use the gpt-3.5-turbo-16k-0613 model. This model provides the same quality of commit messages as the previous model but at a fraction of the cost (ce91d6b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.73/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.73/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.74/","title":"V0.0.74","text":""},{"location":"releases/v0.0.74/#version-0074","title":"Version 0.0.74","text":"<p>This new version includes an update to the pip installation in the test workflow and the addition of a new dependency, beartype==0.15.0.</p>"},{"location":"releases/v0.0.74/#new-features","title":"New Features","text":"<ul> <li>Added beartype==0.15.0 to the list of dependencies in pyproject.toml (8c4db1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.74/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated pip installation in the test-pypi-package.yaml workflow to use the <code>python -m pip install</code> command instead of <code>pipx</code> to ensure the correct version of pip is used for installing the <code>llamabot[all]</code> package (2e860a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.74/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.75/","title":"V0.0.75","text":""},{"location":"releases/v0.0.75/#version-0075","title":"Version 0.0.75","text":"<p>This new version includes several enhancements to the CLI module of LlamaBot. The improvements focus on automating the process of writing commit messages and ensuring consistency. The version also includes codebase improvements such as the removal of unnecessary comments.</p>"},{"location":"releases/v0.0.75/#new-features","title":"New Features","text":"<ul> <li>A new command <code>autowrite_commit_message</code> has been added to the <code>git.py</code> file in the <code>llamabot/cli</code> directory. This command automatically generates a commit message based on the diff and writes it to the <code>.git/COMMIT_EDITMSG</code> file. Error handling has also been included in case any exceptions occur during the process. (185613) (Eric Ma)</li> <li>A new command <code>install_commit_message_hook</code> has been added to the Git subcommand for LlamaBot CLI. This command installs a commit message hook that runs the commit message through the bot, automating the process of writing commit messages and ensuring consistency. (d1254e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.75/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.75/#deprecations","title":"Deprecations","text":"<ul> <li>Unnecessary comments in <code>git.py</code> have been removed to improve the codebase. (ecf9c0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.76/","title":"V0.0.76","text":""},{"location":"releases/v0.0.76/#version-0076","title":"Version 0.0.76","text":"<p>This new version includes several enhancements to the CLI module and the Llamabot model. It also includes a bug fix for the autowrite_commit_message function.</p>"},{"location":"releases/v0.0.76/#new-features","title":"New Features","text":"<ul> <li>Help messages for subcommands have been added to the CLI module. This will provide users with more information on how to use each command. (f4de87) (Eric Ma)</li> <li>The model_chat_token_budgets in Llamabot have been updated. New models have been added to the dictionary and token budgets for existing models have been updated. (52522b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.76/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The autowrite_commit_message function in the CLI module has been fixed. Print statements have been replaced with echo for consistent output and error messages are now written to stderr instead of stdout. (a66ead) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.76/#deprecations","title":"Deprecations","text":"<ul> <li>The unused 'apps' subcommand has been removed from the CLI module. This subcommand was not being used and has been safely removed. (0ea7b3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.77/","title":"V0.0.77","text":""},{"location":"releases/v0.0.77/#version-0077","title":"Version 0.0.77","text":"<p>This new version introduces several enhancements to the release workflow, including the addition of release notes generation and the configuration of the OPENAI_API_KEY. It also includes improvements to the llamabot CLI and the documentation.</p>"},{"location":"releases/v0.0.77/#new-features","title":"New Features","text":"<ul> <li>Added fetch-depth parameter to the checkout action in the release-python-package workflow. This allows the action to fetch the entire history of the repository. (c25fe84) (Eric Ma)</li> <li>Upgraded the GitHub Actions checkout step to use version 4 and enabled the fetch-tags option. This ensures that all tags are fetched during the checkout process. (dadcf60) (Eric Ma)</li> <li>Added a new step in the release-python-package workflow to configure the OPENAI_API_KEY using llamabot. This is necessary for the successful generation of release notes. (6c17c10) (Eric Ma)</li> <li>Added OPENAI_API_KEY to environment variables in configure.py. This allows the application to access the OpenAI API key from the environment variables, improving security. (8df3cda) (Eric Ma)</li> <li>Updated the GitHub Actions workflow for releasing a new version of the Python package to include the release notes in the body of the GitHub release. (07150dc) (Eric Ma)</li> <li>Introduced a bot for converting git remote URL to HTTPS URL. This enhances the functionality of the release notes notebook. (85009ad) (Eric Ma)</li> <li>Added release notes generation to the GitHub workflow for releasing the Python package. (3d28e12) (Eric Ma)</li> <li>Introduced a new feature to the llamabot CLI, a command for generating release notes. This automates the process of generating release notes. (df181dd) (Eric Ma)</li> <li>Allowed setting default model by name in the <code>configure.py</code> file of the llamabot CLI. This provides more flexibility in setting the default model. (d223c43) (Eric Ma)</li> <li>Added a new Jupyter notebook 'release-notes.ipynb' in the 'scratch_notebooks' directory. The notebook contains code for generating release notes from git commit logs. (9ab58a5) (Eric Ma)</li> <li>Added the ability to specify the model name via an environment variable. This allows for more flexibility when deploying the bot in different environments. (127b6c9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.77/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.77/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.78/","title":"V0.0.78","text":""},{"location":"releases/v0.0.78/#version-0078","title":"Version 0.0.78","text":"<p>This new version includes several improvements to the release workflow and bug fixes. The release notes handling has been updated and simplified, and several bugs in the GitHub Actions workflow have been fixed.</p>"},{"location":"releases/v0.0.78/#new-features","title":"New Features","text":"<ul> <li>Release notes handling in the GitHub workflow has been updated. The workflow now copies the release notes to a temporary location before creating a release in the GitHub repository. This ensures that the release notes are correctly included in the release (d9ab5b) (Eric Ma)</li> <li>The source of the release notes in the GitHub Actions workflow for releasing a Python package has been changed. Instead of using an environment variable, it now reads from a markdown file in the docs/releases directory. The filename is based on the version number (3958ff) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.78/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>A bug in the GitHub Actions workflow for releasing a Python package has been fixed. The copy command used to copy the release notes was incorrect and has been fixed (7cda28) (Eric Ma)</li> <li>The file path for the release notes in the release-python-package GitHub workflow has been corrected. The version number now correctly includes a 'v' prefix when reading the markdown file (e03626) (Eric Ma)</li> <li>The path for the release notes in the GitHub Actions workflow has been corrected. The previous path was causing issues in the workflow execution. The path has been updated to correctly point to the release notes file (75978b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.78/#deprecations","title":"Deprecations","text":"<ul> <li>The step of copying release notes to a temporary location has been removed and the original file is directly referenced in the release action. This simplifies the workflow and reduces unnecessary operations (eb2aef) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.79/","title":"V0.0.79","text":""},{"location":"releases/v0.0.79/#version-0079","title":"Version 0.0.79","text":"<p>This version introduces a new prompt decorator and tests, improves the release workflow, fixes bugs in the GitHub Actions workflow, and removes the dependency on the 'outlines' package.</p>"},{"location":"releases/v0.0.79/#new-features","title":"New Features","text":"<ul> <li>A new prompt decorator has been added in the scratch_notebooks directory, enhancing the functionality of functions by adding a prompt feature. Tests have been included to ensure the decorator works as expected with different types of function arguments (d023f22) (Eric Ma).</li> <li>Tests for blogging prompts in the prompt_library directory have been added. These tests validate the output of different blogging prompt functions (d023f22) (Eric Ma).</li> <li>The release notes handling in the GitHub workflow has been updated. The workflow now copies the release notes to a temporary location before creating a release in the GitHub repository (3884962) (Eric Ma).</li> <li>The source of the release notes in the GitHub Actions workflow for releasing a Python package has been changed. It now reads from a markdown file in the docs/releases directory (3884962) (Eric Ma).</li> <li>The file path for the release notes in the release-python-package GitHub workflow has been corrected. The version number now correctly includes a 'v' prefix when reading the markdown file (3884962) (Eric Ma).</li> <li>The path for the release notes in the GitHub Actions workflow has been corrected. The previous path was causing issues in the workflow execution. The path has been updated to correctly point to the release notes file (3884962) (Eric Ma).</li> </ul>"},{"location":"releases/v0.0.79/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The file path for the release notes in the release-python-package GitHub workflow was incorrect and has been fixed (3884962) (Eric Ma).</li> </ul>"},{"location":"releases/v0.0.79/#deprecations","title":"Deprecations","text":"<ul> <li>The step of copying release notes to a temporary location has been removed and the original file is directly referenced in the release action. This simplifies the workflow and reduces unnecessary operations (3884962) (Eric Ma).</li> <li>The 'outlines' package was removed from the dependencies in the environment.yml and pyproject.toml files (af23aae) (Eric Ma).</li> </ul>"},{"location":"releases/v0.0.79/#refactors","title":"Refactors","text":"<ul> <li>The use of the <code>outlines</code> package has been replaced with a custom <code>prompt_manager</code> module across multiple files in the <code>llamabot</code> project. The <code>prompt_manager</code> provides a <code>prompt</code> decorator that turns Python functions into Jinja2-templated prompts, similar to the functionality provided by <code>outlines</code>. This refactor removes the dependency on the <code>outlines</code> package, simplifying the project's dependencies and potentially improving maintainability (dbe78e4) (Eric Ma).</li> </ul>"},{"location":"releases/v0.0.8/","title":"V0.0.8","text":""},{"location":"releases/v0.0.8/#version-008","title":"Version 0.0.8","text":"<p>This new version 0.0.8 of Llamabot introduces enhanced functionality and improved documentation.</p>"},{"location":"releases/v0.0.8/#new-features","title":"New Features","text":"<ul> <li>The application now exposes more arguments, providing users with greater flexibility and control. (f42815) (Eric Ma)</li> <li>Additional docstrings have been added, improving the documentation and making the code easier to understand and use. (f42815) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.8/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.80/","title":"V0.0.80","text":""},{"location":"releases/v0.0.80/#version-0080","title":"Version 0.0.80","text":"<p>This version includes several improvements to the ChatBot, QueryBot, and SimpleBot classes, including new parameters for additional configuration options and improved code readability. It also simplifies the pip install command used in the release-python-package GitHub workflow and removes unnecessary clutter from the codebase.</p>"},{"location":"releases/v0.0.80/#new-features","title":"New Features","text":"<ul> <li>Added <code>streaming</code> and <code>verbose</code> parameters to the <code>ChatBot</code> class initialization method, providing more flexibility in controlling the chat history streaming and verbosity during the bot initialization (a69c0f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.80/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Simplified the pip install command used in the release-python-package GitHub workflow. The previous command attempted to install all optional dependencies, which is not necessary for writing release notes. The new command only installs the package itself (2dffac) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.80/#refactors","title":"Refactors","text":"<ul> <li>Updated parameter names and descriptions in ChatBot, QueryBot, and SimpleBot for consistency and clarity. Added 'streaming' and 'verbose' parameters to SimpleBot for additional configuration options. Improved code readability by breaking up long lines and comments (6c0b37) (Eric Ma)</li> <li>Removed a large block of commented out code from the prompt_manager.py file, improving readability and reducing clutter in the codebase (7f4b0a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.80/#other","title":"Other","text":"<ul> <li>Bumped version from 0.0.79 to 0.0.80 (385221) (github-actions)</li> </ul>"},{"location":"releases/v0.0.81/","title":"V0.0.81","text":""},{"location":"releases/v0.0.81/#version-0081","title":"Version 0.0.81","text":"<p>This new version introduces a significant enhancement to the QueryBot class, providing more control over the printing of debug messages.</p>"},{"location":"releases/v0.0.81/#new-features","title":"New Features","text":"<ul> <li>Added a 'verbose' parameter to the QueryBot class to control the printing of debug messages (eee3e9) (Eric Ma)</li> <li>Updated the initialization of the LangChain model to use the new 'verbose' parameter instead of a hardcoded value (eee3e9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.81/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.81/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.82/","title":"V0.0.82","text":""},{"location":"releases/v0.0.82/#version-0082","title":"Version 0.0.82","text":"<p>This new version primarily focuses on improving code readability and maintainability. It also introduces a new feature to handle different numbers of tags in the git log when writing release notes.</p>"},{"location":"releases/v0.0.82/#new-features","title":"New Features","text":"<ul> <li>Added conditions to handle different numbers of tags in git log (645a36) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.82/#improvements","title":"Improvements","text":"<ul> <li>Reformatted code in multiple files for better readability (871316) (Eric Ma)</li> <li>Added newline at the end of the release notes file (871316) (Eric Ma)</li> <li>Improved handling of cases with no tags or only one tag in the git repository (871316) (Eric Ma)</li> <li>Removed unnecessary comments from <code>llamabot/panel_utils.py</code> and <code>tests/cli/test_cli_utils.py</code> (871316) (Eric Ma)</li> <li>Reformatted docstrings for better readability in multiple test files (871316) (Eric Ma)</li> <li>Updated docstrings for test functions to be more descriptive in <code>tests/test_file_finder.py</code> and <code>tests/test_recorder.py</code> (871316) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.82/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.82/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.83/","title":"V0.0.83","text":""},{"location":"releases/v0.0.83/#version-0083","title":"Version 0.0.83","text":"<p>This new version introduces more flexibility and control over the token budget and chunk sizes used in the chatbot. It also includes a new attribute to store the model name used by the bot and a bug fix to ensure multiple document paths are handled correctly.</p>"},{"location":"releases/v0.0.83/#new-features","title":"New Features","text":"<ul> <li>Added support for <code>response_tokens</code> and <code>history_tokens</code> parameters in the <code>QueryBot</code> class. These parameters allow the user to specify the number of tokens to use for the response and history in the chatbot. Also, a <code>chunk_sizes</code> parameter has been added to the <code>make_or_load_vector_index</code> function to specify a list of chunk sizes to use for the LlamaIndex TokenTextSplitter (a1de812) (Eric Ma)</li> <li>Introduced a new attribute 'model_name' to both QueryBot and SimpleBot classes. This attribute will be used to store the name of the model used by the bot (d5d684) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.83/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Modified the <code>doc_paths</code> parameter in the chat function of the llamabot/cli/doc.py file to receive a list of doc_paths, ensuring that the function can handle multiple document paths correctly (c763327) (Eric Ma)</li> <li>Changed the variable name in the chat function from <code>doc_path</code> to <code>doc_paths</code> for better clarity and consistency (11111e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.83/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.84/","title":"V0.0.84","text":""},{"location":"releases/v0.0.84/#version-0084","title":"Version 0.0.84","text":"<p>This new version introduces enhancements to the QueryBot class, adds a notebook for evaluating multiscale embeddings, and updates the funding configuration.</p>"},{"location":"releases/v0.0.84/#new-features","title":"New Features","text":"<ul> <li>A new notebook for evaluating multiscale embeddings has been added. This notebook, \"zotero_multiscale.ipynb\", provides an in-depth look at the effectiveness of multiscale embeddings compared to single-scale embeddings in LlamaBot's QueryBot class. It includes an explanation of multiscale embeddings, the motivation behind using them, and the implementation details. It also includes code to load a document from a Zotero library, create instances of QueryBot with different chunk sizes, and test their performance on different prompts. (24f9b6) (Eric Ma)</li> <li>The default chunk_sizes parameter in the QueryBot class has been updated to [2000]. This change ensures that the LlamaIndex TokenTextSplitter uses a chunk size of 2000 tokens by default. (f9d7f6) (Eric Ma)</li> <li>The GitHub funding platform in FUNDING.yml has been updated to use an array instead of a single string to support multiple contributors. (da221f) (Eric Ma)</li> <li>A new funding configuration file has been added to the project. This file includes supported funding model platforms such as GitHub and Patreon. (68c974) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.84/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.84/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.0.85/","title":"V0.0.85","text":""},{"location":"releases/v0.0.85/#version-0085","title":"Version 0.0.85","text":"<p>This version introduces several enhancements and refactors to the Llamabot project. The changes include improvements to the codebase's flexibility and maintainability, updates to the documentation, and the addition of new features.</p>"},{"location":"releases/v0.0.85/#new-features","title":"New Features","text":"<ul> <li>Added a new parameter <code>model_name</code> to the <code>chat</code> function in <code>zotero.py</code>, allowing users to specify the language model to use. (c03a13f) (Eric Ma)</li> <li>Introduced a new Jupyter notebook 'ollama.ipynb' demonstrating the implementation of a simple chatbot named 'ollama' using the 'llamabot' library. (c4919b2) (Eric Ma)</li> <li>Added a new <code>.vscode/extensions.json</code> file with a list of recommended extensions for Visual Studio Code. (964bafa) (Eric Ma)</li> <li>Added a new file <code>model_dispatcher.py</code> in the <code>llamabot/bot</code> directory, which contains a function <code>create_model</code> that dispatches and creates the right model based on the model name. (3dee9ea) (Eric Ma)</li> <li>Updated <code>simplebot.py</code> to use the <code>create_model</code> function from <code>model_dispatcher.py</code> instead of directly creating the model. (3dee9ea) (Eric Ma)</li> <li>Added a prompt to the <code>default_model</code> function in <code>configure.py</code> that informs the user to run <code>llamabot configure default-model</code> to set the default model. (b7a50e5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.85/#refactors","title":"Refactors","text":"<ul> <li>Replaced the hardcoded model name \"codellama\" with the default language model from the config file in <code>simplebot.py</code>. (bfb47a2) (Eric Ma)</li> <li>Moved model token constants to a new file <code>model_tokens.py</code> for better organization and maintainability. (f2a1f46) (Eric Ma)</li> <li>Refactored <code>QueryBot</code> class in <code>querybot.py</code> to use <code>create_model</code> function from <code>model_dispatcher.py</code> for model creation. (f2a1f46) (Eric Ma)</li> <li>Simplified model creation and token budget calculation in <code>chatbot.py</code>. (491ab6f) (Eric Ma)</li> <li>Removed an unnecessary echo message that was instructing the user to set the default model in the <code>default_model</code> function of <code>configure.py</code>. (d3c3751) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.85/#documentation","title":"Documentation","text":"<ul> <li>Added instructions on how to specify a model when using the <code>chat</code> command in <code>zotero.md</code>. (9b07f17) (Eric Ma)</li> <li>Introduced a new tutorial file <code>ollama.md</code> providing a comprehensive guide on how to run a chatbot using <code>llamabot</code> and <code>Ollama</code>. (9b07f17) (Eric Ma)</li> <li>Added a newline at the end of the release notes for versions v0.0.82, v0.0.83, and v0.0.84. (0001e76) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.85/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.85/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.0.86/","title":"V0.0.86","text":""},{"location":"releases/v0.0.86/#version-0086","title":"Version 0.0.86","text":"<p>This version includes several enhancements and updates to the codebase, including the addition of new tutorials, refactoring of the code, and updates to the Python version used in the GitHub Actions workflow.</p>"},{"location":"releases/v0.0.86/#new-features","title":"New Features","text":"<ul> <li>Added a tutorial for building a QueryBot chat interface with file upload functionality. This tutorial guides users on how to build a chat interface using the QueryBot and Panel libraries. (4b5799a) (Eric Ma)</li> <li>Introduced a new tutorial in the documentation that guides users on how to create a simple chat interface using the <code>SimpleBot</code> class from the <code>llamabot</code> library and the <code>Panel</code> library. (efaef316) (Eric Ma)</li> <li>Introduced a new Jupyter notebook 'panel-chat.ipynb' in the 'scratch_notebooks' directory. The notebook includes code for setting up a chat interface using the Panel library, and integrating it with a chatbot for interactive responses. (ba5d8009) (Eric Ma)</li> <li>Introduced a new Jupyter notebook 'zotero-panel.ipynb' in the 'scratch_notebooks' directory. The notebook contains code for creating a Zotero panel with interactive widgets for configuring Zotero API key, library ID, and library type. (8f477ec6) (Eric Ma)</li> <li>Introduced a new instance of SimpleBot named 'feynman' to the ollama notebook. The bot is tasked with explaining complex concepts, specifically in this case, the challenge of enzyme function annotation and the introduction of a machine learning algorithm named CLEAN. (7f844dca) (Eric Ma)</li> <li>Added \".html\": \"UnstructuredReader\" to EXTENSION_LOADER_MAPPING in doc_processor.py to enable processing of .html files. (45d6485c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.86/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the python version used in the GitHub workflow for code style checks to 3.11. (d10e7e18) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.86/#refactor","title":"Refactor","text":"<ul> <li>Removed unused imports from <code>querybot.py</code> and updated <code>make_or_load_vector_index</code> function to take <code>service_context</code> as a parameter instead of creating it within the function. (935e3dad) (Eric Ma)</li> <li>Removed the unused @validate_call decorator from the call method in querybot.py. (3f7e8c0b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.86/#documentation","title":"Documentation","text":"<ul> <li>Added instructions to the documentation on how to use local Ollama models with LlamaBot. It includes a Python code snippet demonstrating how to specify the <code>model_name</code> keyword argument when creating a <code>SimpleBot</code> instance. (57f12809) (Eric Ma)</li> <li>Updated the documentation for LlamaBot. It introduces two options for getting access to language models: using local models with Ollama or using the OpenAI API. (fc42049c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.86/#chore","title":"Chore","text":"<ul> <li>Updated the versions of pre-commit hooks for pre-commit-hooks, black, and ruff-pre-commit. It also replaces the darglint hook with pydoclint for better documentation linting. (9cc49022) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.87/","title":"V0.0.87","text":""},{"location":"releases/v0.0.87/#version-0087","title":"Version 0.0.87","text":"<p>This new version introduces several enhancements and features to improve the flexibility and maintainability of the code. The major highlight of this release is the dynamic scraping of Ollama model names, which allows the code to adapt to changes in the Ollama model library. Additionally, the codebase has been updated to Python 3.10, and new models have been added to the llama_model_keywords list.</p>"},{"location":"releases/v0.0.87/#new-features","title":"New Features","text":"<ul> <li>Dynamically scrape Ollama model names from the Ollama website. If the website cannot be reached, a static list of model names is used as a fallback. The function is cached using lru_cache to improve performance. (1f7e27) (Eric Ma)</li> <li>Added a function to automatically update the list of Ollama models. A new Python script has been added to the hooks in the pre-commit configuration file. This script scrapes the Ollama AI library webpage to get the latest model names and writes them to a text file. (f22007) (Eric Ma)</li> <li>Added the content.code.copy feature to the theme configuration in mkdocs.yaml. This feature allows users to easily copy code snippets from the documentation. (594d16) (Eric Ma)</li> <li>Added beautifulsoup4, lxml, and requests to the environment.yml file. These packages are necessary for the automatic scraping of ollama models. (2737a9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.87/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The method ollama_model_keywords() in model_dispatcher.py has been refactored. The dynamic scraping of model names from the Ollama website has been removed. Instead, the model names are now read from a static text file distributed with the package. This change simplifies the code and removes the dependency on the BeautifulSoup and requests libraries. (73d25) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.87/#deprecations","title":"Deprecations","text":"<ul> <li>The 'Commit release notes' step has been separated from the 'Write release notes' step in the release-python-package workflow. The 'pre-commit' package installation has been moved to the 'Commit release notes' step. (4613a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.87/#other-changes","title":"Other Changes","text":"<ul> <li>The target Python version in the Black configuration has been updated from Python 3.9 to Python 3.10. (cfadb3) (Eric Ma)</li> <li>Some of the existing models have been reordered and new ones have been added to the llama_model_keywords list in the model_dispatcher module. (22ade) (Eric Ma)</li> <li>A newline has been added at the end of the v0.0.86 release notes file. This change is in line with the standard file formatting conventions. (c22810) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.88/","title":"V0.0.88","text":""},{"location":"releases/v0.0.88/#version-0088","title":"Version 0.0.88","text":"<p>This new version brings updates to the ollama model names and sorting method, updates to dependencies, and a temporary fix to the openai version. It also includes enhancements to the model name handling in llamabot.</p>"},{"location":"releases/v0.0.88/#new-features","title":"New Features","text":"<ul> <li>Updated ollama model names and implemented a new sorting method. The models are now sorted by newest. (a19004) (Eric Ma)</li> <li>Enhanced model name handling in llamabot. The model names in ollama_model_names.txt have been reordered for better organization, and additional code cells have been added to ollama.ipynb for testing and demonstrating the use of PromptRecorder and SimpleBot. (57389f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.88/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Temporarily limited the version of openai dependency to &lt;=0.28.1 in pyproject.toml. This is due to an issue with OpenAI's update breaking a lot of LangChain. (1d881a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.88/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated langchain and llama_index dependencies in pyproject.toml. The langchain version has been set to 0.0.330 and llama_index version set to 0.8.62. This ensures three-way compatibility with openai, langchain, and llama-index until langchain is upgraded to work with the openai Python API without error. (e3cf0d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.89/","title":"V0.0.89","text":""},{"location":"releases/v0.0.89/#version-0089","title":"Version 0.0.89","text":"<p>This version includes several refactoring changes, new features, and documentation updates. The main focus of this release was to improve the code organization and efficiency, and to update the usage of the OpenAI API.</p>"},{"location":"releases/v0.0.89/#new-features","title":"New Features","text":"<ul> <li>Added a new test for the ImageBot class in the llamabot library. The test checks the behavior of the call method when it is invoked outside of a Jupyter notebook and no save path is provided. (0e23857) (Eric Ma)</li> <li>Introduced a new Jupyter notebook under the docs/examples directory. The notebook demonstrates how to use the ImageBot API to generate images from text using the OpenAI API. (8779040) (Eric Ma)</li> <li>Added ImageBot class to bot module for generating images based on prompts. (7174058) (Eric Ma)</li> <li>Increased the default token budget from 2048 to 4096 and added token budget for the new \"mistral\" model. (7f13698) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.89/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the cache-downloads-key in the pr-tests.yaml workflow file. The key now includes a hash of the 'environment.yml' file to ensure cache is updated when the environment changes. (1c12ff5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.89/#refactors","title":"Refactors","text":"<ul> <li>Moved the initialization of the OpenAI client into the <code>default_model</code> function. (bd50b90) (Eric Ma)</li> <li>Removed the direct access to the environment variable for the OpenAI API key in the client initialization. (7cb3d09) (Eric Ma)</li> <li>Changed the way model list attributes are accessed in the configure.py file of the llamabot CLI. (4deb93f) (Eric Ma)</li> <li>Extracted the filename generation logic, which was previously inside the ImageBot class, to a separate function named filename_bot. (aec4f3c) (Eric Ma)</li> <li>Removed direct assignment of OpenAI API key in init.py and replaced direct model list retrieval from OpenAI with client's model list method. (66fbcec) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.89/#documentation","title":"Documentation","text":"<ul> <li>Updated the docstring for the filename_bot function in the imagebot.py file. The updated docstring now includes parameter and return value descriptions. (c5dd51d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.89/#dependencies","title":"Dependencies","text":"<ul> <li>Updated the micromamba version from '1.4.5-0' to '1.5.1-2' in the pr-tests.yaml workflow. (6341f35) (Eric Ma)</li> <li>Updated dependencies versions including llama_index and langchain in environment.yml and pyproject.toml. (e9229cc) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.89/#tests","title":"Tests","text":"<ul> <li>Removed the deadline for the test_codebot_instance function in the python_prompt_library test suite to prevent potential timeout issues. (4a30e96) (Eric Ma)</li> <li>Removed the deadline for the simple bot initialization test to prevent false negatives due to time constraints. (16ee108) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.9/","title":"V0.0.9","text":""},{"location":"releases/v0.0.9/#009","title":"0.0.9","text":"<p>This new version includes several enhancements and new features, including the addition of a chatbot test, the integration of pytest-cov into the conda environment, and the successful implementation of streaming with SimpleBot. The chatbot UI prototype is now operational, and the code has been refactored for better organization and efficiency.</p>"},{"location":"releases/v0.0.9/#new-features","title":"New Features","text":"<ul> <li>Added a test for the chatbot functionality (0cc812) (Eric Ma)</li> <li>Integrated pytest-cov into the conda environment for better code coverage during testing (592297) (Eric Ma)</li> <li>Confirmed that streaming works with SimpleBot, enhancing real-time communication capabilities (049c23) (Eric Ma)</li> <li>Refactored panel markdown callback handler into panel_utils for better code organization (400bd0) (Eric Ma)</li> <li>Developed a rudimentary prototype of the chatbot UI, paving the way for user interaction (0e0bd5) (Eric Ma)</li> <li>Updated the simplebot panel example, providing a more comprehensive demonstration of the bot's capabilities (8515cf) (Eric Ma)</li> <li>Refactored bot.py into individual .py files for better code management and readability (5e97ed) (Eric Ma)</li> <li>Switched to Python version 3.10, taking advantage of the latest features and improvements in the language (f4c28f) (Eric Ma)</li> <li>Ensured the presence of typer-cli, enhancing command line interface functionality (856fbc) (Eric Ma)</li> <li>Added typer to optional dependencies, providing more flexibility in package installation (2e853e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.0.9/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.0.9/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.1.0/","title":"V0.1.0","text":""},{"location":"releases/v0.1.0/#version-010","title":"Version 0.1.0","text":"<p>This release includes several new features, bug fixes, and improvements to the codebase.</p>"},{"location":"releases/v0.1.0/#new-features","title":"New Features","text":"<ul> <li>Update default language model to Mistral and remove OpenAI API key warning (e74954b) (Eric Ma): The default language model used by the <code>SimpleBot</code> class has been updated to Mistral, which is a more cost-effective option compared to the previously used gpt-3.5-turbo-16k-0613 model. The OpenAI API key warning has also been removed, as the Mistral model does not require an API key.</li> <li>Add API key support for QABot and SimpleBot (b5f8253) (Eric Ma): This commit adds support for providing API keys to the QABot and SimpleBot classes, allowing for secure access to external services. This enhancement improves the security and flexibility of the bot's functionality.</li> <li>Update default language model environment variable (4bfd362) (Eric Ma): The default language model environment variable has been updated from <code>OPENAI_DEFAULT_MODEL</code> to <code>DEFAULT_LANGUAGE_MODEL</code> to align with the changes in the codebase.</li> <li>Update default language model to gpt-3.5-turbo-1106 (c8f0893) (Eric Ma): The default language model used by the commitbot has been updated to \"gpt-3.5-turbo-1106\" for improved performance and cost efficiency.</li> <li>Add logging for API key usage (3be39ad) (Eric Ma): Logging has been added to SimpleBot to log the usage of the API key for debugging and monitoring purposes.</li> <li>Add model_name parameter to SimpleBot instance (6a78332) (Eric Ma): A new parameter, model_name, has been added to the SimpleBot instance in the llamabot/cli/git.py file. The model_name is set to \"mistral/mistral-medium\". This change allows for more flexibility and customization when using the SimpleBot.</li> <li>Add new model name to ollama_model_names.txt (3110dc9) (Eric Ma): 'megadolphin' has been added to the list of model names in ollama_model_names.txt.</li> <li>Add new model name and refactor test_docstore (17352b8) (Eric Ma): 'llama-pro' has been added to ollama_model_names.txt and the test_docstore function has been refactored to remove unused imports and the make_fake_document function.</li> <li>Add Knowledge Graph bot (963cd63) (Eric Ma): A new feature has been added to the codebase, the Knowledge Graph bot (KGBot). The KGBot takes in a chunk of text and returns a JSON of triplets. It is tested with mistral-medium and uses the default language model. The bot is called with a query and returns a JSON of triplets.</li> <li>Add QABot class to llamabot (21197c1) (Eric Ma): A new class, DocQABot, has been added to the qabot.py file. This bot is designed to use pre-computed questions and answers to generate a response. It includes methods for adding documents for the bot to query and for calling the QABot. This enhancement will improve the bot's ability to generate responses based on the provided documents.</li> <li>Add DocumentStore class for LlamaBot (117baf7) (Eric Ma): A new feature has been added to the codebase, a DocumentStore class for LlamaBot. This class wraps around ChromaDB and provides methods to append and retrieve documents from the store. The DocumentStore class is defined in the newly created file llamabot/components/docstore.py.</li> <li>Add top-level API for llamabot's components (b2cf9f0) (Eric Ma): A new file, __init__.py, has been added which serves as the top-level API for llamabot's components.</li> </ul>"},{"location":"releases/v0.1.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix logging of API key (932beec) (Eric Ma): The commit fixes the logging of the API key in the SimpleBot class to display the complete key instead of just the first 5 characters. This change improves the clarity and security of the logging information.</li> <li>Fix environment variable retrieval in write_release_notes function (c627b18) (Eric Ma): This commit fixes an issue where the environment variable was not being retrieved correctly in the write_release_notes function.</li> <li>Fix stream parameter not being passed to bot function (185f2bc) (Eric Ma): This commit fixes an issue where the stream parameter was not being passed to the bot function in the cli/git module.</li> </ul>"},{"location":"releases/v0.1.0/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.1.1/","title":"V0.1.1","text":""},{"location":"releases/v0.1.1/#version-011","title":"Version 0.1.1","text":"<p>This release includes several improvements and bug fixes to enhance the overall functionality and stability of the project.</p>"},{"location":"releases/v0.1.1/#new-features","title":"New Features","text":"<ul> <li>Bumped version number to 0.1.1 (1ac7b6f, github-actions)</li> <li>Added python-slugify to project dependencies for better string processing capabilities (330152, Eric Ma)</li> <li>Removed unnecessary logger import and usage in SimpleBot (37182cb, Eric Ma)</li> <li>Streamlined the querybot notebook and updated the repository URL (bc17da7, Eric Ma)</li> <li>Enhanced the querybot notebook with utility functions and integrated chromadb (8a74c73, Eric Ma)</li> <li>Added python-slugify dependency to the environment.yml (f5e2dea, Eric Ma)</li> <li>Implemented a new 'repo' subcommand in the CLI module for repository interactions (58fe450, Eric Ma)</li> <li>Added progress bar and deduplication to document processing (1e76998, Eric Ma)</li> <li>Slugified collection names for document storage (4d8dfa2, Eric Ma)</li> <li>Optimized document loading and splitting in doc_processor (6731b97, Eric Ma)</li> <li>Improved API key storage and usage documentation (c1bd6d9, Eric Ma)</li> <li>Updated default model to gpt-4 in git.py (3817430, Eric Ma)</li> <li>Added support for document collections in LlamaBot (009890c, Eric Ma)</li> <li>Improved file search algorithm in file_finder (87d079c, Eric Ma)</li> <li>Added support for configuring multiple API keys (d00c14d, Eric Ma)</li> <li>Updated QueryBot parameters and model names in pdf.ipynb (c83d834, Eric Ma)</li> <li>Added support for configuring multiple API providers (230e816, Eric Ma)</li> </ul>"},{"location":"releases/v0.1.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed assertion for split_document function (61e0688, Eric Ma)</li> <li>Ensured non-negative chunk_size parameter in doc_processor (7f9d17b, Eric Ma)</li> <li>Adjusted assertions for split_document_with_overlap (7237647, Eric Ma)</li> <li>Fixed adding documents only when document_paths is not empty (0808ced, Eric Ma)</li> <li>Updated the QueryBot constructor to support optional document_paths (72e6c02, Eric Ma)</li> <li>Removed unnecessary newline characters and updated python version in imagebot.ipynb (2d415c8, Eric Ma)</li> <li>Updated the chatbot to use the Mistral model instead of GPT-4 (51773db, Eric Ma)</li> <li>Added support for LiteLLM models in LLaMaBot documentation (249b3e5, Eric Ma)</li> </ul>"},{"location":"releases/v0.1.1/#deprecations","title":"Deprecations","text":"<ul> <li>Removed simple_chatbot.ipynb from the repository (59f1d4e, Eric Ma)</li> <li>Removed scratch notebook from examples (b4f0e94, Eric Ma)</li> <li>Updated deprecations section in v0.1.0 release notes (776e98f, Eric Ma)</li> </ul>"},{"location":"releases/v0.1.2/","title":"V0.1.2","text":"<p>Here are the release notes based on the provided commit log:</p>"},{"location":"releases/v0.1.2/#version-012","title":"Version 0.1.2","text":"<p>This release includes new model names for llamabot and an update to its dependencies.</p>"},{"location":"releases/v0.1.2/#new-features","title":"New Features","text":"<ul> <li>Added 'qwen' and 'tinydolphin' to the list of llamabot model names and updated dependencies to include 'pydantic' (21c1492) (Eric Ma)</li> <li>Bumped version number from 0.1.1 to 0.1.2 (76b9b4) (github-actions)</li> </ul>"},{"location":"releases/v0.1.2/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes were included in this release.</p>"},{"location":"releases/v0.1.2/#deprecations","title":"Deprecations","text":"<p>No deprecations were included in this release.</p> <p>Note: The commit <code>cd04ed38</code> is not included in the release notes as it only added release notes for version 0.1.1.</p>"},{"location":"releases/v0.10.0/","title":"V0.10.0","text":""},{"location":"releases/v0.10.0/#version-0100","title":"Version 0.10.0","text":"<p>This release introduces significant enhancements and refactoring to the AgentBot, improving its functionality, error handling, and modularity. New testing frameworks and caching mechanisms have been implemented to ensure robustness and efficiency.</p>"},{"location":"releases/v0.10.0/#new-features","title":"New Features","text":"<ul> <li>Introduced a caching mechanism and tool execution enhancements to store results from tool executions with timestamps, allowing cached results to be used as arguments in subsequent tool calls. (2512c6) (Eric Ma)</li> <li>Enhanced message handling and error management in AgentBot, including support for BaseMessage in user message creation and explicit error handling tools. (6b1a74) (Eric Ma)</li> <li>Added new AgentBot functionality to execute a sequence of tools based on user input, improving the bot's interactivity and responsiveness. (ad32fe) (Eric Ma)</li> <li>Updated numpy and added biopython to dependencies, expanding the project's capability for scientific computations. (1e0283) (Eric Ma)</li> <li>Enhanced AgentBot with argument tracking and concise logging, improving the transparency and traceability of tool executions. (3acee6) (Eric Ma)</li> <li>Enhanced AgentBot to use AIMessage for final output, providing a more structured and readable output format. (c45379) (Eric Ma)</li> <li>Added tool management and enhancements in agent processing, introducing a new decorator for better tool management and implementing a new tool to indicate the completion of agent's message processing. (ce19bb) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed FTS index creation on the correct table instance to ensure proper functionality of the full-text search capabilities. (358ec1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.0/#deprecations","title":"Deprecations","text":"<ul> <li>Refactored AgentBot to not inherit from SimpleBot, making it an independent class which enhances its modularity and reduces dependencies. (e2b8af) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.1/","title":"V0.10.1","text":""},{"location":"releases/v0.10.1/#version-0101","title":"Version 0.10.1","text":"<p>This minor release includes a version bump and improvements in type hinting within the llamabot components.</p>"},{"location":"releases/v0.10.1/#new-features","title":"New Features","text":"<ul> <li>Improved type hinting in the <code>process_messages</code> function to support variable length tuples, enhancing the function's flexibility and correctness in handling different input types. (440a6af) (Eric Ma)</li> <li>Updated llamabot version in <code>pixi.lock</code> from 0.9.21 to 0.10.0, ensuring compatibility with the latest features and fixes. (440a6af) (Eric Ma)</li> <li>Extended <code>test_process_messages_with_nested_types</code> to better verify the handling of various input types, improving test coverage and reliability. (440a6af) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.10.1/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul> <p>This release primarily focuses on internal improvements and does not introduce any new user-facing features or deprecations.</p>"},{"location":"releases/v0.10.10/","title":"V0.10.10","text":""},{"location":"releases/v0.10.10/#version-01010","title":"Version 0.10.10","text":"<p>This release includes enhancements to testing, a bug fix, and updates to dependencies, improving the overall stability and functionality of the software.</p>"},{"location":"releases/v0.10.10/#new-features","title":"New Features","text":"<ul> <li>Added support for checking if the model's response schema is compatible with StructuredBot requirements. This ensures that only supported models are used, enhancing reliability. (5df4ed3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue where the AgentBot was not properly initialized with the model_name parameter in test scenarios, ensuring that tests reflect accurate initialization conditions. (18b7c4d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.10/#test-enhancements","title":"Test Enhancements","text":"<ul> <li>Added a new test to verify that StructuredBot correctly handles unsupported models by raising an appropriate error. This helps in maintaining robust error handling in the system. (aa05cad) (Eric Ma)</li> <li>Introduced a test to ensure proper initialization of AgentBot with the model_name parameter, confirming that the setup is correctly handled. (6e4f9ef) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.10/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated the litellm dependency to version 1.59.8 to maintain compatibility with new versions and ensure the software remains up-to-date with its dependencies. (9bd830d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.11/","title":"V0.10.11","text":""},{"location":"releases/v0.10.11/#version-01011","title":"Version 0.10.11","text":"<p>This release introduces several enhancements and updates, focusing on improving the security and functionality of script execution in sandbox environments, refining the AgentBot capabilities, and updating dependencies for better performance and security.</p>"},{"location":"releases/v0.10.11/#new-features","title":"New Features","text":"<ul> <li>Implemented a secure sandbox environment for executing agent-generated Python scripts using Docker, ensuring resource and security constraints are maintained. (6be4d1) (Eric Ma)</li> <li>Enhanced the AgentBot with new capabilities for internet search using the DuckDuckGo API and improved script execution handling. (0a0392) (Eric Ma)</li> <li>Added new example notebooks demonstrating the usage of the AgentBot class and provided additional examples and model experiments in existing notebooks. (928fef) (Eric Ma)</li> <li>Added secure script execution for LLM agents using Docker to prevent unauthorized access and ensure script integrity. (b73e2b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.11/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Ensured script execution within the Docker container explicitly uses Python, addressing issues related to file permissions or incorrect shebang configurations. (77e8b0) (Eric Ma)</li> <li>Enhanced error handling and typing in sandbox execution to improve reliability and clarity when errors occur during script execution. (a27af1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.11/#deprecations","title":"Deprecations","text":"<ul> <li>Removed unused 'write_and_execute_script' function from the AgentBot class to streamline the codebase. (1c9ae8) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.12/","title":"V0.10.12","text":""},{"location":"releases/v0.10.12/#version-01012","title":"Version 0.10.12","text":"<p>This release includes updates to dependencies and improvements to the StructuredBot's handling of specific model outputs.</p>"},{"location":"releases/v0.10.12/#new-features","title":"New Features","text":"<ul> <li>Updated the litellm dependency to ensure compatibility and performance enhancements. (a905216) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.12/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Enhanced the StructuredBot to correctly initialize and handle ollama_chat models, which support structured outputs. This update includes a new condition to bypass standard checks for these models and an updated error message indicating support for gpt-4 models. A test case has been added to verify correct initialization without errors. (84dc794) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.12/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.2/","title":"V0.10.2","text":""},{"location":"releases/v0.10.2/#version-0102","title":"Version 0.10.2","text":"<p>This release includes improvements to the LlamaBot components, specifically enhancing the functionality of the FTS index in the LanceDBDocStore.</p>"},{"location":"releases/v0.10.2/#new-features","title":"New Features","text":"<ul> <li>Enabled the replacement of existing Full-Text Search (FTS) indexes to ensure up-to-date search capabilities in LanceDBDocStore. (973f850) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the LlamaBot version in the pixi.lock file to maintain consistency with the latest software version, fixing dependency issues. (973f850) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.2/#deprecations","title":"Deprecations","text":"<p>No deprecations were introduced in this version.</p>"},{"location":"releases/v0.10.3/","title":"V0.10.3","text":""},{"location":"releases/v0.10.3/#version-0103","title":"Version 0.10.3","text":"<p>This release includes new document store components for the llamabot package, enhancing its functionality and usability.</p>"},{"location":"releases/v0.10.3/#new-features","title":"New Features","text":"<ul> <li>Added new document store components including BM25DocStore, LanceDBDocStore, and ChromaDBDocStore to the llamabot package, making it more versatile in handling different types of document storage. (ebe6adf) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.3/#bug-fixes","title":"Bug Fixes","text":"<p>None</p>"},{"location":"releases/v0.10.3/#deprecations","title":"Deprecations","text":"<p>None</p>"},{"location":"releases/v0.10.3/#version-0102","title":"Version 0.10.2","text":"<p>This version introduces the AgentBot to the llamabot package and includes various refactoring improvements to enhance code maintainability and readability.</p>"},{"location":"releases/v0.10.3/#new-features_1","title":"New Features","text":"<ul> <li>Introduced AgentBot to the llamabot package, expanding its capabilities to handle more complex tasks. (6197623) (Eric Ma)</li> <li>Added a comprehensive tutorial for building an AgentBot capable of handling tasks like restaurant bill management and stock price analysis, complete with detailed setup instructions and code snippets. (2099e6b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.3/#bug-fixes_1","title":"Bug Fixes","text":"<p>None</p>"},{"location":"releases/v0.10.3/#deprecations_1","title":"Deprecations","text":"<p>None</p>"},{"location":"releases/v0.10.4/","title":"V0.10.4","text":""},{"location":"releases/v0.10.4/#version-0104","title":"Version 0.10.4","text":"<p>This release includes enhancements to the StructuredBot, updates to the GitHub Actions workflow for Python package releases, and a transition from bumpversion to bump2version for version management.</p>"},{"location":"releases/v0.10.4/#new-features","title":"New Features","text":"<ul> <li>Enhanced StructuredBot with JSON extraction and validation improvements. This update introduces a new parameter to handle optional validation failures and refactors JSON extraction into a separate function for better clarity. Tests have been updated to cover new functionalities and scenarios. (976e0e1) (Eric Ma)</li> <li>Updated GitHub Actions workflow for Python package release to include setup for Pixi Environment with caching capabilities. The installation commands have been changed to use 'uv tool install' for better efficiency. (6af34ac) (Eric Ma)</li> <li>Replaced bumpversion with bump2version in the GitHub Actions workflow to manage project versions more reliably. This change includes updates to all references of bumpversion to bump2version in the workflow commands. (7c96f91) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.4/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.5/","title":"V0.10.5","text":""},{"location":"releases/v0.10.5/#version-0105","title":"Version 0.10.5","text":"<p>This release includes several refinements and bug fixes to improve the functionality and efficiency of our bots. Key updates include enhancements to the message handling in LlamaBot, refinements in JSON response handling in SimpleBot, and critical bug fixes in StructuredBot.</p>"},{"location":"releases/v0.10.5/#new-features","title":"New Features","text":"<ul> <li>Introduced developer-specific message handling in LlamaBot, adding a new DeveloperMessage class and updating message roles for better clarity and functionality. (32d1db5) (Eric Ma)</li> <li>Added developer import and alias in the messages module of LlamaBot to handle developer-specific messages more efficiently. (7e29952) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed model construction in StructuredBot to directly utilize parsed codeblocks, enhancing the accuracy and reliability of model operations. (683c93d) (Eric Ma)</li> <li>Corrected JSON string handling in StructuredBot's mock responses to ensure proper format compliance and data integrity. (05af1c2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.5/#refactorings","title":"Refactorings","text":"<ul> <li>Streamlined JSON response handling in SimpleBot by employing Pydantic models, ensuring robust data validation and error handling. (3aad11a) (Eric Ma)</li> <li>Refined message filtering and response format handling in LlamaBot to focus on essential data, improving performance and maintainability. (5cf1531) (Eric Ma)</li> <li>Overhauled the configuration and test handling in StructuredBot, removing redundant configurations and correcting data types for enhanced clarity and efficiency. (9ffe21e) (Eric Ma)</li> <li>Removed a redundant debug print statement in SimpleBot, cleaning up the output and focusing on essential information. (ebcf93d) (Eric Ma)</li> </ul> <p>This version continues our commitment to delivering robust and efficient software solutions, with improvements driven by user feedback and technological advancements.</p>"},{"location":"releases/v0.10.6/","title":"V0.10.6","text":""},{"location":"releases/v0.10.6/#version-0106","title":"Version 0.10.6","text":"<p>This release introduces new functionalities for handling developer messages and improves the robustness of message creation with additional test coverage.</p>"},{"location":"releases/v0.10.6/#new-features","title":"New Features","text":"<ul> <li>Added a new <code>dev</code> function to create <code>DeveloperMessage</code> instances from various content types, enhancing message handling capabilities. This function supports strings, file paths, and <code>BaseMessage</code> objects, with added error handling for non-existent file paths. (6cf6f39) (Eric Ma)</li> <li>Implemented new test cases for the <code>dev()</code> function and <code>DeveloperMessage</code> creation, ensuring the function handles a variety of input types correctly, including edge cases like non-existent files. (18b36ce) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.6/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.10.6/#deprecations","title":"Deprecations","text":""},{"location":"releases/v0.10.7/","title":"V0.10.7","text":""},{"location":"releases/v0.10.7/#version-0107","title":"Version 0.10.7","text":"<p>This release includes improvements to the CLI documentation and bot initialization, enhancing clarity and consistency across the system.</p>"},{"location":"releases/v0.10.7/#new-features","title":"New Features","text":"<ul> <li>Reorganized the import statements to ensure a logical grouping and order, making the codebase easier to navigate and maintain. (088d9f) (Eric Ma)</li> <li>Updated the bot initialization process to consistently use the 'llamabot' module alias, streamlining future updates and maintenance. (088d9f) (Eric Ma)</li> <li>Introduced a new prompt function to provide users with information about out-of-date checks, improving user interaction and system feedback. (088d9f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.7/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.8/","title":"V0.10.8","text":""},{"location":"releases/v0.10.8/#version-0108","title":"Version 0.10.8","text":"<p>This release includes updates to project dependencies and the addition of new packages, enhancing functionality and maintaining up-to-date security standards.</p>"},{"location":"releases/v0.10.8/#new-features","title":"New Features","text":"<ul> <li>Updated 'litellm' to version '&gt;=1.56.10' and added 'ollama' and 'pdf2image' to the project dependencies for enhanced functionality and new features. (fdc4bfe) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.8/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.10.9/","title":"V0.10.9","text":""},{"location":"releases/v0.10.9/#version-0109","title":"Version 0.10.9","text":"<p>This release includes enhancements to the QueryBot, adding support for additional query input types and expanding the model names database. The test suite for QueryBot has also been improved to include user interaction scenarios.</p>"},{"location":"releases/v0.10.9/#new-features","title":"New Features","text":"<ul> <li>Enhanced QueryBot to support more diverse query input types, improving its flexibility and usability. This update allows QueryBot to handle different types of message inputs more effectively. (314f78a) (Eric Ma)</li> <li>Updated the list of model names in the ollama_model_names.txt file and improved the QueryBot test suite to include tests for user interactions, ensuring better coverage and reliability. (b28f30f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.10.9/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.10.9/#deprecations","title":"Deprecations","text":""},{"location":"releases/v0.11.0/","title":"V0.11.0","text":""},{"location":"releases/v0.11.0/#version-0110","title":"Version 0.11.0","text":"<p>This release introduces several enhancements and bug fixes that improve the functionality and stability of the software. Key updates include new dependency management features, enhanced error handling, and improvements in the documentation and testing setups.</p>"},{"location":"releases/v0.11.0/#new-features","title":"New Features","text":"<ul> <li>Added 'rank-bm25' library to dependency management for enhanced ranking capabilities in search functionalities (27b53f) (Eric Ma)</li> <li>Updated README and improved error handling for optional dependencies to guide users through installation processes (f15271) (Eric Ma)</li> <li>Introduced SQLAlchemy and greenlet as new dependencies to enhance database operations and asynchronous processing (41b646) (Eric Ma)</li> <li>Added 'all' extra dependency group to package configuration to simplify dependency management (cadc85) (Eric Ma)</li> <li>Added functionality to generate and display historical events in notebooks, enhancing interactive data exploration (639b88) (Eric Ma)</li> <li>Introduced a new Jupyter notebook for structured bot demonstration, showcasing advanced usage scenarios (9bb3dd) (Eric Ma)</li> <li>Updated pre-commit configuration and added new files for an upcoming talk at PyData Boston 2025, improving code quality checks (1c8c95) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed Python package installation command in release workflow to ensure all CLI dependencies are correctly installed (b3ddd3) (Eric Ma)</li> <li>Generalized exception handling when opening a table in LanceDBDocStore to prevent crashes due to unhandled exceptions (d17a8e) (Eric Ma)</li> <li>Ensured the storage path exists before connecting to the database in LanceDBDocStore, preventing errors during database initialization (6d3b0f) (Eric Ma)</li> <li>Added import error handling for 'astor' and 'git' modules to provide clear guidance on missing dependencies (e9f0b6) (Eric Ma)</li> <li>Corrected the dependency installation command in the CI workflow to include all necessary packages for testing (194e80) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.0/#deprecations","title":"Deprecations","text":"<ul> <li>Removed cache-related functionality from the project, streamlining the codebase and focusing on core features (844eb0) (Eric Ma)</li> <li>Deleted unused CLI application file, removing outdated or relocated functionalities (cf6e2f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.1/","title":"V0.11.1","text":""},{"location":"releases/v0.11.1/#version-0111","title":"Version 0.11.1","text":"<p>This minor release includes a series of refactoring improvements aimed at optimizing import handling across various modules, enhancing code readability, maintainability, and performance.</p>"},{"location":"releases/v0.11.1/#new-features","title":"New Features","text":"<ul> <li>Refactored import statement for <code>pyzotero</code> to only occur within the <code>load_zotero</code> function to reduce unnecessary loading (d6b415) (Eric Ma)</li> <li>Improved import handling and initialization of the progress bar in the zotero library for better performance and clarity (ebfedb) (Eric Ma)</li> <li>Reorganized import statements and error handling in <code>zotero.py</code> to enhance code readability and reduce global try-except blocks (264e61) (Eric Ma)</li> <li>Optimized import handling for <code>prompt_toolkit</code> in CLI utilities to decrease initial load time and minimize import errors (751c06) (Eric Ma)</li> <li>Moved <code>pyperclip</code> import within functions to optimize module loading and reduce initial script load time (144cec) (Eric Ma)</li> <li>Relocated <code>nbformat</code> import to function scope to improve module's initial load time and avoid unnecessary imports (80a8c3) (Eric Ma)</li> <li>Refactored import error handling for <code>pyperclip</code> and <code>git</code> modules to specific functions, ensuring error messages are contextually relevant (464615) (Eric Ma)</li> <li>Reorganized CLI imports and spacing in the save method to adhere to PEP 8 and improve readability (194da5) (Eric Ma)</li> <li>Moved the import of <code>prompt_toolkit</code> to within the <code>default_model</code> function to reduce initial load time (673e91) (Eric Ma)</li> <li>Relocated <code>pyperclip</code> import to specific functions to optimize module loading and improve startup performance (bd57b2) (Eric Ma)</li> <li>Refactored import checks to be function-local in <code>code_manipulation</code> for better modularity and streamlined module initialization (7eb94c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.11.1/#deprecations","title":"Deprecations","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.11.2/","title":"V0.11.2","text":""},{"location":"releases/v0.11.2/#version-0112","title":"Version 0.11.2","text":"<p>This release includes several improvements and bug fixes in the CI workflows, enhancing the testing environment and script management. Additionally, a minor version bump reflects these incremental changes.</p>"},{"location":"releases/v0.11.2/#new-features","title":"New Features","text":"<ul> <li>Added a Python setup step specifically for the UV environment in the PR tests workflow to ensure the correct environment setup. (e930d2) (Eric Ma)</li> <li>Refactored CI test scripts to use a dedicated script for llamabot testing, improving maintainability and consistency. (23068e) (Eric Ma)</li> <li>Updated CI workflows to replace the 'bare' environment with 'uv' and added new test jobs to enhance testing coverage and environment specificity. (3e96b1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected the model name format in the <code>yo.py</code> script to ensure consistency across the application. (9a2e8d) (Eric Ma)</li> <li>Added a delay after the installation script in the CI workflow to prevent race conditions and ensure proper initialization. (bb3ea3) (Eric Ma)</li> <li>Removed an unnecessary and redundant command from the CI workflow, streamlining the process. (7188db) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.2/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.11.3/","title":"V0.11.3","text":""},{"location":"releases/v0.11.3/#version-0113","title":"Version 0.11.3","text":"<p>This release includes enhancements to the Ollama model integration, the addition of the Pylance dependency for improved project configuration, and a refactoring of the GitHub Actions workflow to optimize testing processes.</p>"},{"location":"releases/v0.11.3/#new-features","title":"New Features","text":"<ul> <li>Enhanced Ollama model integration with new model names and updated notebook examples for better clarity and functionality. (4d085a) (Eric Ma)</li> <li>Added Pylance as a dependency under the 'rag' extra in the project configuration to support various environments. (b9a984) (Eric Ma)</li> <li>Refactored GitHub Actions workflow to use a matrix strategy for test jobs, supporting multiple test types and environments. This optimization ensures efficient CI processes and conditional code coverage uploads. (93e498) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.11.3/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.11.4/","title":"V0.11.4","text":""},{"location":"releases/v0.11.4/#version-0114","title":"Version 0.11.4","text":"<p>This release includes updates to dependencies and the addition of a new optional dependency, enhancing the flexibility and security of the package.</p>"},{"location":"releases/v0.11.4/#new-features","title":"New Features","text":"<ul> <li>Updated the hash for the llamabot package to ensure integrity and security of the dependency. (aecfc2) (Eric Ma)</li> <li>Added a new optional dependency, llamabot[ui], under the 'rag' extra in pyproject.toml to enhance user interface capabilities. (aecfc2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.11.4/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.11.5/","title":"V0.11.5","text":""},{"location":"releases/v0.11.5/#version-0115","title":"Version 0.11.5","text":"<p>This release includes updates to project dependencies and enhancements to the project's package management.</p>"},{"location":"releases/v0.11.5/#new-features","title":"New Features","text":"<ul> <li>Updated project dependencies and refreshed the lockfile to ensure compatibility and security. (0cf734) (Eric Ma)</li> <li>Added 'sentence-transformers' to the 'rag' extra dependencies, enhancing the project's NLP capabilities. (0cf734) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.11.5/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.11.6/","title":"V0.11.6","text":""},{"location":"releases/v0.11.6/#version-0116","title":"Version 0.11.6","text":"<p>This release includes several bug fixes and enhancements to the ChromaDBDocStore, improving its functionality and stability. Additionally, the underlying dependencies have been updated to ensure compatibility and performance.</p>"},{"location":"releases/v0.11.6/#new-features","title":"New Features","text":"<ul> <li>Enhanced ChromaDBDocStore with support for embeddings and metadata in document handling, allowing for more complex data structures and storage capabilities. (3b68e7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the default argument for metadata in the append method to prevent issues with mutable default arguments. (67c47a) (Eric Ma)</li> <li>Corrected validation logic for document metadata and embeddings length to ensure data integrity and provide clearer error messages. (e0d7f5) (Eric Ma)</li> <li>Added validation for input list lengths in ChromaDBDocStore to ensure consistency across documents, metadata, and embeddings, preventing runtime errors. (64c2ca) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.6/#refactorings","title":"Refactorings","text":"<ul> <li>Removed unnecessary pydantic validate_call decorators from methods, simplifying the codebase and reducing dependencies. (ee096a) (Eric Ma)</li> <li>Simplified the document addition logic in ChromaDBDocStore by streamlining the process of adding documents. (612fa4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.6/#test-enhancements","title":"Test Enhancements","text":"<ul> <li>Refactored tests to use temporary directories, ensuring better isolation and cleanup during test executions. (56cd71) (Eric Ma)</li> <li>Updated tests to remove mocking and directly verify functionality, enhancing the reliability and accuracy of tests. (aad368) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.6/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated astral-sh/setup-uv from version 3 to 6, ensuring compatibility with the latest tooling and features. (d6211f) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.11.7/","title":"V0.11.7","text":""},{"location":"releases/v0.11.7/#version-0117","title":"Version 0.11.7","text":"<p>This release introduces enhancements to the QueryBot, including new support for ChromaDB and improvements to the test framework for better maintainability and readability.</p>"},{"location":"releases/v0.11.7/#new-features","title":"New Features","text":"<ul> <li>Enhanced QueryBot with support for ChromaDB and additional configuration options. This update allows users to use ChromaDB as a document store and configure custom paths for document storage. (9aeaaf) (Eric Ma)</li> <li>Refactored and simplified QueryBot test cases to improve readability and maintainability. This change includes removing complex hypothesis-based parameterization and using fixed values for easier understanding and maintenance. (a53cb5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.11.7/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.11.8/","title":"V0.11.8","text":""},{"location":"releases/v0.11.8/#version-0118","title":"Version 0.11.8","text":"<p>This release includes a minor version bump and a significant refactor in the usage of the slugify function to enhance consistency in naming conventions across different classes.</p>"},{"location":"releases/v0.11.8/#new-features","title":"New Features","text":"<ul> <li>Refactored slugify usage to ensure consistent naming conventions across various database document store classes. This change includes removing unnecessary slugify usage and adding it where needed for uniformity. (dd2f66) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.11.8/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.11.9/","title":"V0.11.9","text":""},{"location":"releases/v0.11.9/#version-0119","title":"Version 0.11.9","text":"<p>This release includes enhancements to the LanceDBDocStore, improving its functionality and testing, alongside routine maintenance updates.</p>"},{"location":"releases/v0.11.9/#new-features","title":"New Features","text":"<ul> <li>Enhanced the LanceDBDocStore to support metadata and embeddings in append and extend operations, and added robust testing for these functionalities. (bbbcb5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.11.9/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.11.9/#deprecations","title":"Deprecations","text":""},{"location":"releases/v0.12.0/","title":"V0.12.0","text":""},{"location":"releases/v0.12.0/#version-0120","title":"Version 0.12.0","text":"<p>This release introduces significant enhancements to bot functionalities, improved modularity in code, and updates to dependencies for better compatibility and performance.</p>"},{"location":"releases/v0.12.0/#new-features","title":"New Features","text":"<ul> <li>Enhanced AgentBot and tools with improved functionality and error handling, including retry logic and randomized headers for DuckDuckGo search. (99bc6f0) (Eric Ma)</li> <li>Enhanced database handling and testing in llamabot, including directory-specific <code>.gitignore</code> handling. (e959c8a) (Eric Ma)</li> <li>Introduced sqlite-based logging for bot interactions, replacing the PromptRecorder class. (e078b45) (Eric Ma)</li> <li>Enhanced bot functionality with memory and structured response handling, including chat memory support and validation against Pydantic models. (bda168b) (Eric Ma)</li> <li>Refactored bot components for improved modularity and functionality, including the removal of the ChatBot class and updates to QueryBot and SimpleBot. (6fb0c84) (Eric Ma)</li> <li>Added smoke tests configuration to PR tests workflow to ensure robustness before deployment. (ea6c9a8) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated dependencies to resolve compatibility issues, ensuring smoother operations and integration with other software components. (d1f0db) (Eric Ma)</li> <li>Fixed an uninitialized variable issue in SimpleBot class to prevent potential runtime errors. (ce66d05) (Eric Ma)</li> <li>Ensured sqlite_log function appends the response message to the messages list, improving logging accuracy. (5f90e28) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.0/#deprecations","title":"Deprecations","text":"<ul> <li>Removed ChromaDB support from the codebase due to dependency issues, streamlining the document store functionality. (ede526e) (Eric Ma)</li> <li>Removed unused PromptRecorder and related functionality, simplifying the logging and interaction recording processes. (454e9b1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.1/","title":"V0.12.1","text":""},{"location":"releases/v0.12.1/#version-0121","title":"Version 0.12.1","text":"<p>This release includes several enhancements and bug fixes that improve the functionality and documentation of the project, along with dependency updates.</p>"},{"location":"releases/v0.12.1/#new-features","title":"New Features","text":"<ul> <li>Introduced new agent capabilities and a notebook demonstration for LlamaBot, showcasing advanced functionalities like dataset downloading, model training, and cross-validation. (9aeddb5) (Eric Ma)</li> <li>Added a new example notebook to demonstrate LlamaBot functionalities, enhancing the interactive experience and usability of the bot. (f4cba9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the model name and improved documentation in the introductory notebook to enhance clarity and user understanding. (e4c1db) (Eric Ma)</li> <li>Removed unnecessary timeout checks in <code>test_write_and_execute_script_schema</code>, simplifying the test process and removing irrelevant assertions. (22ec7e) (Eric Ma)</li> <li>Updated the model name and improved documentation in the introductory notebook, ensuring accurate and clear information is presented. (7a6e70) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.1/#enhancements","title":"Enhancements","text":"<ul> <li>Updated dependencies and model names to keep the project up-to-date with the latest versions and standards, ensuring better performance and compatibility. (1ec551) (Eric Ma)</li> <li>Updated the llamabot package to its latest version, ensuring the project benefits from the latest improvements and bug fixes. (366f41) (Eric Ma)</li> <li>Added the 'ty' package to dependencies to ensure all necessary libraries are available for the project's functionality. (a5d3d3) (Eric Ma)</li> <li>Improved code formatting and readability in notebooks, making the code cleaner and easier to understand. (ac65e6) (Eric Ma)</li> <li>Moved import statements to cell definitions in notebooks to avoid potential naming conflicts and improve modularity. (425466) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.1/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated the llamabot package to ensure compatibility with the latest features and improvements. (366f41) (Eric Ma)</li> <li>Added the 'ty' package to the project's dependencies to support new functionalities. (a5d3d3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.1/#documentation","title":"Documentation","text":"<ul> <li>Improved documentation in the introductory notebook to enhance user understanding and provide clearer instructions. (7a6e70) (Eric Ma)</li> <li>Updated and improved documentation across various notebooks, ensuring all information is current and clear. (e4c1db) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.10/","title":"V0.12.10","text":""},{"location":"releases/v0.12.10/#version-01210","title":"Version 0.12.10","text":"<p>This release includes critical bug fixes to enhance the functionality and stability of the document store features.</p>"},{"location":"releases/v0.12.10/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.12.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue in LanceDBDocStore where setting <code>use_tantivy</code> to <code>False</code> prevents a <code>ValueError</code> during FTS index creation by ensuring <code>field_names</code> is a string. (89a6869) (Eric Ma)</li> <li>Corrected a <code>ValueError</code> in <code>create_fts_index</code> by changing the <code>field_names</code> argument from a list to a string when <code>use_tantivy</code> is set to <code>False</code>. (4ad2dab) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.10/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this version.</li> </ul>"},{"location":"releases/v0.12.11/","title":"V0.12.11","text":""},{"location":"releases/v0.12.11/#version-01211","title":"Version 0.12.11","text":"<p>This release includes several documentation improvements, enhancements in CI workflows, and updates to dependencies and testing strategies.</p>"},{"location":"releases/v0.12.11/#new-features","title":"New Features","text":"<ul> <li>Added a GitHub Actions workflow to automatically update Ollama models daily and on demand, removing the need for a local pre-commit hook for this task. (7b1f7c2) (Eric Ma)</li> <li>Introduced a new job in GitHub workflows to check for code changes and conditionally run tests, optimizing CI resources. (a7fcff3) (Eric Ma)</li> <li>Added comprehensive tutorial for using the <code>@prompt</code> decorator, covering its usage, integration, and best practices. (4a41dce) (Eric Ma)</li> <li>Updated README with instructions for using local models with LMStudio, including setup and example code. (7771976) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.11/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Removed test for StructuredBot unsupported model raising ValueError, reflecting changes in model support handling. (a79ebbe) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.11/#deprecations","title":"Deprecations","text":"<ul> <li>Removed extras from the chonkie dependency in pyproject.toml to simplify dependency management. (5c992fd) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.2/","title":"V0.12.2","text":""},{"location":"releases/v0.12.2/#version-0122","title":"Version 0.12.2","text":"<p>This minor release includes a simplification of the .gitignore file creation process within the ensure_db_in_gitignore function, enhancing maintainability and clarity.</p>"},{"location":"releases/v0.12.2/#new-features","title":"New Features","text":"<ul> <li>Simplified the creation of the .gitignore file in the .llamabot directory to only ignore all files within it, removing unnecessary comments and exceptions. (f7bfe9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.2/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.12.2/#deprecations","title":"Deprecations","text":"<p>Please note that this release does not include any new bug fixes or deprecations.</p>"},{"location":"releases/v0.12.3/","title":"V0.12.3","text":""},{"location":"releases/v0.12.3/#version-0123","title":"Version 0.12.3","text":"<p>This release introduces significant enhancements to the AgentBot and LlamaBot components, improving their capabilities in autonomous planning, execution, and precise summarization. Additionally, a new notebook demonstration for AgentBot usage is included, showcasing practical applications with various tools and models.</p>"},{"location":"releases/v0.12.3/#new-features","title":"New Features","text":"<ul> <li>Added a notebook demonstrating how to use AgentBot with specific tools and models, including examples for summarizing album ratings, training classifiers, and making sports predictions. (2fcb347) (Eric Ma)</li> <li>Introduced a new system prompt for a summarization bot that provides focused and professional summaries of web content, along with customizable bot configuration options. (7c3dc11) (Eric Ma)</li> <li>Enhanced AgentBot with autonomous planning and execution capabilities, including default system prompts and a planner bot for strategic task management. (d95e20f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.3/#bug-fixes","title":"Bug Fixes","text":"<p>None in this release.</p>"},{"location":"releases/v0.12.3/#deprecations","title":"Deprecations","text":"<p>None in this release.</p>"},{"location":"releases/v0.12.4/","title":"V0.12.4","text":""},{"location":"releases/v0.12.4/#version-0124","title":"Version 0.12.4","text":"<p>This release includes significant enhancements to the LlamaBot framework, improving tool handling, debugging, and agent setup. Additionally, the testing framework has been updated to support new schema structures.</p>"},{"location":"releases/v0.12.4/#new-features","title":"New Features","text":"<ul> <li>Refactored the LlamaBot agents notebook to update dependencies and improve agent setup, enhancing example usage with new capabilities like dataset analysis and sports predictions. (ec344d6) (Eric Ma)</li> <li>Updated the AgentBot to support an optional planning bot and improved tool call handling with enhanced logging and concurrency management. (d28cce4) (Eric Ma)</li> <li>Modified the tool decorator to support a nested function schema structure, improving JSON schema consistency across tools. (d33d28) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.12.4/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul> <p>This version continues to refine the functionality and usability of the LlamaBot, ensuring better performance and easier integration in various applications.</p>"},{"location":"releases/v0.12.5/","title":"V0.12.5","text":""},{"location":"releases/v0.12.5/#version-0125","title":"Version 0.12.5","text":"<p>This release includes significant enhancements in UI component reusability, improvements in tool handling and logging within the AgentBot system, and general codebase optimizations for better maintainability and performance.</p>"},{"location":"releases/v0.12.5/#new-features","title":"New Features","text":"<ul> <li>Enhanced UI component guidelines with detailed instructions on using Jinja2 macros for consistency and reusability. This update facilitates the use of shared UI components across different templates, ensuring a consistent and maintainable codebase. (8eadbe) (Eric Ma)</li> <li>Introduced a new 'message_expansion' macro to encapsulate message display logic, replacing inline HTML and promoting code reuse across templates. (0dc239) (Eric Ma)</li> <li>Refactored AgentBot tool handling and iteration logic to simplify initialization and enhance the logging and message serialization processes. A new tool call display feature was also added to the web UI for better visibility of operations. (c704cd) (Eric Ma)</li> <li>Added a new 'respond_to_user' tool in AgentBot to streamline responses to users, ensuring immediate and accurate replies. This feature also includes updates to tool handling and logging to improve the overall functionality and reliability of the system. (24ec8a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None listed in this release.</li> </ul>"},{"location":"releases/v0.12.5/#deprecations","title":"Deprecations","text":"<ul> <li>None listed in this release.</li> </ul>"},{"location":"releases/v0.12.6/","title":"V0.12.6","text":""},{"location":"releases/v0.12.6/#version-0126","title":"Version 0.12.6","text":"<p>This release includes several enhancements and new features aimed at improving the functionality and user experience of our software. Key updates include refinements in agent setup, benchmarking tools, and significant improvements in our logging and UI components.</p>"},{"location":"releases/v0.12.6/#new-features","title":"New Features","text":"<ul> <li>Introduced a new HTML endpoint for rendering the main experiment view page, enhancing user interaction with experiment data. (47c4be4) (Eric Ma)</li> <li>Added a main prompt comparison page endpoint to facilitate easier access and comparison of different prompts. (be39a28) (Eric Ma)</li> <li>Implemented detailed run metadata tracking for AgentBot, enhancing monitoring and debugging capabilities. (98c9694) (Eric Ma)</li> <li>Added run metadata tracking for StructuredBot to monitor validation attempts and outcomes, improving reliability and performance insights. (5f57e63) (Eric Ma)</li> <li>Enhanced QueryBot with run metadata tracking to provide detailed insights into query performance and efficiency. (3849731) (Eric Ma)</li> <li>Introduced run metadata tracking for SimpleBot, allowing detailed monitoring of call metrics and tool usage. (d6418dc) (Eric Ma)</li> <li>Added new tools and metrics for agent performance measurement in agents.py, improving the setup and benchmarking process. (2a49bde) (Eric Ma)</li> <li>Updated agentbot tutorial to include the use of the gpt-4.1 model and introduced a section on using the planner bot. (d1f48ab) (Eric Ma)</li> <li>Added column visibility controls for metrics in the experiment details view, enhancing user interface flexibility. (0b49f33) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No specific bug fixes were noted in this release.</li> </ul>"},{"location":"releases/v0.12.6/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were introduced in this release.</li> </ul>"},{"location":"releases/v0.12.7/","title":"V0.12.7","text":""},{"location":"releases/v0.12.7/#version-0127","title":"Version 0.12.7","text":"<p>This release introduces new features enhancing documentation and example interfaces, improves the configurability of embedding models in LanceDBDocStore, and includes several bug fixes and refinements in documentation and codebase.</p>"},{"location":"releases/v0.12.7/#new-features","title":"New Features","text":"<ul> <li>Added a new example script demonstrating how to build an interactive chat interface using LlamaBot and Marimo, complete with a grid layout JSON file for the UI. (133f12e) (Eric Ma)</li> <li>Introduced configurable embedding model settings in LanceDBDocStore, allowing users to specify embedding registries and models, enhancing customization. (3356016) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the initialization of the Vector field in LanceDBDocStore to use the correct dimensions, ensuring proper setup. (3595285) (Eric Ma)</li> <li>Updated the embedding function in LanceDBDocStore to use 'minishlab/potion-base-8M' for improved performance and compatibility. (56532ac) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.7/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Removed outdated note about GPT4 authorship from the QueryBot tutorial, ensuring clarity and accuracy in documentation. (a00798a) (Eric Ma)</li> <li>Updated the QueryBot tutorial to recommend explicit document store and chat memory management, replacing GPT-4 specific instructions with more general LLM usage and introducing LanceDBDocStore for better management. (046286c) (Eric Ma)</li> <li>Added comprehensive documentation for the new LlamaBot documentation chat interface example, including setup and usage instructions. (133f12e) (Eric Ma)</li> <li>Enhanced the QueryBot tutorial to demonstrate how to configure embedding model settings in LanceDBDocStore, complete with default settings and customization options. (3356016) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.7/#refinements","title":"Refinements","text":"<ul> <li>Simplified the embedding function initialization in LanceDBDocStore by removing redundant imports and optimizing the creation process. (2db2f46) (Eric Ma)</li> <li>Commented out unused hypothesis strategies import and related unused code in test_docstore.py, cleaning up the test suite. (bef8924) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.8/","title":"V0.12.8","text":""},{"location":"releases/v0.12.8/#version-0128","title":"Version 0.12.8","text":"<p>This release introduces new dependencies and enhancements in the document store functionalities, improving performance and relevance of search results.</p>"},{"location":"releases/v0.12.8/#new-features","title":"New Features","text":"<ul> <li>Added the rerankers package version 0.10.0 as a new dependency to enhance functionality across multiple environments and platforms. This update also necessitated updates to other dependencies. (5323b31) (Eric Ma)</li> <li>Replaced RRFReranker with ColbertReranker in LanceDBDocStore for improved document retrieval and added a new example notebook to demonstrate its usage. This change enhances the document retrieval capabilities by utilizing a more efficient reranker. (2f4a607) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.8/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored the document store component by replacing FTS index creation with table optimization for better performance and added reranking to the retrieve method to improve search result relevance. (77d1536) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.8/#dependencies","title":"Dependencies","text":"<ul> <li>Updated llamabot package hash due to changes in dependencies, ensuring compatibility and stability with the new rerankers package. (5323b31) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.9/","title":"V0.12.9","text":""},{"location":"releases/v0.12.9/#version-0129","title":"Version 0.12.9","text":"<p>This release includes a refactor to enhance flexibility in configuration, along with dependency updates to improve stability and security.</p>"},{"location":"releases/v0.12.9/#new-features","title":"New Features","text":"<ul> <li>Refactored <code>summarize_web_results</code> in <code>llamabot/components/tools.py</code> to utilize an environment variable for setting the default model name, enhancing configurability and reducing hard-coded values. (aa3ad4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.12.9/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.12.9/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.12.9/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated <code>prefix-dev/setup-pixi</code> from version 0.8.8 to 0.8.10 to incorporate the latest patches and improvements. This update includes minor enhancements and bug fixes as detailed in the dependency's release notes. (0d566f) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.13.0/","title":"V0.13.0","text":""},{"location":"releases/v0.13.0/#version-0130","title":"Version 0.13.0","text":"<p>This release introduces significant enhancements to the chat memory system, including new features for graph-based retrieval and improved node selection logic. Additionally, there are updates to testing frameworks, documentation improvements, and dependency updates.</p>"},{"location":"releases/v0.13.0/#new-features","title":"New Features","text":"<ul> <li>Introduced a unified chat memory system that supports both linear and graph-based threading, complete with message summarization and node selection strategies. (ed4589b) (Eric Ma)</li> <li>Added graph-aware retrieval capabilities to the chat memory system, allowing for context-sensitive message retrieval based on conversation threading. (1a994e9) (Eric Ma)</li> <li>Enhanced the llamabot components to expose user, assistant, system, and dev roles directly from the messages module, facilitating easier component integration. (e6b8a9d) (Eric Ma)</li> <li>Implemented a new assistant function in llamabot components to create AIMessages from multiple content types, improving message handling flexibility. (6b8a551) (Eric Ma)</li> <li>Updated the example script to align with the new marimo version and included a clarifying bot message about the necessity of tests, enhancing the documentation's clarity and relevance. (6d8843c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected the order of retrieved messages in the chat memory's linear retrieval test to match the expected sequence. (b898a63) (Eric Ma)</li> <li>Fixed the LinearNodeSelector to correctly select the last assistant node instead of the leaf node, aligning node selection with expected behavior. (b5941bc) (Eric Ma)</li> <li>Addressed an issue in SimpleBot's test initialization by fixing the parameter name mismatch in the constructor. (453c1f3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.0/#deprecations","title":"Deprecations","text":"<ul> <li>Removed outdated development container documentation, streamlining the project's documentation set. (9a3d17a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.1/","title":"V0.13.1","text":""},{"location":"releases/v0.13.1/#version-0131","title":"Version 0.13.1","text":"<p>This release introduces the new ToolBot agent for single-turn tool execution, major improvements to code execution tooling, enhanced error handling, and significant refactoring for clarity and maintainability. Several dependency and workflow updates are also included, along with targeted bug fixes and test improvements.</p>"},{"location":"releases/v0.13.1/#new-features","title":"New Features","text":"<ul> <li>Added ToolBot agent for single-turn tool selection and execution, supporting core tools and optional chat memory integration. ToolBot processes user messages, selects the appropriate tool, and executes it in a single turn. (21f3c2, Eric Ma)</li> <li>Implemented a detailed system prompt for ToolBot, guiding tool selection, argument extraction, and code generation best practices. (7b15fa, Eric Ma)</li> <li>Introduced the write_and_execute_code tool, which dynamically parses, compiles, and executes user-defined Python functions with arguments in a sandboxed environment. (bb392b, Eric Ma)</li> <li>Enhanced prompt generation in notebooks to include global variables, callable functions, dataframes, and other variables for improved context in user requests. (d0a7e8, Eric Ma)</li> <li>Added example data generation and department info setup for testing and demonstration purposes in agent notebooks. (d0a7e8, Eric Ma)</li> </ul>"},{"location":"releases/v0.13.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed argument name in ToolBot initialization for the write_and_execute_code tool, ensuring correct function signature usage. (70f597, Eric Ma)</li> <li>Removed the token argument from the uv publish command in the release workflow, as it is no longer required. (868cfe, Eric Ma)</li> <li>Fixed ToolBot test setup by passing an explicit globals dictionary to the write_and_execute_code tool, ensuring consistent and isolated test environments. (a4ff16, Eric Ma)</li> <li>Marked LanceDB-related tests as expected to fail due to file system issues, improving test reliability. (4ef7f9, Eric Ma)</li> </ul>"},{"location":"releases/v0.13.1/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the entire Zotero CLI and related components, including CLI commands, prompt library, completer, library wrappers, utilities, and tests. Also removed pyzotero and related dependencies from the project. (5981b0, Eric Ma)</li> <li>Removed devcontainer configuration files and related GitHub workflows, discontinuing support for development container setup. (713552, Eric Ma)</li> </ul> <p>Additional Improvements:</p> <ul> <li>Refactored ToolBot and write_and_execute_code for improved clarity, error handling, and maintainability. (bb392b, a31344, 490fcb, ca610c, 7b15fa, d0a7e8, Eric Ma)</li> <li>Updated dependencies in notebooks, including upgrades to llamabot, marimo, and the addition of matplotlib, seaborn, and others. (d0a7e8, fb3617, Eric Ma)</li> <li>Improved GitHub Actions workflows for Python package release and code review, including permissions and trigger simplifications. (53f466, 5d6965, fc1d2b, 4fda1a, Eric Ma)</li> <li>Enhanced and updated documentation and tutorials for new agent features, usage, and best practices. (90f057, 0ad2f3, eefb61, 9b678a, Eric Ma)</li> <li>Cleaned up codebase by removing unused code, commented-out docstrings, and trailing whitespace. (ca610c, 623baa, 366ceb, fb3617, Eric Ma)</li> <li>Updated and improved test coverage for ToolBot and related components, including model name updates and tool naming consistency. (09864d, 4b9b14, a4ff16, 4ef7f9, Eric Ma)</li> <li>Updated third-party dependencies via Dependabot for improved security and compatibility. (baff52, 311371, 876ea5, Dependabot)</li> </ul> <p>This release lays the foundation for more flexible, tool-driven agents and improves the developer experience with better error handling, testing, and documentation.</p>"},{"location":"releases/v0.13.10/","title":"V0.13.10","text":""},{"location":"releases/v0.13.10/#version-01310","title":"Version 0.13.10","text":"<p>This release introduces enhancements in documentation, testing, and CI/CD workflows, along with the addition of new server capabilities for LlamaBot documentation.</p>"},{"location":"releases/v0.13.10/#new-features","title":"New Features","text":"<ul> <li>Introduced a new MCP server for LlamaBot documentation, which includes a FastMCP server with a docs_search tool, pre-built LanceDB database, and new CLI commands for building and launching the server. This feature also integrates MCP database build into the CI/CD workflow and supports a specific installation pattern. (ecad5ba) (Eric Ma)</li> <li>Added comprehensive documentation guidelines and development tools instructions, specifying the transition from Jupyter to Marimo notebooks, introducing markdown linting, and outlining the documentation structure based on the Di\u00e1taxis framework. This update also includes development setup instructions using pixi and content guidelines to streamline documentation efforts. (7fc6dbb) (Eric Ma)</li> <li>Added a detailed StructuredBot tutorial and improved documentation for various bots and tools, enhancing the understanding and usability of these components. This includes updates to AGENTS.md and README.md, along with fixes to typos and formatting across multiple documentation files. (173afc4) (Eric Ma)</li> <li>Improved the test coverage for the docstore build in the CLI by adding mocks for external dependencies and verifying the creation and appending of documents in the docstore. (81ce01c) (Eric Ma)</li> <li>Enhanced the test_build_docstore function to use Python's tempfile.TemporaryDirectory for better handling of temporary directories and files, ensuring automatic cleanup and reducing manual maintenance. (892c1db) (Eric Ma)</li> <li>Updated the .gitignore file to include the LanceDB documentation database directory, preventing unnecessary tracking of files created during CI/CD processes. (eba1499) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.13.10/#deprecations","title":"Deprecations","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.13.11/","title":"V0.13.11","text":""},{"location":"releases/v0.13.11/#version-01311","title":"Version 0.13.11","text":"<p>This release includes improvements to the CI workflow for building Python packages, specifically integrating the MCP documentation database into the build process and optimizing the build steps.</p>"},{"location":"releases/v0.13.11/#new-features","title":"New Features","text":"<ul> <li>Integrated the MCP documentation database into the Python package build process, ensuring that the database files are included in the final package. This change includes a new verification step to check for the existence of database files before the package is rebuilt. (f7293d0) (Eric Ma)</li> <li>Refactored the release-python-package workflow to build the Python package only once after the MCP database is built, removing redundant build steps and updating the documentation to reflect this change. (daeb731) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.11/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.13.11/#deprecations","title":"Deprecations","text":""},{"location":"releases/v0.13.2/","title":"V0.13.2","text":""},{"location":"releases/v0.13.2/#version-0132","title":"Version 0.13.2","text":"<p>This minor release includes refinements in code naming and logging, alongside routine version updates.</p>"},{"location":"releases/v0.13.2/#new-features","title":"New Features","text":"<ul> <li>Renamed 'kwargs' to 'keyword_args' in the <code>write_and_execute_code_wrapper</code> function to enhance code readability and maintainability. (3ef8f7) (Eric Ma)</li> <li>Replaced print statements with <code>logger.debug</code> in <code>notebooks/agents.py</code> to improve the logging system. (3ef8f7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected the version number in the release notes to align with the actual software version. (e41ef27) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.2/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul> <p>This version continues to refine the project's codebase and documentation, ensuring greater clarity and control in logging functionalities.</p>"},{"location":"releases/v0.13.3/","title":"V0.13.3","text":""},{"location":"releases/v0.13.3/#version-0133","title":"Version 0.13.3","text":"<p>This release includes an update to the llamabot dependency, enhancing stability and performance.</p>"},{"location":"releases/v0.13.3/#new-features","title":"New Features","text":"<ul> <li>Updated llamabot dependency from version 0.13.1 to 0.13.2, ensuring compatibility with the latest features and improvements (d7d326) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.13.3/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.13.4/","title":"V0.13.4","text":""},{"location":"releases/v0.13.4/#version-0134","title":"Version 0.13.4","text":"<p>This release includes significant improvements in docstring parsing and function metadata extraction, enhancing performance and compatibility with various docstring styles.</p>"},{"location":"releases/v0.13.4/#new-features","title":"New Features","text":"<ul> <li>Introduced <code>function_to_dict</code> and <code>parse_docstring</code> utilities to enhance function metadata extraction, supporting numpy, google, and sphinx-style docstrings. This feature allows for better integration with OpenAI function calling by using docstring parsing and type annotations. (609097) (Eric Ma)</li> <li>Added comprehensive tests for the new <code>parse_docstring</code> utility to ensure graceful handling of malformed and long descriptions across different docstring styles. (7cf220) (Eric Ma)</li> <li>Refactored docstring parsing in the <code>llamabot.components.tools</code> to use pre-compiled regex patterns, improving performance and readability while enhancing parameter parsing flexibility. (68fd2f) (Eric Ma)</li> <li>Major refactor of docstring parsing to utilize the <code>docstring-parser</code> library, replacing custom regex-based parsing. This update simplifies the type extraction logic and improves handling of optional parameters, ensuring better compatibility and maintainability. (ac93d5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.13.4/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.13.5/","title":"V0.13.5","text":""},{"location":"releases/v0.13.5/#version-0135","title":"Version 0.13.5","text":"<p>This release includes enhancements to developer tools and general maintenance updates.</p>"},{"location":"releases/v0.13.5/#new-features","title":"New Features","text":"<ul> <li>Enhanced the <code>function_to_dict</code> tool to merge short and long docstring descriptions into a single output, improving documentation clarity. This update also includes a new example notebook demonstrating the function's application in compound interest calculations, along with additional tests to ensure functionality. (14b3bf5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.13.5/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.13.6/","title":"V0.13.6","text":""},{"location":"releases/v0.13.6/#version-0136","title":"Version 0.13.6","text":"<p>This release introduces enhancements to tool serialization and documentation, along with the addition of new computational tools in the notebooks.</p>"},{"location":"releases/v0.13.6/#new-features","title":"New Features","text":"<ul> <li>Added default parameter values to function schema serialization to enhance clarity and usability in tool interactions. This update includes improvements to documentation and handling of keyword arguments in function execution wrappers. (1d4b338) (Eric Ma)</li> <li>Enhanced the agentbot tutorial to include comprehensive parsing and utilization of various docstring styles, improving the self-documentation capabilities of tools. This update makes it easier to understand and use LlamaBot tools effectively. (adbc9fc) (Eric Ma)</li> <li>Introduced a new compound interest calculation tool in the notebooks, which is exposed through the llamabot module. This feature includes the ability to access the JSON schema of the tool, facilitating its use in financial computations. (5ca44f3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None listed in this release.</li> </ul>"},{"location":"releases/v0.13.6/#deprecations","title":"Deprecations","text":"<ul> <li>None listed in this release.</li> </ul>"},{"location":"releases/v0.13.7/","title":"V0.13.7","text":""},{"location":"releases/v0.13.7/#version-0137","title":"Version 0.13.7","text":"<p>This release introduces several enhancements to the ToolBot and context engineering functionalities, improving error handling, callable support, and data analysis capabilities. Additionally, the release includes various refactoring efforts to streamline code and improve clarity.</p>"},{"location":"releases/v0.13.7/#new-features","title":"New Features","text":"<ul> <li>Added callable support to ToolBot, allowing dynamic message generation and enhanced output handling for the write_and_execute_code tool. This update also includes comprehensive tests and documentation for new features. (9392bd) (Eric Ma)</li> <li>Introduced convenience functions in the context_engineering module to describe DataFrames and nested data structures within globals, providing detailed column analysis and support for recursive traversal. (3eaee0) (Eric Ma)</li> <li>Launched the ToolBot module with detailed documentation, a tutorial, and an example script for single-turn tool execution and function calling, integrated into the llamabot package. (408699) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>There are no specific bug fixes listed in this release.</li> </ul>"},{"location":"releases/v0.13.7/#deprecations","title":"Deprecations","text":"<ul> <li>There are no deprecations in this release.</li> </ul>"},{"location":"releases/v0.13.8/","title":"V0.13.8","text":""},{"location":"releases/v0.13.8/#version-0138","title":"Version 0.13.8","text":"<p>This release includes new features enhancing data handling and integration capabilities, along with critical bug fixes improving the robustness of error handling in code execution tools.</p>"},{"location":"releases/v0.13.8/#new-features","title":"New Features","text":"<ul> <li>Added a new dataset containing IC50 values with additional confounders such as instrument, operator, temperature, pH, and passage number for enhanced data analysis in notebooks. (b84edb) (Eric Ma)</li> <li>Introduced CSV file upload and processing capabilities, along with API key input for enhanced ToolBot integration, providing a more robust and user-friendly interface for data manipulation and configuration. (1ffe62) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Modified error handling in <code>llamabot.components.tools</code> to raise exceptions directly instead of returning error strings, ensuring clearer error reporting and compliance with best practices in error management. (ce082b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.8/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this version.</li> </ul>"},{"location":"releases/v0.13.9/","title":"V0.13.9","text":""},{"location":"releases/v0.13.9/#version-0139","title":"Version 0.13.9","text":"<p>This release includes documentation improvements, a critical bug fix, and updates to our development workflows to enhance the consistency and reliability of our environment.</p>"},{"location":"releases/v0.13.9/#new-features","title":"New Features","text":"<ul> <li>Added documentation for Marimo notebooks and introduced a validation command to ensure notebook integrity. (16f5b3) (Eric Ma)</li> <li>Updated testing instructions in AGENTS.md to emphasize the use of <code>pixi run</code> for maintaining environment consistency. (5fb19d) (Eric Ma)</li> <li>Generalized guidance for LLM agents by renaming CLAUDE.md to AGENTS.md and updating its content. (883bbc) (Eric Ma)</li> <li>Removed specific GitHub Actions workflows for Claude to streamline our automation processes. (43ac39) (Eric Ma)</li> <li>Updated the Ollama models list to keep our models up-to-date with the latest changes. (a4690f) (github-actions[bot])</li> <li>Enhanced the <code>update-ollama-models</code> GitHub workflow by adding write permissions for managing contents and pull requests. (07cc6a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.9/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Removed a redundant <code>logger.remove()</code> call to prevent potential logging issues on Windows platforms. (46a4d9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.13.9/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated the <code>prefix-dev/setup-pixi</code> dependency from 0.9.0 to 0.9.1 to incorporate the latest improvements and bug fixes. (f44523) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.14.0/","title":"V0.14.0","text":""},{"location":"releases/v0.14.0/#version-0140","title":"Version 0.14.0","text":"<p>This release introduces significant enhancements to the AgentBot with the implementation of the ReAct pattern, improves test reliability, and updates dependencies.</p>"},{"location":"releases/v0.14.0/#new-features","title":"New Features","text":"<ul> <li>Implemented the ReAct pattern for AgentBot, replacing the previous logic with a structured reasoning and action framework, enhancing transparency and decision-making processes. (470b1a2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.14.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated tests to use the local Llama 3.2 model, simplifying assertions and removing complex mocking that was causing failures. (551251f) (Eric Ma)</li> <li>Mocked ToolBot LLM calls in AgentBot tests to ensure tests run fast and reliably without external dependencies. (424d7e9) (Eric Ma)</li> <li>Further refined mocking strategies in AgentBot tests to ensure all tests pass without requiring real LLM calls, improving test coverage for the ReAct pattern. (dddbe4c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.14.0/#deprecations","title":"Deprecations","text":"<ul> <li>Complete replacement of the previous AgentBot implementation with the new ReAct pattern, removing the planner_bot logic. (470b1a2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.0/","title":"V0.15.0","text":""},{"location":"releases/v0.15.0/#version-0150","title":"Version 0.15.0","text":"<p>This release includes significant refactoring, removing outdated dependencies and modules to streamline the project and improve maintainability.</p>"},{"location":"releases/v0.15.0/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this release.</li> </ul>"},{"location":"releases/v0.15.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No specific bug fixes were mentioned in this release.</li> </ul>"},{"location":"releases/v0.15.0/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the entire Python CLI module and its registration, simplifying the project structure (68ece7) (Eric Ma)</li> <li>Eliminated all functions related to AST manipulation using the astor library from <code>code_manipulation.py</code>, reducing complexity (68ece7) (Eric Ma)</li> <li>Removed the astor library from dependency files, decreasing the number of third-party dependencies (68ece7) (Eric Ma)</li> <li>Deleted tests related to the <code>code_manipulation</code> module that depended on astor and the Python CLI, streamlining the test suite (68ece7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.1/","title":"V0.15.1","text":""},{"location":"releases/v0.15.1/#version-0151","title":"Version 0.15.1","text":"<p>This minor release includes important bug fixes and updates to maintain compatibility with the latest standards and configurations.</p>"},{"location":"releases/v0.15.1/#new-features","title":"New Features","text":"<ul> <li>Expanded CI/CD documentation and examples, including a two-phase build process and artifacts configuration. This update helps in understanding and implementing CI/CD processes more effectively. (b959a5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed a deprecation warning in Pixi by updating the configuration from <code>project</code> to <code>workspace</code> in <code>pyproject.toml</code>, ensuring compatibility with the latest Pixi standards without altering existing functionalities. (3da1e48) (Eric Ma)</li> <li>Resolved a critical TypeError in the LanceDB embedding function by adding a <code>trust_remote_code=True</code> parameter, which is necessary for using the minishlab/potion-base-8M model with StaticEmbedding. This fix addresses a CI/CD build failure in MCP database generation. (b28b59) (Eric Ma)</li> <li>Modified the MCP database collection logic to exclude release notes, reducing database pollution and streamlining the data collection process. (b959a5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.1/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.15.2/","title":"V0.15.2","text":""},{"location":"releases/v0.15.2/#version-0152","title":"Version 0.15.2","text":"<p>This release includes important bug fixes to enhance the functionality and reliability of the release notes generation process.</p>"},{"location":"releases/v0.15.2/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.15.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Improved the release notes generation to correctly use the newest tag for the filename and ensure accurate commit log ranges. This fix also includes error handling for scenarios where no tags are found, ensuring that the version bump has been executed before generating release notes. (9a248b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.2/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were introduced in this version.</li> </ul>"},{"location":"releases/v0.15.3/","title":"V0.15.3","text":""},{"location":"releases/v0.15.3/#version-0153","title":"Version 0.15.3","text":"<p>This release includes minor updates and maintenance improvements.</p>"},{"location":"releases/v0.15.3/#new-features","title":"New Features","text":"<ul> <li>Added release notes for the previous version 0.15.2 (967edf) (github-actions)</li> </ul>"},{"location":"releases/v0.15.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.15.3/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul> <p>Please note that the commit log provided does not detail specific new features, bug fixes, or deprecations beyond the addition of release notes. For a more detailed list of changes, please refer to the full commit history or the detailed release notes for version 0.15.2.</p>"},{"location":"releases/v0.15.5/","title":"V0.15.5","text":""},{"location":"releases/v0.15.5/#version-0155","title":"Version 0.15.5","text":"<p>This release includes minor version bumps, dependency updates, and significant internal improvements to enhance performance and maintainability.</p>"},{"location":"releases/v0.15.5/#new-features","title":"New Features","text":"<ul> <li>Updated project versioning and documentation to reflect new release 0.15.4 (fd4b8e) (Eric Ma)</li> <li>Removed Tantivy dependency, transitioning to LanceDB's native full-text search capabilities, ensuring better integration and performance (44f54f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Replaced deprecated <code>SDPMChunker</code> with <code>SemanticChunker</code> in <code>doc_processor.py</code>, updating the parameter for better text chunking (d55328) (Eric Ma)</li> </ul>"},{"location":"releases/v0.15.5/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Bumped <code>prefix-dev/setup-pixi</code> from 0.9.1 to 0.9.2 to incorporate the latest fixes and enhancements (5cba8e) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.16.0/","title":"V0.16.0","text":""},{"location":"releases/v0.16.0/#version-0160","title":"Version 0.16.0","text":"<p>This release introduces significant enhancements to the message handling and memory integration in AgentBot, improving its reasoning and observation capabilities.</p>"},{"location":"releases/v0.16.0/#new-features","title":"New Features","text":"<ul> <li>Added memory integration and automatic response handling in AgentBot, allowing for dynamic conversation memory updates and responses based on stored thoughts and observations. (41a9fe9) (Eric Ma)</li> <li>Introduced new message types, ThoughtMessage and ObservationMessage, to represent agent's reasoning and observations, enhancing message handling in AgentBot and memory systems. (e607aea) (Eric Ma)</li> <li>Added thought() and observation() factory functions in components to facilitate the creation of ThoughtMessage and ObservationMessage instances, streamlining their usage in the system. (4efd093) (Eric Ma)</li> </ul>"},{"location":"releases/v0.16.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.16.0/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.16.1/","title":"V0.16.1","text":""},{"location":"releases/v0.16.1/#version-0161","title":"Version 0.16.1","text":"<p>This release introduces significant enhancements to the AgentBot, including caching, execution history tracking, and performance metrics, along with minor version updates.</p>"},{"location":"releases/v0.16.1/#new-features","title":"New Features","text":"<ul> <li>Added caching for tool calls in AgentBot to reduce redundant executions and enhance efficiency. This feature also includes tracking of execution history and performance metrics like cache hit rates and tool usage statistics. (3afdf1) (Eric Ma)</li> <li>Enhanced the ToolBot to utilize execution history for better tool selection, and updated response mechanisms to allow for custom tool choices. (3afdf1) (Eric Ma)</li> <li>Improved documentation to include examples of caching, validation, execution history, and metrics usage. Also, added tests for the new caching infrastructure and metrics. (3afdf1) (Eric Ma)</li> <li>Fixed chat memory visualization by sanitizing double quotes in message content for better compatibility with Mermaid diagrams. Also, refined error handling in statistical experiment design extraction. (3afdf1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.16.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No specific bug fixes noted in this release.</li> </ul>"},{"location":"releases/v0.16.1/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.16.2/","title":"V0.16.2","text":""},{"location":"releases/v0.16.2/#version-0162","title":"Version 0.16.2","text":"<p>This release includes significant enhancements to the AgentBot, expanded testing capabilities, and updates to the continuous integration workflows.</p>"},{"location":"releases/v0.16.2/#new-features","title":"New Features","text":"<ul> <li>Refactored AgentBot to utilize native tool calls within a single ReAct loop, eliminating the dependency on ToolBot. This update simplifies the internal architecture and improves error handling and tool usage statistics. (61c9920) (Eric Ma)</li> <li>Added new tests for AgentBot integrating the qwen2.5 model with shell tool functionality, ensuring robustness and reliability in real-world scenarios. (68170af) (Eric Ma)</li> <li>Expanded the test matrix to include Python 3.13 in continuous integration and release workflows, ensuring compatibility with the latest Python version. (3ad0cced) (Cursor Agent)</li> </ul>"},{"location":"releases/v0.16.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None listed in this release.</li> </ul>"},{"location":"releases/v0.16.2/#deprecations","title":"Deprecations","text":"<ul> <li>Deprecated the use of .cursor command documentation and ToolBot in favor of streamlined AgentBot operations. (61c9920) (Eric Ma)</li> </ul>"},{"location":"releases/v0.16.3/","title":"V0.16.3","text":""},{"location":"releases/v0.16.3/#version-0163","title":"Version 0.16.3","text":"<p>This release includes enhancements to the document storage capabilities, updates to dependencies, and several bug fixes and refactorings aimed at improving the usability and functionality of our software.</p>"},{"location":"releases/v0.16.3/#new-features","title":"New Features","text":"<ul> <li>Added partitioning support to LanceDBDocStore, allowing documents to be grouped logically by criteria such as source or topic. This feature includes methods for managing and querying partitions. (c0dea30) (github-actions[bot])</li> <li>Introduced a new PocketFlow minimalist LLM framework notebook, showcasing the usage of core PocketFlow classes and example nodes for various tasks like summarization and topic extraction. (9a11b7d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.16.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an import error in Jupyter notebooks that occurred when displaying images by moving IPython imports inside the Jupyter environment check. (4b546dc) (Eric Ma)</li> <li>Corrected the version string in agentbot_build.py to reflect the accurate generation version. (85a87e8) (Eric Ma)</li> </ul>"},{"location":"releases/v0.16.3/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored function signatures and return values in <code>notebooks/pocketflow_testdrive.py</code> for clarity and consistency, including changes to return values and function parameters. (a8b79e3) (Eric Ma)</li> <li>Improved PocketFlow visualization and testdrive, including moving visualization components and updating flow structures. (4929a9f, 59487f5, 087b265, 2485cb5) (Cursor Agent)</li> <li>Removed legacy PocketFlow implementation and updated imports and model names in the notebook. (c2e30bf) (Eric Ma)</li> </ul>"},{"location":"releases/v0.16.3/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Bumped prefix-dev/setup-pixi from 0.9.2 to 0.9.3, ensuring compatibility and security with the latest versions. (28f7590) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.16.3/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Updated the Ollama models list to include the latest models available for use. (14f063c) (github-actions[bot])</li> </ul>"},{"location":"releases/v0.17.0/","title":"V0.17.0","text":""},{"location":"releases/v0.17.0/#version-0170","title":"Version 0.17.0","text":"<p>This release introduces a significant architectural change with the integration of PocketFlow, enhancing the AgentBot's capabilities and flexibility. It also includes various bug fixes and documentation updates to align with the new system.</p>"},{"location":"releases/v0.17.0/#new-features","title":"New Features","text":"<ul> <li>Replaced AgentBot with a new PocketFlow-based implementation for improved graph orchestration and tool wrapping. (72c91d) (Eric Ma)</li> <li>Updated the PocketFlow testdrive notebook with better flow visualization, tool integration, and agent design. (3085cd) (Eric Ma)</li> <li>Restored default tools in ToolBot, ensuring 'today_date' and 'respond_to_user' are included by default. (2f2e6d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed failing AgentBot tests to handle type mismatches and import errors correctly. (ea490a) (Eric Ma)</li> <li>Corrected end of file formatting issues to adhere to project standards. (2d6c29) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.0/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Overhauled AgentBot documentation to reflect the new PocketFlow-based implementation and removed outdated references. (afd6c5) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.1/","title":"V0.17.1","text":""},{"location":"releases/v0.17.1/#version-0171","title":"Version 0.17.1","text":"<p>This release includes important bug fixes to enhance the stability and performance of the AgentBot.</p>"},{"location":"releases/v0.17.1/#new-features","title":"New Features","text":"<ul> <li>No new features were added in this version.</li> </ul>"},{"location":"releases/v0.17.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue where the AgentBot's memory state was incorrectly reset, leading to loss of shared state across calls. (91b0c9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.1/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this version.</li> </ul>"},{"location":"releases/v0.17.10/","title":"Version 0.17.10","text":"<p>This release includes important dependency updates, enhancements to release notes formatting, and maintenance updates to our models list.</p>"},{"location":"releases/v0.17.10/#new-features","title":"New Features","text":"<ul> <li>Updated the release notes to use H1 headings for better markdown compliance (2e31f0b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No specific bug fixes in this release.</li> </ul>"},{"location":"releases/v0.17.10/#maintenance","title":"Maintenance","text":"<ul> <li>Updated the Ollama models list to keep our data up-to-date (ffd62a0) (github-actions[bot])</li> <li>Bumped <code>peter-evans/create-pull-request</code> from version 7 to 8, ensuring compatibility and security (d3532a7) (dependabot[bot])</li> <li>Bumped <code>actions/cache</code> from version 4 to 5, improving caching efficiency and reliability (1d1af48) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.17.10/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.17.11/","title":"Version 0.17.11","text":"<p>This release includes minor documentation improvements to enhance readability and maintainability.</p>"},{"location":"releases/v0.17.11/#new-features","title":"New Features","text":"<ul> <li>Added release notes for the previous version to document changes and updates (b2c3b2) (github-actions)</li> </ul>"},{"location":"releases/v0.17.11/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed section headings in documentation to use H2 instead of H3 for better hierarchy and readability (a4595a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.11/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.17.13/","title":"Version v0.17.13","text":"<p>This release brings major improvements to release note generation, version handling, and CI/CD automation. Users now have explicit control over version strings, improved workflow reliability, and enhanced flexibility in versioning conventions. Several bug fixes and workflow enhancements ensure smoother releases and better developer experience.</p>"},{"location":"releases/v0.17.13/#new-features","title":"New Features","text":"<ul> <li>Added a <code>--version</code> flag to the <code>llamabot git write-release-notes</code> CLI command, allowing users to explicitly specify the version number for release notes generation. This enables cleaner CI/CD workflows and prevents LLMs from inferring incorrect version numbers from commit messages. (9ba26e, Eric Ma)</li> <li>Introduced automatic patch release on PR merge to the main branch, enabling seamless patch version bumps and releases when PRs are merged. Manual workflows for major/minor releases are preserved. (815290, Eric Ma)</li> </ul>"},{"location":"releases/v0.17.13/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Improved version name handling and event type checks in workflows, ensuring correct behavior across different GitHub Actions event types and preventing issues with missing or incorrect version names. (21f86a, Eric Ma)</li> <li>Fixed trailing whitespace in workflow files to comply with pre-commit hooks. (018306, Eric Ma)</li> <li>Enhanced workflow reliability by ensuring tests pass before automatic release, switching to a <code>workflow_run</code> trigger that depends on successful test completion. (feaf71, Eric Ma)</li> <li>Improved handling of concurrent PR merges in the release workflow by adding a git fetch and rebase step before pushing, preventing non-fast-forward push errors. (84422f, Eric Ma)</li> <li>Normalized version strings and handled edge cases, including stripping the 'v' prefix for consistency, validating empty/whitespace-only version strings, and improving test coverage for these scenarios. (2486dd, Eric Ma)</li> </ul>"},{"location":"releases/v0.17.13/#deprecations","title":"Deprecations","text":"<ul> <li>Removed all logic that enforced a specific version string format (such as stripping the 'v' prefix), allowing users to control their own versioning conventions. The version string is now used exactly as provided by the user or from git tags. (62543d, Eric Ma)</li> <li>Replaced <code>bump2version</code> with <code>uv version</code> for version management, consolidating version information in <code>pyproject.toml</code> and simplifying the version bumping process. (620eff, Eric Ma)</li> </ul>"},{"location":"releases/v0.17.13/#internal-improvements","title":"Internal Improvements","text":"<ul> <li>Refactored commit range and version determination logic in release note generation to eliminate code duplication and improve maintainability. (98837c, Eric Ma)</li> <li>Added <code>.lance</code> files and ACE implementation files to <code>.gitignore</code> to prevent accidental commits of experimental or database files. (f7914c, c21e66, Eric Ma)</li> </ul>"},{"location":"releases/v0.17.13/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Bumped <code>rossjrw/pr-preview-action</code> from 1.6.3 to 1.8.0 in two steps, keeping CI/CD preview actions up to date. (288128, 2d0af2, dependabot[bot])</li> </ul>"},{"location":"releases/v0.17.14/","title":"Version v0.17.14","text":"<p>This release marks a minor version bump to v0.17.14. It includes routine maintenance and preparation for upcoming features.</p>"},{"location":"releases/v0.17.14/#new-features","title":"New Features","text":"<ul> <li>Version update to 0.17.14 for system consistency and future updates (c475320) (github-actions)</li> </ul>"},{"location":"releases/v0.17.14/#bug-fixes","title":"Bug Fixes","text":"<p>There are no bug fixes in this release.</p>"},{"location":"releases/v0.17.14/#deprecations","title":"Deprecations","text":"<p>There are no deprecations in this release.</p>"},{"location":"releases/v0.17.2/","title":"V0.17.2","text":""},{"location":"releases/v0.17.2/#version-0172","title":"Version 0.17.2","text":"<p>This release introduces several enhancements across different modules, improving functionality and ensuring better performance and validation.</p>"},{"location":"releases/v0.17.2/#new-features","title":"New Features","text":"<ul> <li>Added automatic and manual optimization features to LanceDBDocStore, which optimizes the database after a certain number of operations and allows for manual optimization triggers. (dfef865) (Eric Ma)</li> <li>Introduced validation for user-provided tools in AgentBot to ensure they are properly decorated with <code>@tool</code> and <code>@nodeify</code>, enhancing reliability and correctness. (22c07f9) (Eric Ma)</li> <li>Enhanced llamabot by supporting additional keyword arguments in completion functions and enforcing decoration requirements for tools, improving flexibility and integration capabilities. (1235244) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.2/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored the import of ToolBot in PocketFlow to avoid circular import issues, enhancing code maintainability and reliability. (dafd349) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.17.2/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul> <p>This version continues to focus on enhancing the user experience and robustness of the system through thoughtful features and necessary refactorings.</p>"},{"location":"releases/v0.17.3/","title":"V0.17.3","text":""},{"location":"releases/v0.17.3/#version-0173","title":"Version 0.17.3","text":"<p>This release includes several enhancements to visualization, error handling, and tool integration in the PocketFlow framework, improving usability and functionality.</p>"},{"location":"releases/v0.17.3/#new-features","title":"New Features","text":"<ul> <li>Enhanced code execution in PocketFlow with detailed error reporting, variable tracking, and self-healing guidance, improving debugging and user interaction. (0a4a9e) (Eric Ma)</li> <li>Added module detection and improved tool selection guidance in tools, helping users to make informed decisions about tool usage based on available libraries. (40a541) (Eric Ma)</li> <li>Introduced a new tool <code>return_object_to_user</code> in AgentBot and ToolBot, allowing for the return of Python objects from the calling context's global dictionary, enhancing modularity and reusability. (e9e801) (Eric Ma)</li> <li>Enhanced PocketFlow visualization with automatic graph direction detection and terminal node coloring, improving clarity and visual appeal. (2745b03) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed test cases in agentbot and toolbot to handle None categorized variables and incorrect import paths, ensuring more reliable testing. (47bd727) (Eric Ma)</li> <li>Corrected an unreachable <code>elif</code> block in <code>prompt_manager.py</code> and adjusted test cases to properly check for missing template variables, enhancing code reliability. (b5b9f50) (Eric Ma)</li> <li>Updated <code>test_prompt_with_missing_kwargs</code> to accurately check for missing template variables, ensuring that the test reflects expected functionality. (fc7bb5b) (Eric Ma)</li> <li>Added an explanatory comment to the exception handler in <code>categorize_globals</code>, clarifying the rationale behind ignoring certain exceptions, improving code maintainability. (af88ec0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.3/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored Docker container run command to create cache directory in a single shell command, simplifying setup in sandbox environments. (636df72) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.3/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.17.4/","title":"V0.17.4","text":""},{"location":"releases/v0.17.4/#version-0174","title":"Version 0.17.4","text":"<p>This release includes several enhancements and bug fixes improving the functionality and user experience across various modules, particularly focusing on span logging, bot behavior, and agent design.</p>"},{"location":"releases/v0.17.4/#new-features","title":"New Features","text":"<ul> <li>Introduced a span-based logging feature for LLM traces, enhancing traceability and debugging capabilities for bots. (1fd64e9) (Eric Ma)</li> <li>Added a new cell in the experiment_design_agent notebook to demonstrate a perfect prompt using markdown, enhancing user guidance and example clarity. (10a1540) (Eric Ma)</li> <li>Revamped the experiment design agent to function as an inquisitive consultation assistant, improving interaction quality and decision-making support. (be7b8f2) (Eric Ma)</li> <li>Implemented a two-step receipt processing method using OCR and structuring bots in the PyData Boston 2025 tutorial, optimizing data extraction and processing. (4fd99f9) (Eric Ma)</li> <li>Added the inspect_globals tool to Llamabot for better context awareness by listing available objects in globals. (51f6246) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Ensured bots always create a new trace_id for each call to prevent inheriting trace_ids from the context, addressing a critical bug in trace management. (92de1b8) (Eric Ma)</li> <li>Improved span visualization styling and layout to prevent overflow and enhance visual balance between panels. (d0c15e2) (Eric Ma)</li> <li>Fixed incorrect usage of the get_spans function in the span_logging_example notebook by changing attribute dictionary to keyword arguments. (d69a842) (Eric Ma)</li> <li>Addressed flaky tests related to AgentBot's max_iterations and improved test fixtures for better reliability and performance. (0d60c54) (Eric Ma)</li> <li>Corrected the parameter mismatch in test_agentbot by updating the mock function to use 'response' instead of 'message'. (2ff52c0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.4/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the View Span Tree section from the span logging example notebook as part of documentation updates to reflect current features. (1730d04) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.5/","title":"V0.17.5","text":""},{"location":"releases/v0.17.5/#version-0175","title":"Version 0.17.5","text":"<p>This release introduces the SpanList class for enhanced span visualization and management, along with improvements in automatic logging functionalities.</p>"},{"location":"releases/v0.17.5/#new-features","title":"New Features","text":"<ul> <li>Introduced the SpanList class to manage and visualize multiple spans from different traces in a unified HTML view. This update includes automatic input/output logging enhancements to the @span decorator, improving traceability and debugging capabilities. (c4cf814) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.5/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.17.5/#deprecations","title":"Deprecations","text":""},{"location":"releases/v0.17.6/","title":"V0.17.6","text":""},{"location":"releases/v0.17.6/#version-0176","title":"Version 0.17.6","text":"<p>This release includes several enhancements and bug fixes that improve the functionality and user experience of our software tools, particularly focusing on bot configurations, tutorial structures, and span handling in observability.</p>"},{"location":"releases/v0.17.6/#new-features","title":"New Features","text":"<ul> <li>Enhanced <code>extract_tool_calls</code> to support JSON content wrapped in a <code>{'tools': [...]}</code> dictionary and both 'arguments' and 'parameters' keys for tool call arguments. (c5896df) (Eric Ma)</li> <li>Added <code>list_uploaded_files</code> tool to the coordinator bot for better file management and workflow guidance. (5de98d1) (Eric Ma)</li> <li>Introduced new fields and detailed markdown explanations for structured data handling in PyData Boston 2025 tutorials. (2672123, 465804b) (Eric Ma)</li> <li>Added exercises and HTML rendering capabilities for <code>InvoiceData</code> and <code>ReceiptData</code> schemas in Marimo notebooks. (561c871) (Eric Ma)</li> <li>Implemented HTML configuration displays for <code>SimpleBot</code> and <code>StructuredBot</code> with enhanced <code>repr_html</code> methods. (55f986d) (Eric Ma)</li> <li>Major refactor and pedagogical improvements to the PyData Boston 2025 tutorial structure for a better learning experience. (4f6a2c7) (Eric Ma)</li> <li>Simplified span handling across bot classes to improve traceability and observability. (3f1fa72, 42dea7a) (Eric Ma)</li> <li>Automatically named spans using the variable name of the bot instance to enhance traceability in logging. (10eebf3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated display spans tests to match <code>SpanList</code> behavior ensuring accurate test results. (c6ca4f2) (Eric Ma)</li> <li>Fixed incorrect class name recording in spans for <code>StructuredBot</code> by using <code>type()</code> to correctly identify Pydantic models. (5bb9967, 355666a) (Eric Ma)</li> <li>Addressed span nesting issues to ensure proper parent-child relationships in span hierarchies. (11e8e10) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.6/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations were introduced in this version.</li> </ul>"},{"location":"releases/v0.17.7/","title":"V0.17.7","text":""},{"location":"releases/v0.17.7/#version-0177","title":"Version 0.17.7","text":"<p>This release includes enhancements to the <code>@tool</code> decorator, improvements in documentation, and various bug fixes and refactorings to improve the clarity and functionality of the code.</p>"},{"location":"releases/v0.17.7/#new-features","title":"New Features","text":"<ul> <li>Enhanced <code>@tool</code> decorator to automatically integrate with <code>@nodeify</code> and <code>@span</code>, simplifying usage and improving integration with AgentBot and observability tools. (2640322) (Eric Ma)</li> <li>Added a new tutorial demonstrating how to build practical LLM agents using a workflow-first approach, including a benchmark notebook for tool calling accuracy. (020eb25) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue in test assertions to align with updated error messages regarding AgentBot integration. (3205d9a) (Eric Ma)</li> <li>Corrected an unused function call in the invoice test code to enhance performance and maintainability. (684c498) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.7/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Improved clarity in the PyData Boston 2025 tutorial README, including better descriptions and additional instructions for using Molab. (084ca11) (Eric Ma)</li> <li>Updated the tutorial content to include discussion questions and additional thoughts on defining LLM agents and workflows. (ee50b44) (Eric Ma)</li> <li>Updated tutorial titles and headings for better clarity and simplicity in conveying the focus on LLM agents. (f6cef79, 37517b0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.7/#refactorings","title":"Refactorings","text":"<ul> <li>Removed redundant hidden code cell in the PyData Boston 2025 tutorial, streamlining the user experience. (cc6f182) (Eric Ma)</li> <li>Improved code formatting in the backoffice module for better readability and consistency. (13d2934) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.7/#chore","title":"Chore","text":"<ul> <li>Updated the Ollama models list to keep the project dependencies fresh and up-to-date. (e869964) (github-actions[bot])</li> </ul> <p>This version continues to refine the user experience and expand the capabilities of the software, ensuring better integration and easier usage for developers and end-users alike.</p>"},{"location":"releases/v0.17.8/","title":"V0.17.8","text":""},{"location":"releases/v0.17.8/#version-0178","title":"Version 0.17.8","text":"<p>This release includes important updates to the supported Python versions, enhancements to documentation, and various bug fixes improving the overall stability and usability of the software.</p>"},{"location":"releases/v0.17.8/#new-features","title":"New Features","text":"<ul> <li>Added comprehensive documentation for all bot types and components, helping users select the appropriate bot for their needs and providing detailed API references and usage examples. (f762ae1) (Eric Ma)</li> <li>Introduced a comprehensive getting started guide for contributing to LlamaBot, detailing setup instructions, development workflow, and project structure. (66cb488) (Eric Ma)</li> <li>Renamed 'getting-started.md' to 'setup.md' and updated the title to 'Development Setup' to better reflect its content and purpose. (643b964) (Eric Ma)</li> <li>Updated and expanded design documentation for Log Viewer and Unified Chat Memory, adding detailed sections on features, data models, and implementation plans. (6beab9b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed YAML parsing issue where Python version numbers were misinterpreted, ensuring correct version installations in GitHub Actions. (270f488) (Eric Ma)</li> <li>Corrected markdown list formatting in README for consistent bullet point rendering. (07897eb) (Eric Ma)</li> <li>Fixed code block formatting in the anthropic_api example to include proper delimiters. (7fa8e9f) (Eric Ma)</li> <li>Improved formatting and consistency across various documentation files, enhancing readability and structure. (e2caa95) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.8/#deprecations","title":"Deprecations","text":"<ul> <li>Dropped support for Python 3.9 and updated the minimum Python version requirement to 3.10, reflecting changes in the project's technology stack. (d93aab4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.9/","title":"V0.17.9","text":""},{"location":"releases/v0.17.9/#version-0179","title":"Version 0.17.9","text":"<p>This release includes enhancements to error handling, updates to documentation, and routine maintenance updates to model lists and markdown files.</p>"},{"location":"releases/v0.17.9/#new-features","title":"New Features","text":"<ul> <li>Added interactive UI elements for style customization in the blog banner generation tutorial, allowing users to select artistic styles and other visual elements dynamically. (841c2a) (Eric Ma)</li> <li>Integrated Marimo notebook conversion directly into the MkDocs build process, streamlining documentation updates and ensuring consistency across generated markdown files. (6b7c1b) (Eric Ma)</li> <li>Added new how-to guides formatted as Marimo notebooks, which are automatically converted to markdown, enhancing the documentation with practical, structured examples. (6111da) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.9/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Improved error messaging for the <code>@prompt</code> decorator to clearly indicate the necessity of using parentheses and valid role arguments, enhancing developer experience and error traceability. (caa435) (Eric Ma)</li> <li>Fixed the injection process in the Marimo to markdown conversion script to prevent duplicate entries and ensure command instructions are included below the Molab shield. (a14330) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.9/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Added a new how-to guide for generating blog banner images, complete with instructions and inclusion in the MkDocs navigation. (fd86f9) (Eric Ma)</li> <li>Updated documentation build processes to verify that no markdown files are present in the site directory post-build, ensuring clean deployment. (48882e) (Eric Ma)</li> <li>Enhanced documentation links to use absolute URLs and improved navigation in MkDocs, removing outdated links and commands. (3bbe2d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.17.9/#chore","title":"Chore","text":"<ul> <li>Routine updates to the Ollama models list, ensuring the latest models are available for use. (9f4f2d, efa8c9, 4d310f, bcb9db, ccc036) (github-actions[bot])</li> <li>Updated generated markdown files from Marimo notebooks to reflect the latest changes and integrations. (42c235, 694360) (github-actions)</li> </ul>"},{"location":"releases/v0.17.9/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the 'pypi-prerelease-mode' option from the pixi.lock file environments, simplifying the configuration. (e7b26f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.0/","title":"V0.2.0","text":""},{"location":"releases/v0.2.0/#version-020","title":"Version 0.2.0","text":"<p>This release includes several improvements and new features for the LlamaBot project.</p>"},{"location":"releases/v0.2.0/#new-features","title":"New Features","text":"<ul> <li>API Server: Merged pull request #28, which introduces an API server for the LlamaBot project. (4ea160a, Eric Ma)</li> <li>Mock Response and API Key Support: Added <code>api_key</code> and <code>mock_response</code> parameters to the SimpleBot constructor for OpenAI API key usage and testing with predefined responses. (2f6d1d9, Eric Ma)</li> <li>Streaming Response Test: Implemented a new test case to verify that SimpleBot can stream responses correctly. (5ddb804, Eric Ma)</li> <li>Delta Content Printing: The SimpleBot class now prints the delta content to the console after processing each message for better readability. (d657b4a, Eric Ma)</li> <li>ChatBot UI Jupyter Notebook: Created a new Jupyter notebook for ChatBot UI development, including the setup of necessary classes and functions. (bb4397a, Eric Ma)</li> <li>ChatUIMixin: Introduced a new ChatUIMixin class for easier integration of chat functionalities in LlamaBot components. (4209b18, Eric Ma)</li> <li>Streamlined Message Handling and Typing: Simplified the message construction and typing in the SimpleBot class for improved readability and maintainability. (65e026c, Eric Ma)</li> <li>Streaming Response for Chat Messages: Implemented streaming response functionality in the ChatBot class for better real-time interactivity. (1ebc356, Eric Ma)</li> <li>Improved Response Streaming: Extracted streaming logic into a separate method and ensured consistent yielding of AIMessage instances in the SimpleBot class. (08636a7, Eric Ma)</li> <li>Toggleable Streaming Responses: Added a <code>stream</code> parameter to the generate_response method in the SimpleBot class to control streaming behavior. (565aed7, Eric Ma)</li> <li>Streaming Response Capability: Implemented a new stream_response method in the SimpleBot class for streaming responses incrementally. (2a8254c, Eric Ma)</li> <li>Response Generation Extraction: Simplified the generate_response method in the SimpleBot class by extracting the response generation logic into a new _make_response function. (0ad9a1e, Eric Ma)</li> <li>API Key Instructions: Added instructions for setting API keys for other providers in the documentation. (55ec13e, Eric Ma)</li> <li>Standardized LlamaBot Naming Convention: Corrected the casing of 'LLaMaBot' to 'LlamaBot' throughout the index.md documentation and separated API provider configuration instructions into subsections for OpenAI and Mistral. (7fd2e13, Eric Ma)</li> <li>New Model Names and CLI Options Refactoring: Added 'stablelm2' and 'duckdb-nsql' to the list of available models and refactored command-line interface arguments in serve.py to use Typer options instead of arguments. (e6a2122, Eric Ma)</li> <li>FastAPI Endpoint for QueryBot: Implemented APIMixin to allow QueryBot to serve FastAPI endpoints and added a <code>serve</code> command to the CLI for starting a FastAPI server with QueryBot. (5edd84b, Eric Ma)</li> <li>Improved System Prompt for QueryBot: Modified the system prompt in QueryBot to be more specific about the source of knowledge and clarified the response behavior when the repository does not contain the answer. (5f7ce51, Eric Ma)</li> <li>LlamaBot CLI Usage Guide: Added a comprehensive guide for the LlamaBot CLI in the documentation, including installation instructions, key commands, and usage examples. (9f0b1c8, Eric Ma)</li> </ul>"},{"location":"releases/v0.2.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>ImageBot Import Path Update: Changed the import path of AIMessage from langchain.schema to llamabot.components.messages to reflect the new module structure. (27904d0, Eric Ma)</li> <li>Error Handling for Image URL Retrieval: Added an exception raise in the ImageBot.generate_image method to handle cases where no image URL is found in the response. (27904d0, Eric Ma)</li> <li>Disabled Streaming in SimpleBot Tests: Passed <code>stream=False</code> when creating a SimpleBot instance in tests to ensure consistent behavior without relying on streaming features. (e559114, Eric Ma)</li> <li>Ensured Non-Empty Strings in Bot Tests: Modified tests to generate non-empty strings for system_prompt and human_message using hypothesis strategies. (e8fed0a, Eric Ma)</li> </ul>"},{"location":"releases/v0.2.0/#deprecations","title":"Deprecations","text":"<ul> <li>Removed Unused Panel App Creation Code: Removed the <code>create_panel_app</code> function and its related imports from python.py as they are no longer used. (4469b35, Eric Ma)</li> <li>Removed PanelMarkdownCallbackHandler Class: Removed the PanelMarkdownCallbackHandler class as it is no longer required in the llamabot project. (b7ef263, Eric Ma)</li> <li>Removed Unused pytest Import and Obsolete Test: Removed an unused import of pytest in test_simplebot.py and deleted the test_simple_bot_stream_response function, which is no longer needed due to changes in the SimpleBot streaming response logic. (ed0756b, Eric Ma)</li> <li>Removed model_dispatcher Module: The model_dispatcher.py module has been removed as part of a refactoring effort. This change simplifies the llamabot architecture by delegating model dispatch responsibilities to a new system or removing the need for such functionality entirely. (0887618, Eric Ma)</li> <li>Removed api_key Command from configure.py: The api_key command was deprecated and has been removed to simplify configuration. Users should now set API keys directly via environment variables. (2752d7e, Eric Ma)</li> </ul>"},{"location":"releases/v0.2.1/","title":"V0.2.1","text":""},{"location":"releases/v0.2.1/#version-021","title":"Version 0.2.1","text":"<p>This release includes improvements to the Zotero integration, QueryBot initialization, and the removal of Ollama response content.</p>"},{"location":"releases/v0.2.1/#new-features","title":"New Features","text":"<ul> <li>Add a space in the mock to test the strip works (cced1c9) (Aidan Brewis)</li> <li>Strip the message content from Ollama responses (2c37454) (Aidan Brewis)</li> <li>Update QueryBot initialization and import paths for better readability (f44c98d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Sanitize collection names using slugify to ensure URL-friendliness (9e7d717) (Eric Ma)</li> <li>Adjust pydantic dependency location for proper package resolution (eef70a0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.1/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul> <p>Note: The release notes for version 0.2.0 have been added in commit 3331d44.</p>"},{"location":"releases/v0.2.2/","title":"V0.2.2","text":""},{"location":"releases/v0.2.2/#version-022","title":"Version 0.2.2","text":"<p>This release includes several updates and improvements to the project.</p>"},{"location":"releases/v0.2.2/#new-features","title":"New Features","text":"<ul> <li>Added support for passing keyword arguments to the <code>simple</code> and <code>querybot</code> functions. (d88862b) (Rena Lu)</li> <li>Updated the import statement for <code>llama_index</code> to improve code readability. (6f48ccd) (Rena Lu)</li> <li>Updated the <code>llama_index</code> requirement to the latest version. (4bc1078) (Rena Lu)</li> <li>Added a newline at the end of the v0.2.1 release notes document to maintain consistency and proper formatting in markdown files. (48ee571) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Removed a test print statement that was accidentally left in the code. (9b85753) (Rena Lu)</li> </ul>"},{"location":"releases/v0.2.2/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul> <p>Note: A code style update was also made in this release, but it does not introduce any new features or bug fixes. (e3f6211) (Rena Lu)</p>"},{"location":"releases/v0.2.3/","title":"V0.2.3","text":""},{"location":"releases/v0.2.3/#version-023","title":"Version 0.2.3","text":"<p>This release includes several enhancements to the ChatBot and QueryBot functionalities, as well as improvements to testing and streaming capabilities.</p>"},{"location":"releases/v0.2.3/#new-features","title":"New Features","text":"<ul> <li>Bump version to 0.2.3 (d61ee4c) (github-actions)</li> <li>Enhance ChatBot functionality and testing: Add <code>stream_target</code> parameter, update test cases, remove deprecated methods, and introduce new tests (17428e8) (Eric Ma)</li> <li>Add unit test for QueryBot initialization using Hypothesis for property-based testing (2aa9461) (Eric Ma)</li> <li>Streamline stdout streaming in QueryBot and remove unnecessary constructor parameter (2f39593) (Eric Ma)</li> <li>Enhance SimpleBot tests with stream API scenarios (0ea04b8) (Eric Ma)</li> <li>Remove redundant test_chatbot_call and update test_chatbot_repr (3ec0aa8) (Eric Ma)</li> <li>Simplify mocking in chatbot repr test (244dad9) (Eric Ma)</li> <li>Remove debug print statement and streamline commit message echo in SimpleBot (f16d7a3) (Eric Ma)</li> <li>Correct dictionary access and message concatenation in SimpleBot (79d2929) (Eric Ma)</li> <li>Allow passing additional kwargs to completion function in ChatBot (4058693) (Eric Ma)</li> <li>Enhance streaming capabilities and add type hints in ChatBot (c11aace) (Eric Ma)</li> <li>Remove deprecated Jupyter notebook example for streaming (d6693a3) (Eric Ma)</li> <li>Replace 'stream' parameter with 'stream_target' for more flexible output options (1211115) (Eric Ma)</li> <li>Enhance SimpleBot streaming capabilities and update notebook examples (ab8c359) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fix ChatBot response mocking in unit test (7c02d18) (Eric Ma)</li> <li>Correct dictionary access and message concatenation in SimpleBot (79d2929) (Eric Ma)</li> <li>Replace pdfminer with pdfminer.six for better Python 3 support (79908b1) (Eric Ma)</li> <li>Replace pdfreader with pdfminer for improved PDF processing (7910e3e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.3/#deprecations","title":"Deprecations","text":"<ul> <li>Remove 'api' stream_target from ChatBot and change the expected return type for consumers of the ChatBot class (c11aace) (Eric Ma)</li> <li>Replace 'stream' boolean parameter with 'stream_target' in ChatBot and SimpleBot constructors (1211115) (Eric Ma)</li> </ul> <p>Please note that some breaking changes have been introduced in this release. Make sure to update your code accordingly. For more details, refer to the individual commit messages.</p>"},{"location":"releases/v0.2.4/","title":"V0.2.4","text":""},{"location":"releases/v0.2.4/#version-024","title":"Version 0.2.4","text":"<p>This release includes improvements to the autorecord function, enhanced chat command, and updates to Python kernel versions.</p>"},{"location":"releases/v0.2.4/#new-features","title":"New Features","text":"<ul> <li>Autorecord function has been streamlined to record only the last message content, reducing data processing and potential performance issues (268590, Eric Ma)</li> <li>The chat command in the CLI now includes a timestamped session name for better traceability and organization of chat sessions (268590, Eric Ma)</li> </ul>"},{"location":"releases/v0.2.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The Python kernel version in sembr notebook has been updated to 3.11.7 to ensure compatibility with the latest libraries and features (0ad4701, Eric Ma)</li> </ul>"},{"location":"releases/v0.2.4/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release</li> </ul> <p>Note: The commit 9153c5d is a refactoring commit that improves the readability and maintenance of the notebook code, but it does not introduce any new features or bug fixes. The commit b120061 and 31b1056 are related to version bumping and release notes, respectively. The merge commit ae66c86 is not associated with any new features, bug fixes, or deprecations.</p>"},{"location":"releases/v0.2.5/","title":"V0.2.5","text":"<p>Here are the release notes based on the provided commit log:</p>"},{"location":"releases/v0.2.5/#version-025","title":"Version 0.2.5","text":"<p>This release includes a small fix to the <code>plaintext_loader</code> function in the <code>doc_processor</code> module. The file open mode was changed from \"r\" to \"r+\" to allow for additional operations on the file if needed in the future.</p>"},{"location":"releases/v0.2.5/#new-features","title":"New Features","text":"<p>There are no new features in this release.</p>"},{"location":"releases/v0.2.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The file open mode in <code>plaintext_loader</code> function was changed from \"r\" (read-only) to \"r+\" (read and write). This allows for additional operations on the file if needed in the future. (8251fdc) (Eric Ma)</li> </ul>"},{"location":"releases/v0.2.5/#deprecations","title":"Deprecations","text":"<p>There are no deprecations in this release.</p> <p>Note: The commit <code>48bb8c4</code> is related to version bump and does not introduce any new features or bug fixes. The commit <code>faa971d</code> is related to adding release notes and does not introduce any new features or bug fixes. Therefore, they are not included in the release notes.</p>"},{"location":"releases/v0.3.1/","title":"V0.3.1","text":""},{"location":"releases/v0.3.1/#version-031","title":"Version 0.3.1","text":"<p>This release includes several new features and improvements to enhance the functionality and performance of the project.</p>"},{"location":"releases/v0.3.1/#new-features","title":"New Features","text":"<ul> <li>Bumped version to 0.3.0, introducing new features and improvements (7a0bcbe, Eric Ma)</li> <li>Streamlined API key configuration for release notes generation by utilizing environment variables directly (48c9231, Eric Ma)</li> <li>Added reset call to querybot test to maintain test isolation and consistency (5d28f7e, Eric Ma)</li> <li>Enhanced <code>test_querybot</code> with stream target and input validations to improve test coverage and robustness (54a8a5c, Eric Ma)</li> <li>Updated the underlying model in llamabot/prompt_library/git.py from <code>mistral/mistral-medium</code> to <code>gpt-4-0125-preview</code> to improve response quality and accuracy (603ad73, Eric Ma)</li> <li>Added support for different LLM models, initial message, and Panel integration to the <code>chat</code> command (8fac319, Eric Ma)</li> <li>Added ChatUIMixin to QueryBot and updated initialization and usage for more flexible handling of user input and output (a662e8b, Eric Ma)</li> <li>Added support for initial message and serving the chat interface to ChatUIMixin (d546a9c, Eric Ma)</li> <li>Added BM25 search algorithm to DocumentStore for more flexible and accurate document retrieval (c265c1a, Eric Ma)</li> <li>Added rank-bm25 library for BM25 ranking to improve search result accuracy (0eee328, Eric Ma)</li> <li>Added rank-bm25 library as a dependency for advanced search functionality (29cdb0d, Eric Ma)</li> <li>Updated commitbot model to mistral/mistral-medium for cost savings (00dc294, Eric Ma)</li> <li>Added new Jupyter notebook for parsing Zotero library with Ollama Mistral model (ae383cb, Eric Ma)</li> <li>Updated model name to mistral-medium in commitbot() function of git.py (a5f918f, Eric Ma)</li> <li>Updated transformer model to \"mistralai/Mistral-7B-v0.1\" for improved performance and accuracy (87208bf, Eric Ma)</li> <li>Added FastAPI example with async endpoint (fe20f55, Eric Ma)</li> <li>Ensured save_filename is a Path object before saving chat logs (0cb480d, Eric Ma)</li> <li>Added interactive JavaScript and HTML outputs to Jupyter notebook example (6ee552f, Eric Ma)</li> <li>Switched to micromamba for environment setup in docs (547b20e, Eric Ma)</li> <li>Copied README to docs/index.md during docs build process (6ce70ae, Eric Ma)</li> </ul>"},{"location":"releases/v0.3.1/#bug-fixes","title":"Bug Fixes","text":"<p>There are no bug fixes in this release.</p>"},{"location":"releases/v0.3.1/#deprecations","title":"Deprecations","text":"<p>There are no deprecations in this release.</p>"},{"location":"releases/v0.4.0/","title":"V0.4.0","text":""},{"location":"releases/v0.4.0/#version-040","title":"Version 0.4.0","text":"<p>This release includes several enhancements and improvements to the document store, query bot, and CLI functionality. Additionally, LanceDB has been integrated as a new document store option, and the project dependencies have been updated.</p>"},{"location":"releases/v0.4.0/#new-features","title":"New Features","text":"<ul> <li>Bump version to 0.4.0 (3278cc4) (github-actions)</li> <li>Switch QueryBot to use LanceDB instead of ChromaDB (144534e) (Eric Ma)</li> <li>Add initial Dockerfile for doc_chat deployment (fb3e873) (Eric Ma)</li> <li>Add post-document addition hook and enhance query flexibility (04d6cb6) (Eric Ma)</li> <li>Remove BM25DocStore integration (c2072de) (Eric Ma)</li> <li>Add tantivy to project dependencies (64434bb) (Eric Ma)</li> <li>Add notebook to demonstrate URL markdown issue (846d2b2) (Eric Ma)</li> <li>Prevent document duplication in LanceDBDocStore (6d88525) (Eric Ma)</li> <li>Allow custom initial message in chat function (84e3fd2) (Eric Ma)</li> <li>Enhance bot's system prompt for clarity (6d71a36) (Eric Ma)</li> <li>Enhance logging and simplify document retrieval logic in QueryBot (701669e) (Eric Ma)</li> <li>Update default model to gpt-4-0125-preview (2cebeaa) (Eric Ma)</li> <li>Switch DocumentStore to LanceDBDocStore for question and document storage (688b0fe) (Eric Ma)</li> <li>Remove DocumentStore alias and enhance test coverage (7103b84) (Eric Ma)</li> <li>Add initial Jupyter notebook for LanceDB integration (476d031) (Eric Ma)</li> <li>Add chromadb to project dependencies (baa83f2) (Eric Ma)</li> <li>Ensure chat loop only runs in non-serve mode (53ab092) (Eric Ma)</li> <li>Introduce abstract document store and LanceDB integration (5d9d9c0) (Eric Ma)</li> <li>Integrate LanceDBDocStore and BM25DocStore for document retrieval (f625e57) (Eric Ma)</li> <li>Add lancedb to project dependencies (c8a4699) (Eric Ma)</li> <li>Add sentence-transformers and remove chromadb (ce7c610) (Eric Ma)</li> </ul>"},{"location":"releases/v0.4.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Remove deprecated reproduce_failure decorator (4e6c007) (Eric Ma)</li> <li>Handle empty document retrieval gracefully (8c12d93) (Eric Ma)</li> <li>Remove redundant reset call in test_querybot (e5e4bc9) (Eric Ma)</li> <li>Ensure proper reset of stores in querybot tests (151a41d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.4.0/#deprecations","title":"Deprecations","text":"<ul> <li>Remove scratch notebook docstore_lancedb (666dbf6) (Eric Ma)</li> <li>Remove obsolete cache prototype notebook (6d88525) (Eric Ma)</li> <li>Remove DocumentStore alias (7103b84) (Eric Ma)</li> <li>Remove chromadb from dependencies (ce7c610) (Eric Ma)</li> </ul>"},{"location":"releases/v0.4.1/","title":"V0.4.1","text":""},{"location":"releases/v0.4.1/#version-041","title":"Version 0.4.1","text":"<p>This release includes several improvements to the CLI tool's performance, test coverage, and code maintainability.</p>"},{"location":"releases/v0.4.1/#new-features","title":"New Features","text":"<ul> <li>Expanded model names list and increased default query results: Added <code>command-r</code> and <code>mxbai-embed-large</code> to the model names list, and increased the default number of query results from 10 to 20 for better performance and more relevant search results. (1389534, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.1/#improvements","title":"Improvements","text":"<ul> <li>Improved CLI tool execution time test: Introduced a new test case to measure the execution time of the llamabot CLI tool and ensured it does not exceed a predefined threshold (2 seconds) to maintain performance expectations. (09f776e, Eric Ma)</li> <li>Adjusted CLI tool execution time threshold: Increased the execution time assertion from 2.0 seconds to 3.0 seconds to accommodate changes in the CLI tool's performance characteristics and ensure reliable tests under varying execution conditions. (ca0d353, Eric Ma)</li> <li>Simplified <code>test_call_in_jupyter</code> with patching: Replaced <code>mocker</code> usage with <code>unittest.mock.patch</code> for consistency and clarity, utilized <code>MagicMock</code> directly for mocking responses, and streamlined the test by removing redundant setup and assertions. (ee29c25, Eric Ma)</li> <li>Optimized import statements: Moved imports to function scope in <code>imagebot.py</code> and <code>docstore.py</code> to improve import efficiency and potentially reduce the initial load time of the modules. (d4e5920, Eric Ma)</li> <li>Streamlined embedding and schema definition: Replaced <code>Optional</code> type import with <code>Callable</code>, removed <code>DocstoreEntry</code> class from global scope, integrated its definition within <code>LanceDBDocStore</code> constructor, and simplified embedding function retrieval. (7621520, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.1/#code-maintenance-and-optimization","title":"Code Maintenance and Optimization","text":"<ul> <li>Bumped version to 0.4.1: Updated the version number to 0.4.1 using GitHub Actions. (6b1df0c, github-actions)</li> <li>Merged pull request #50: Incorporated changes from the <code>improve-cli-timing</code> branch. (e24911a, Eric Ma)</li> <li>Optimized imports and removed debug logs: Moved <code>openai.OpenAI</code> import to <code>ImageBot</code> constructor, removed unused <code>loguru.logger</code> imports and related debug log statements, and encapsulated <code>lancedb.embeddings.get_registry</code> import within <code>DocstoreEntry</code> constructor. (f158ddd, Eric Ma)</li> <li>Optimized imports and dynamic loading: Removed unused imports, implemented dynamic import loading for <code>panel</code>, <code>pandas</code>, <code>chromadb</code>, and <code>lancedb</code>, and adjusted the scope of import statements to function-level where applicable. (2339555, Eric Ma)</li> <li>Ensured newline at end of v0.4.0 release notes: Added a newline at the end of the v0.4.0 release notes file to ensure compliance with POSIX standards and improve compatibility with various text processing tools. (b9bd9c5, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.1/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul>"},{"location":"releases/v0.4.2/","title":"V0.4.2","text":""},{"location":"releases/v0.4.2/#version-042","title":"Version 0.4.2","text":"<p>This release includes improvements to the Llamabot CLI documentation, enhanced code standards enforcement, and fixes for release notes generation.</p>"},{"location":"releases/v0.4.2/#new-features","title":"New Features","text":"<ul> <li>Improved Llamabot CLI documentation and usage, including new <code>chat</code> command details and optional parameters (f683efe, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Ensured single newline at end of release notes to prevent multiple newlines (ac80ce6, Eric Ma)</li> <li>Enforced pre-commit checks to run twice on failure for stricter code standards enforcement (26236c4, Eric Ma)</li> <li>Fixed missing newline at the end of the v0.4.1 release notes document (39f75a9, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.2/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul> <p>Note: The version bump commit (42331e3, github-actions) is not included in the release notes summary.</p>"},{"location":"releases/v0.4.3/","title":"V0.4.3","text":""},{"location":"releases/v0.4.3/#version-043","title":"Version 0.4.3","text":"<p>This release includes an update to the <code>ollama</code> model names list and the <code>panel</code> dependency.</p>"},{"location":"releases/v0.4.3/#new-features","title":"New Features","text":"<ul> <li>Added <code>codegemma</code> to the list of <code>ollama</code> model names for enhanced functionality. (cdf1fa6, Eric Ma)</li> <li>Updated the <code>panel</code> dependency from <code>1.1.0</code> to <code>1.3.0</code> for improved stability and features. This update ensures better performance and compatibility with the latest libraries. (cdf1fa6, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.3/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes were included in this release.</p>"},{"location":"releases/v0.4.3/#deprecations","title":"Deprecations","text":"<p>No deprecations were included in this release.</p>"},{"location":"releases/v0.4.4/","title":"V0.4.4","text":""},{"location":"releases/v0.4.4/#version-044","title":"Version 0.4.4","text":"<p>This release includes several enhancements to the Docker deployments and command-line interface for the chat service.</p>"},{"location":"releases/v0.4.4/#new-features","title":"New Features","text":"<ul> <li>The base Docker image has been switched to <code>python:3.10-slim</code> and environment variables have been introduced for more flexible configuration. Dependencies are now installed directly via <code>pip</code> and <code>curl</code>. The <code>llamabot</code> CLI and <code>ChatUIMixin</code> have been updated to support custom host and port configurations. (43fe652) (Eric Ma)</li> <li>The command-line interface for the API has been updated to include address and port parameters, allowing for more flexible deployment options. (336ad23) (Eric Ma)</li> </ul>"},{"location":"releases/v0.4.4/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes were included in this release.</p>"},{"location":"releases/v0.4.4/#deprecations","title":"Deprecations","text":"<ul> <li>The use of the <code>condaforge/mambaforge</code> base Docker image has been deprecated in favor of <code>python:3.10-slim</code>. The default port has also been changed to <code>6363</code>. These changes may require updates to existing deployment configurations. (43fe652) (Eric Ma)</li> </ul> <p>Note: The commit <code>8e66d0c</code> is a merge commit and does not contain any new features or bug fixes. The commit <code>04d2bc5</code> is a version bump commit and does not contain any new features or bug fixes. The commit <code>84736a0</code> is a release notes commit and does not contain any new features or bug fixes.</p>"},{"location":"releases/v0.4.5/","title":"V0.4.5","text":""},{"location":"releases/v0.4.5/#version-045","title":"Version 0.4.5","text":"<p>This release includes several improvements and updates to the Ollama model list, as well as addressing a compatibility issue with the numpy library.</p>"},{"location":"releases/v0.4.5/#new-features","title":"New Features","text":"<ul> <li>Expanded the list of available Ollama models and updated the Python version requirement to &gt;=3.11 for better compatibility with the latest features (f2a5828, Eric Ma)</li> <li>Added \"codestral\" to the list of Ollama model names to enhance the bot's capabilities (0488db0, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Temporarily pinned the numpy version to &lt;2 in both environment.yml and pyproject.toml to address a compatibility issue with the current codebase (e1021d2, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.5/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release</li> </ul> <p>Note: The Python version requirement has been updated to &gt;=3.11, which may potentially break environments with older Python versions. Please ensure your environment meets the new requirement before upgrading.</p>"},{"location":"releases/v0.4.6/","title":"V0.4.6","text":""},{"location":"releases/v0.4.6/#version-046","title":"Version 0.4.6","text":"<p>This release includes a new feature that allows users to choose between receiving the URL of the generated image or saving the image locally when using the <code>ImageBot</code> class. Additionally, there is a fix for test readability and alignment with the expected use of the <code>bot</code> function parameters.</p>"},{"location":"releases/v0.4.6/#new-features","title":"New Features","text":"<ul> <li>Added a <code>return_url</code> parameter to the <code>ImageBot</code> class to allow users to choose between receiving the URL of the generated image or saving the image locally. This change also includes an updated call signature and logic for handling the new parameter, as well as a new Jupyter notebook workflow for generating banner images based on blog post summaries. (64b1488, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue in the tests where the <code>bot</code> function call in <code>test_call_outside_jupyter</code> was not using the named parameter <code>save_path</code> for clarity and to match function signature expectations. This change improves test readability and aligns with the expected use of the <code>bot</code> function parameters. (18f1095, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.6/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.4.7/","title":"V0.4.7","text":""},{"location":"releases/v0.4.7/#version-047","title":"Version 0.4.7","text":"<p>This release includes several enhancements to the SimpleBot, LlamaBot, and development environment, as well as a new JSON mode example notebook.</p>"},{"location":"releases/v0.4.7/#new-features","title":"New Features","text":"<ul> <li>Added a new Jupyter notebook example demonstrating SimpleBot in JSON mode (644c2c0, Eric Ma)</li> <li>Updated the development container to use <code>mambaorg/micromamba</code> as the base image for faster dependency resolution (fe720e0, Eric Ma)</li> <li>Enhanced diffbot functionality with direct diff printing and new modules for git operations and prompt management (a04b661, Eric Ma)</li> <li>Created a new notebook for GitHub file-level summary, providing tools for detailed pull request analysis (a04b661, Eric Ma)</li> <li>Added a new pylab example script for the SciPy 2024 conference (7596858, Eric Ma)</li> <li>Improved LlamaBot documentation with emojis, clarifications, and new sections for Git commit message writer, automatic release notes, and detailed bot engine documentation (618ae79, Eric Ma)</li> <li>Streamlined the LlamaBot presentation content for brevity and focus (ae1aec2, Eric Ma)</li> <li>Added a new presentation for SciPy 2024 on \"LlamaBot: A Pythonic Interface to LLMs\" (3495b6a, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.7/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed a bug where the initial system prompt was missing in the message flow of the QueryBot class (5f2cae4, Eric Ma)</li> </ul>"},{"location":"releases/v0.4.7/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the \"JSON Mode\" section from the LlamaBot presentation, focusing on core functionalities (ae1aec2, Eric Ma)</li> </ul> <p>Additionally, this release includes various improvements to the development environment, such as updates to the devcontainer configurations, Dockerfile streamlining, and the addition of essential tools for better code quality and collaboration.</p>"},{"location":"releases/v0.4.8/","title":"V0.4.8","text":""},{"location":"releases/v0.4.8/#version-048","title":"Version 0.4.8","text":"<p>This release includes several improvements and updates to the project, focusing on enhancing the development experience, adding new features, and improving documentation.</p>"},{"location":"releases/v0.4.8/#new-features","title":"New Features","text":"<ul> <li>Add Ollama-specific checks for CI pipeline: Introduces a new GitHub Actions workflow for performing Ollama-specific checks on pull requests to the main branch. The workflow includes steps for checking out the repository, downloading Ollama, starting Ollama in the background, and pulling Microsoft's Phi3 model. (d229a22) (Eric Ma)</li> <li>Integrate Ollama setup into pr-tests workflow: Removed the Ollama-specific checks workflow and added Ollama setup steps to the <code>pr-tests.yaml</code> workflow to ensure Ollama models are tested on pull requests. Updated pre-commit hooks to their latest versions to improve code quality checks. Added a new test case for Ollama integration in <code>test_bot.py</code>. (07ab4ec) (Eric Ma)</li> <li>Add Ollama-specific checks for CI pipeline: Introduces a new GitHub Actions workflow <code>ollama-on-ci.yaml</code> for performing Ollama-specific checks on pull requests to the main branch. The workflow includes steps for checking out the repository, downloading Ollama, starting Ollama in the background, and pulling Microsoft's Phi3 model. Utilizes <code>ubuntu-latest</code> runner for executing the workflow steps. (d229a22) (Eric Ma)</li> <li>Migrate dependency configuration to pixi.toml: Moved project and dependency configuration from <code>pyproject.toml</code> to <code>pixi.toml</code> to centralize configuration and simplify project setup. This change organizes dependencies, project metadata, and tasks into a single <code>pixi.toml</code> file, improving readability and maintainability. (1316116) (Eric Ma)</li> <li>Enable pixi to develop LlamaBot: This commit enables pixi to develop LlamaBot by merging the necessary changes from the <code>pixi</code> branch. (9e4d9d3) (Eric Ma)</li> <li>Add eefricker as a contributor for doc: Adds Ethan Fricker as a contributor for documentation. (d415fe7) (allcontributors[bot])</li> <li>Add ElliotSalisbury as a contributor for doc: Adds Elliot Salisbury as a contributor for documentation. (70b30b0) (allcontributors[bot])</li> <li>Add anujsinha3 as a contributor for doc: Adds Anuj Sinha as a contributor for documentation. (7cd087d) (allcontributors[bot])</li> <li>Add anujsinha3 as a contributor for code: Adds Anuj Sinha as a contributor for code. (becd294) (allcontributors[bot])</li> <li>Add reka as a contributor for doc and code: Adds Reka as a contributor for documentation and code. (4314c1f) (allcontributors[bot])</li> <li>Add Pixi environment and update Micromamba setup in CI workflow: Added Pixi environment setup to the CI workflow for enhanced testing capabilities. Updated the Micromamba setup step to include a specific name, making the workflow more readable and maintainable. Expanded the <code>environment-type</code> matrix to include the new 'pixi' environment, alongside the existing 'miniconda' and 'bare' environments. Removed commented-out Miniconda setup code to clean up the workflow file and improve clarity. (320e82b) (Eric Ma)</li> <li>Add mknotebooks to docs: Added <code>mknotebooks</code> to the documentation dependencies in <code>pyproject.toml</code> for enhanced notebook integration. (68b67b6) (Eric Ma)</li> <li>Expand project dependencies and add mknotebooks to docs: Introduced a new <code>[tool.pixi.dependencies]</code> section with a comprehensive list of project dependencies, including <code>openai</code>, <code>panel</code>, <code>bokeh</code>, and others, to facilitate development and ensure compatibility. Specified version constraints for several dependencies such as <code>panel &gt;=1.3.0</code>, <code>bokeh &gt;=3.1.0</code>, <code>astor &gt;=0.8.1</code>, <code>typer &gt;=0.4.7</code>, <code>pydantic &gt;=2.0</code>, and <code>numpy &lt;2</code> to address specific issues and requirements. Added a temporary version constraint for <code>litellm &lt;=1.35.38</code> to maintain JSON mode functionality, pending resolution of a reported issue. Locked <code>beartype</code> to version <code>0.15.0</code> to ensure stability. Expanded the <code>[tool.pixi.pypi-dependencies]</code> section with new entries such as <code>case-converter</code>, <code>rank-bm25</code>, and <code>tantivy</code> to enhance project capabilities. Included a new <code>docs</code> task in <code>[tool.pixi.tasks]</code> for local documentation serving with <code>mkdocs serve</code>. (bc1fd71) (Eric Ma)</li> <li>Install Ollama tool in Docker environment: Added commands to update apt-get and install curl. Installed Ollama using its official installation script. Prepared for future Ollama pull command inclusion by commenting it out for now. (ee755e3) (Eric Ma)</li> <li>Schedule daily development container builds: Add a cron job to the GitHub Actions workflow to trigger the build of the development container every day at 3.14 am. This ensures that the development environment is always up to date with the latest dependencies. (20e0796) (Eric Ma)</li> <li>Update Docker image tag for consistency: Change the Docker image tag from <code>llamabot/devcontainer:latest</code> to <code>ericmjl/llamabot-devcontainer:latest</code> to align with naming conventions across projects. (5bf95bd) (Eric Ma)</li> <li>Add pixi configuration and update git settings: Add <code>.gitattributes</code> for GitHub syntax highlighting of <code>pixi.lock</code> files, treating them as generated YAML files. Update <code>.gitignore</code> to exclude pixi environments and <code>*.egg-info</code> files. Introduce pixi configuration in <code>pyproject.toml</code> for dependency management and task execution, specifying conda channels, platforms, and environments for development, testing, and documentation. (41e3811) (Eric Ma)</li> <li>Add GitHub action for building dev container: Introduces a new GitHub action workflow to build a Docker image from the project's Dockerfile upon every push to the repository. This ensures that the development container is always up to date with the latest changes in the codebase. Utilizes <code>ubuntu-latest</code> runner for executing the build steps. Employs <code>actions/checkout@v2</code> for checking out the repository code before building the Docker image. (4450044) (Eric Ma)</li> <li>Recommend increased Docker RAM for LLMs: Added a recommendation for configuring Docker to use 12-14GB of RAM to support local LLMs effectively. (6c0a9cb) (Eric Ma)</li> <li>Add guide for setting up a development container: This commit introduces a new documentation file, <code>devcontainer.md</code>, under the <code>docs/developer</code> directory. The guide provides comprehensive instructions for setting up a development environment for LlamaBot using development containers. It includes prerequisites such as having <code>git</code> and Docker installed, and step-by-step instructions from forking LlamaBot to making a pull request. This addition aims to simplify the setup process for new contributors, especially those on Windows platforms, by leveraging the development container that comes pre-configured with Ollama. (412e8c3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.4.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Remove hardcoded environments from cache setup: Removed the hardcoded environments 'default' and 'tests' from the cache setup in the GitHub Actions workflow for pull request tests. This change simplifies the configuration and relies on dynamic environment settings. (f16aaff) (Eric Ma)</li> <li>Remove Ollama background service start: Ollama service start in background removed to streamline PR tests setup. This is not necessary b/c the Ollama setup already handles starting the service. (67d0546) (Eric Ma)</li> </ul>"},{"location":"releases/v0.4.8/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.5.0/","title":"V0.5.0","text":""},{"location":"releases/v0.5.0/#version-050","title":"Version 0.5.0","text":"<p>This release includes several improvements and new features for the LlamaBot project.</p>"},{"location":"releases/v0.5.0/#new-features","title":"New Features","text":"<ul> <li>Added a new Jupyter notebook example demonstrating how to build a bot for checking if docstrings match the function source code (3f87427) (Eric Ma)</li> <li>Enhanced the StructuredBot initialization process by including default language model configuration (7a352ca) (Eric Ma)</li> <li>Introduced a new Jupyter notebook example showcasing the usage of LlamaBot with the Groq API (2dc21be) (Eric Ma)</li> <li>Renamed and enhanced the PydanticBot example notebook to better reflect its purpose of extracting structured data from unstructured text (bf27391) (Eric Ma)</li> <li>Added unit tests for StructuredBot functionality to ensure it properly returns a Pydantic model when given a specific input (912a76c) (Eric Ma)</li> <li>Restructured bot imports and added StructuredBot to the top-level API (320869d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.5.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected the conditional check in the GitHub Actions workflow for building the devcontainer to use <code>github.repository_owner</code> instead of <code>github.owner</code>, ensuring the build is triggered by the correct repository owner context (a79b4c4) (Eric Ma)</li> <li>Changed the quote style around <code>github.owner</code> condition to maintain consistency across the project (86b1732) (Eric Ma)</li> <li>Fixed the title in the <code>structuredbot.ipynb</code> example to reflect the correct bot name (1f2e2b7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.5.0/#deprecations","title":"Deprecations","text":"<ul> <li>The import paths for ChatBot, QueryBot, SimpleBot, and ImageBot have changed due to the restructuring of bot imports. Users will need to update their import statements to reflect the new structure (320869d) (Eric Ma)</li> </ul> <p>Please note that this release also includes various improvements to the GitHub Actions workflow, such as adding a push event trigger for the main branch, streamlining conditions for the build-container job, and enabling cache for Docker layers to improve build speed.</p>"},{"location":"releases/v0.5.1/","title":"V0.5.1","text":""},{"location":"releases/v0.5.1/#version-051","title":"Version 0.5.1","text":"<p>This release includes several improvements and new features, focusing on enhancing the bot's functionality, flexibility, and user experience.</p>"},{"location":"releases/v0.5.1/#new-features","title":"New Features","text":"<ul> <li>Structured commit message generation and validation: Added support for generating structured commit messages based on diffs and validating their format. (43fa3eec, Eric Ma)</li> <li>Knowledge graph example: Introduced a new Jupyter notebook example demonstrating how to extract knowledge graph triples from text using LlamaBot's StructuredBot. (14a05ed, Eric Ma)</li> <li>New model names and streaming options: Added new model names for broader coverage and introduced a 'none' stream target for silent operation without streaming output. (68430a7, Eric Ma)</li> <li>Pixi environment setup and tests: Added setup and test commands for the Pixi environment, conditional on the repository owner being 'ericmjl'. (f27412f, Eric Ma)</li> <li>Version management in pyproject.toml: Included search and replace patterns for version updates in pyproject.toml to ensure version consistency across the project. (9b21110, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected CLI command in pr-tests workflow: Fixed the CLI command for the Pixi environment to ensure the correct execution of tests. (9c9549b, Eric Ma)</li> <li>Improved dependency installation: Changed the pip install command from -e to . for correct dependency installation. (d9f3c3b, Eric Ma)</li> <li>Updated default model name for commitbot: Changed the default model name from groq/llama-3.1-70b-versatile to gpt-4-turbo. (6644772, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.1/#deprecations","title":"Deprecations","text":"<ul> <li>Removed miniconda environment: Updated the workflow to use only bare and Pixi environments and removed the environment.yml file. (438defd, Eric Ma)</li> <li>Removed Pixi configuration file: Migrated project configuration to pyproject.toml for better modularity and clarity. (de7eb2e, Eric Ma)</li> </ul> <p>Please note that this release also includes various refactorings, code cleanups, and documentation improvements to enhance the overall quality and maintainability of the project.</p>"},{"location":"releases/v0.5.2/","title":"V0.5.2","text":""},{"location":"releases/v0.5.2/#version-052","title":"Version 0.5.2","text":"<p>This release includes several improvements to the build process, testing, and documentation.</p>"},{"location":"releases/v0.5.2/#new-features","title":"New Features","text":"<ul> <li>Dockerfile has been updated to use pixi, which simplifies the build process and reduces dependencies (4ccd5c9) (Eric Ma)</li> <li>The docs build and deploy process has been simplified with the use of pixi run build-docs (04cde63) (Eric Ma)</li> <li>The GitHub workflow for setting up the pixi environment has been updated (a38145f) (Eric Ma)</li> <li>The prepare-commit-msg hook has been improved to handle commit messages more reliably (75dca4c) (Eric Ma)</li> <li>The LlamaBot configuration tutorial has been improved for better readability and clarity (82b1177) (Eric Ma)</li> </ul>"},{"location":"releases/v0.5.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The test_structuredbot.py file has been updated to use pytest and reduce the number of attempts from 50 to 3 (ac5b1aa) (Eric Ma)</li> <li>The git.py file has been fixed to improve the prepare-commit-msg hook and check if a commit message is provided before running llamabot git compose (c5e9fa1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.5.2/#deprecations","title":"Deprecations","text":"<ul> <li>The miniconda setup in the GitHub workflow has been replaced with the pixi environment setup (a38145f) (Eric Ma)</li> <li>The manual build steps in the docs workflow have been replaced with pixi run build-docs (04cde63) (Eric Ma)</li> <li>The mamba env create command has been replaced with pixi install in the Dockerfile (4ccd5c9) (Eric Ma)</li> <li>The deploy directory in the docs workflow has been updated from ./docs to ./site (04cde63) (Eric Ma)</li> <li>The documentation for installing the commit message hook and auto-composing commit messages has been updated (579edc4) (Eric Ma)</li> </ul>"},{"location":"releases/v0.5.3/","title":"V0.5.3","text":""},{"location":"releases/v0.5.3/#version-053","title":"Version 0.5.3","text":"<p>This release includes updates to the commit message format and documentation, as well as dependency updates.</p>"},{"location":"releases/v0.5.3/#new-features","title":"New Features","text":"<ul> <li>The description length limit for commit messages has been increased from 79 characters to 160 characters, and the body length limit has been increased from 6 entries to 10 entries. (cc1f9bd, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The <code>pixi</code> lockfile has been updated. (cf66961, Eric Ma)</li> <li>The <code>chatbot_nb.ipynb</code> file has been updated. (c354319, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.3/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.5.4/","title":"V0.5.4","text":""},{"location":"releases/v0.5.4/#version-054","title":"Version 0.5.4","text":"<p>This release includes several updates to the validation logic for commit messages, as well as new model names for llamabot.</p>"},{"location":"releases/v0.5.4/#new-features","title":"New Features","text":"<ul> <li>The maximum allowed characters for commit message descriptions has been increased from 79 to 160. Scope descriptions can now contain up to two words instead of one, and the one line description limit has been updated back to 79 characters. Additionally, an emoji field has been added to the CommitMessage model and the commit message format has been modified to include emoji. (6dce0c9, Eric Ma)</li> <li>Three new model names have been added to llamabot: bge-large, paraphrase-multilingual, and bge-m3. (585537f, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>No bug fixes in this release.</li> </ul>"},{"location":"releases/v0.5.4/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.5.5/","title":"V0.5.5","text":""},{"location":"releases/v0.5.5/#version-055","title":"Version 0.5.5","text":"<p>This release includes several updates to improve the functionality and usability of the CommitMessage model.</p>"},{"location":"releases/v0.5.5/#new-features","title":"New Features","text":"<ul> <li>The emoji field is now required in the CommitMessage model (780ced7, Eric Ma)</li> <li>The number of attempts for the StructuredBot has been increased from 3 to 10 (cd4e036, Eric Ma)</li> <li>Field descriptions in the CommitMessage model have been updated for clarity (f614de2, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>The 'scope' field description in the CommitMessage model has been simplified and scope validation has been removed (f614de2, Eric Ma)</li> </ul>"},{"location":"releases/v0.5.5/#deprecations","title":"Deprecations","text":"<ul> <li>The validate_scope method, which enforced a two-word limit on the 'scope' field, has been removed (f614de2, Eric Ma)</li> </ul> <p>Note: The 'infra: update pixi lock file' commit (e52c794, Eric Ma) is not included in the release notes as it does not introduce any user-facing changes.</p> <p>Additionally, the version bump commit (ac5bd14, github-actions) is not included in the release notes as it is an automated commit that does not introduce any new features or bug fixes.</p> <p>The 'Add release notes for 0.5.4' commit (81da62c, github-actions) is also not included in the release notes as it is not relevant to the current release.</p>"},{"location":"releases/v0.6.0/","title":"V0.6.0","text":""},{"location":"releases/v0.6.0/#version-060","title":"Version 0.6.0","text":"<p>This release includes several new features, improvements, and bug fixes.</p>"},{"location":"releases/v0.6.0/#new-features","title":"New Features","text":"<ul> <li>Version bump to 0.6.0: The project version has been updated to 0.6.0. (65f92c2) (github-actions)</li> <li>Guided tutorial for <code>llamabot docs write</code> command: A new documentation file has been created for the LlamaBot CLI, including usage instructions, options, and necessary frontmatter key-value pairs for the <code>llamabot docs write</code> command. (8541107) (Eric Ma)</li> <li>Updated SimpleBot tutorial documentation: The SimpleBot tutorial documentation has been updated with detailed sections on the AIMessage object, new import paths, and additional information on using the Panel app with SimpleBot. (84ebf42) (Eric Ma)</li> <li>Updated recording prompts tutorial: The recording prompts tutorial has been revised for better clarity and integration, including a how-to guide format, metadata section, updated import paths, and parameter name changes. (3f494d5) (Eric Ma)</li> <li>Pixi environment setup in GitHub Actions workflow: A new step has been added to the GitHub Actions workflow to set up the Pixi environment, configure Pixi v0.25.0 with caching, and enable cache writing for pushes to the main branch. (94d7165) (Eric Ma)</li> <li>Local hook for pixi installation: A new local hook has been added to the pre-commit configuration to always run and require serial execution for pixi installation. (521bbd3) (Eric Ma)</li> <li>Updated SHA256 hash for llamabot package: The SHA256 hash in the pixi.lock file for the llamabot package has been updated. (16fc384) (Eric Ma)</li> <li>Added python-frontmatter package: The python-frontmatter 1.1.0 package has been added to the project's dependencies and included in the pixi.lock and pyproject.toml files. (555da15) (Eric Ma)</li> <li>New Jupyter notebook for docbot functionality: A new Jupyter notebook has been created to integrate various documentation tools, set up autoreload for dynamic updates, and implement code cells for handling documentation checks and updates. (5a8f84b) (Eric Ma)</li> <li>Added documentation generation command to CLI: A new CLI command has been added for Markdown documentation generation, including a save method in MarkdownSourceFile, refactored documentation_information function, and updates to the write command for handling outdated or empty documentation. (7976e27) (Eric Ma)</li> <li>Updated ChatBot tutorial documentation: The ChatBot tutorial documentation has been revised with detailed steps on using the ChatBot class in a Jupyter notebook, serving a Panel app, and explaining the message retrieval process for API calls. (4e7092e) (Eric Ma)</li> <li>Updated Dockerfile for development environment setup: The Dockerfile has been updated to replace environment.yml with pixi.lock and pyproject.toml, copy docs, llamabot, and tests directories to the container, and update the Ollama installation command with detailed comments. (9bdc226) (Eric Ma)</li> <li>Added a new CLI tool for managing Markdown documentation: A new file llamabot/cli/docs.py has been created to handle Markdown documentation, including the implementation of classes for Markdown source files and documentation status checking, integration of Typer for CLI command handling, and use of pydantic for validation and frontmatter for Markdown metadata. (57a8781) (Eric Ma)</li> <li>Updated development container setup documentation: The documentation for the development container setup has been updated to highlight the influence of devcontainer.json, detail the inclusion of tests and llamabot directories, describe the 'ollama' software installation, explain postCreateCommand and postStartCommand purposes, and clarify troubleshooting for common failure modes. (043fb9d) (Eric Ma)</li> <li>Added curl and build-essential installation commands to Dockerfile: curl has been added to fetch resources from the web, and build-essential has been added for C++ compilation requirements. (67f99e8) (Eric Ma)</li> <li>Updated Docker and GitHub Actions configurations: A .dockerignore file has been added to exclude unnecessary files from the Docker context, and the build-devcontainer.yaml file has been updated to trigger on pull requests to the main branch. (04af9dd) (Eric Ma)</li> </ul>"},{"location":"releases/v0.6.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed model used in SimpleBot for testing: The model used in test_bot.py has been changed from 'ollama/phi3' to 'ollama/gemma2:2b'. (ecdbd58) (Eric Ma)</li> <li>Enhanced input validation in test cases for QueryBot: Regex filters have been added to block repeated characters in test inputs for QueryBot. (ddbb501) (Eric Ma)</li> <li>Updated project dependencies in pyproject.toml: Version constraints have been removed on 'beartype' and 'litellm', and a comment has been added for the runtime dependencies section. (1a124ea) (Eric Ma)</li> <li>Updated sha256 hash for llamabot package: The sha256 hash for llamabot has been updated in the pixi.lock file. (41b2135) (Eric Ma)</li> <li>Fixed pixi.lock file: The pixi lockfile has been regenerated to fix an issue. (cffa639) (Eric Ma)</li> </ul>"},{"location":"releases/v0.6.0/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.6.1/","title":"V0.6.1","text":""},{"location":"releases/v0.6.1/#version-061","title":"Version 0.6.1","text":"<p>This release includes several improvements and updates to the documentation, as well as new functionality for finding prime numbers.</p>"},{"location":"releases/v0.6.1/#new-features","title":"New Features","text":"<ul> <li>Added functionality to find the next prime number. This includes an <code>is_prime</code> function to check primality and a <code>next_prime</code> function to find the next prime number after a given number. (22b73f3) (Eric Ma)</li> <li>Refactored the <code>test_docs</code> module to use an external source file and improve function documentation. This includes using <code>Path</code> for reading external prime number source code and updating docstrings in <code>is_prime</code> and <code>next_prime</code> functions. (196bde2) (Eric Ma)</li> <li>Updated the <code>llamabot</code> documentation to reflect source code changes and add new test cases. This includes updating the documentation and test suite for new source code changes and adding comprehensive test cases for documentation validation. (4308ba3) (Eric Ma)</li> <li>Improved the formatting and clarity of the CLI documentation for the <code>llamabot docs write</code> command. This includes reformatting the <code>--from-scratch</code> flag section for better clarity, updating the frontmatter section to specify YAML format, simplifying the example section with a complete Markdown file, and changing file and intent sections in the Python script for clearer content blocks. (8108712) (Eric Ma)</li> </ul>"},{"location":"releases/v0.6.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue with the <code>llamabot docs write</code> command to ensure content is cleared at the correct stage when writing from scratch. This includes moving content clearing to the start of the write function if <code>from_scratch</code> is true. (e78b3f1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.6.1/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.6.1/#other-changes","title":"Other Changes","text":"<ul> <li>Refactored the <code>llamabot</code> bot and documentation handling. This includes changing the <code>task_message</code> return type to <code>HumanMessage</code> in <code>StructuredBot</code>, substituting <code>DocumentationOutOfDate</code> with <code>ModelValidatorWrapper</code>, adding <code>ood_checker_bot</code> and <code>docwriter_bot</code> functions for bot instance creation, updating the <code>write</code> function to use new bot functions and handle doc updates, and fixing class name typos in test cases to <code>DocsDoNotCoverIntendedMaterial</code>. (805caa6) (Eric Ma)</li> <li>Updated the documentation for the <code>next_prime</code> function. This includes providing a tutorial on how to use the prime number function and explaining any optimizations made in the source code. (99ae504) (Eric Ma)</li> <li>Updated the documentation for the <code>llamabot docs write</code> command. This includes adding an explanation of how linked files are referenced, detailing the mechanism of referencing files in the <code>linked_files</code> key, and providing an example of relative path usage for linked files. (3fe4571) (Eric Ma)</li> <li>Updated the pytest configuration for the <code>test_docs</code> module. This includes changing the pytest marker from 'llm' to 'llm_eval' and updating the pytest addopts in <code>pyproject.toml</code> to use the new 'llm_eval' marker. (22e067d) (Eric Ma)</li> <li>Updated the pytest marker for the <code>test_docs</code> module. This includes changing the pytest marker from 'llm_evals' to 'llm'. (f506b12) (Eric Ma)</li> <li>Bumped version from 0.6.0 to 0.6.1. (52ac0d9) (github-actions)</li> </ul>"},{"location":"releases/v0.6.2/","title":"V0.6.2","text":""},{"location":"releases/v0.6.2/#version-062","title":"Version 0.6.2","text":"<p>This release includes several improvements to the caching mechanism, new features for testing, and a version bump.</p>"},{"location":"releases/v0.6.2/#new-features","title":"New Features","text":"<ul> <li>Implemented caching mechanism across various bot components, including QueryBot, SimpleBot, and StructuredBot. Added a new cache module and updated dependencies to include diskcache. (b7d2b32, Eric Ma)</li> <li>Added deterministic mock response generation for tests. Imported hashlib to use for generating hash-based mock responses and created a new function generate_mock_response to produce unique responses. (4a49925, Eric Ma)</li> </ul>"},{"location":"releases/v0.6.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Improved caching mechanism in SimpleBot methods by adding ignore={0} to @cache.memoize() in key methods to ignore \"self\" positional arg. (621e8e9, Eric Ma)</li> </ul>"},{"location":"releases/v0.6.2/#refactoring","title":"Refactoring","text":"<ul> <li>Refactored caching in SimpleBot methods, removing caching from stream_panel and stream_api methods and adding caching to generate_response method. (f35bab2, Eric Ma)</li> <li>Refactored caching in bot classes, removing redundant caching decorators from QueryBot and StructuredBot and adding caching to various stream methods in SimpleBot to optimize performance. (282efbc, Eric Ma)</li> <li>Refactored test_simple_bot_stream_stdout to use hypothesis strategies more efficiently, replacing multiple given decorators with a single one using st.data() and using st.tuples to draw system_prompt, human_message, and mock_response together. (8b56779, Eric Ma)</li> </ul>"},{"location":"releases/v0.6.2/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.6.3/","title":"V0.6.3","text":""},{"location":"releases/v0.6.3/#version-063","title":"Version 0.6.3","text":"<p>This release includes updates to caching documentation and configuration, as well as improvements to the llamabot feature.</p>"},{"location":"releases/v0.6.3/#new-features","title":"New Features","text":"<ul> <li>Enhanced caching documentation and configuration: Added a caching section in README.md for configuration and environment variable usage, and updated cache.py to set cache timeout based on an environment variable. (103b7af) (Eric Ma)</li> <li>Updated model names and enhanced cache timeout: Added a comprehensive list of model names to ollama_model_names.txt, and set a default cache timeout of 1 day in cache.py. (3af5f09) (Eric Ma)</li> </ul>"},{"location":"releases/v0.6.3/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes in this release.</p>"},{"location":"releases/v0.6.3/#deprecations","title":"Deprecations","text":"<p>No deprecations in this release.</p>"},{"location":"releases/v0.7.0/","title":"V0.7.0","text":""},{"location":"releases/v0.7.0/#version-070","title":"Version 0.7.0","text":"<p>This release includes several enhancements to the LlamaBot CLI, caching, and model names.</p>"},{"location":"releases/v0.7.0/#new-features","title":"New Features","text":"<ul> <li>Better caching: Improved caching functionality for better performance (Merged pull request #96 from ericmjl/better-caching) (8df15b0) (Eric Ma)</li> <li>Updated Git CLI documentation: Revised and structured Git CLI documentation, including a tutorial, getting started section, commands overview, examples, and conclusion (0ced3a0) (Eric Ma)</li> <li>Enhanced release notes generation: Added a console status indicator during release notes generation and changed the stream target from 'stdout' to 'none' (b6465e8) (Eric Ma)</li> <li>Refactored git command handling and error reporting: Replaced Typer with typer, added start_date and end_date parameters for report generation, and updated compose_git_activity_report to accept time period description (047e696) (Eric Ma)</li> <li>Added clipboard support for report output: Enabled copying report content to the clipboard and added user feedback messages (ff13a5c) (Eric Ma)</li> <li>Enhanced llamabot with new model names and reporting features: Added new model names, implemented a new git subcommand to generate reports based on recent activity, and updated llamabot version to 0.6.3 (00052ef) (Eric Ma)</li> <li>Added new model names to ollama_model_names.txt: Included qwen2.5-coder, solar-pro, nemotron-mini, qwen2.5, bespoke-minicheck, and mistral-small to the model list (11d77f0) (Eric Ma)</li> <li>Enhanced caching documentation and configuration: Added a caching section in README.md for configuration and environment variable usage and updated cache.py to set cache timeout based on an environment variable (9b7f339) (Eric Ma)</li> <li>Updated model names and enhanced cache timeout: Added a comprehensive list of model names to ollama_model_names.txt and set a default cache timeout of 1 day in cache.py (786fcea) (Eric Ma)</li> </ul>"},{"location":"releases/v0.7.0/#bug-fixes","title":"Bug Fixes","text":"<p>No bug fixes were included in this release.</p>"},{"location":"releases/v0.7.0/#deprecations","title":"Deprecations","text":"<p>No deprecations were included in this release.</p>"},{"location":"releases/v0.8.0/","title":"V0.8.0","text":""},{"location":"releases/v0.8.0/#version-080","title":"Version 0.8.0","text":"<p>This release includes several new features and improvements to the LlamaBot Notebook CLI.</p>"},{"location":"releases/v0.8.0/#new-features","title":"New Features","text":"<ul> <li>The maximum allowed description length in DescriptionEntry validation has been increased from 79 to 160 characters. The error message has also been updated to reflect the new character limit. (5dea5a5d, Eric Ma)</li> <li>A comprehensive tutorial has been added for using the LlamaBot Notebook CLI. The tutorial explains the benefits and features of the CLI, and includes usage examples and prerequisites. (c5a76c1d, Eric Ma)</li> <li>The project dependencies have been updated to include nbformat. The pixi.lock file has been updated with a new sha256 checksum for llamabot, and nbformat has been added to the required packages in pixi.lock and pyproject.toml. (416d84d, Eric Ma)</li> <li>A new notebook module has been added to the LlamaBot CLI, which includes a new <code>explain</code> command for notebook code cell explanations. The explained notebook can also be saved with an optional overwrite feature. (27ced02, Eric Ma)</li> <li>A new Jupyter notebook has been added for prototyping a notebook code explainer. The notebook includes data analysis and notebook explanation. Additionally, llama3.2 has been added to ollama_model_names.txt, and the llamabot version has been updated from 0.6.3 to 0.7.0 in pixi.lock. (533027e, Eric Ma)</li> </ul>"},{"location":"releases/v0.8.0/#bug-fixes","title":"Bug Fixes","text":"<p>There are no bug fixes in this release.</p>"},{"location":"releases/v0.8.0/#deprecations","title":"Deprecations","text":"<p>There are no deprecations in this release.</p>"},{"location":"releases/v0.8.1/","title":"V0.8.1","text":""},{"location":"releases/v0.8.1/#version-081","title":"Version 0.8.1","text":"<p>This minor release includes important security fixes, refactoring, and enhancements in documentation and coding style guidelines to improve the overall functionality and security of the LlamaBot CLI.</p>"},{"location":"releases/v0.8.1/#new-features","title":"New Features","text":"<ul> <li>Updated coding style guidelines to prefer functional programming and specified Typer for CLI apps and pytest for testing. (a8207cc) (Eric Ma)</li> <li>Enhanced documentation for LlamaBot Notebook CLI, adding a new 'Recommended Usage' section and clarifications on code and markdown cell usage. (56a68ad) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Removed a sensitive API key from the source code to enhance security. (c997861) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.1/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored the default model usage in LlamaBot CLI to replace hardcoded model names with a function call, ensuring consistency across functions. (f36dd14) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.2/","title":"V0.8.2","text":""},{"location":"releases/v0.8.2/#version-082","title":"Version 0.8.2","text":"<p>This release introduces several enhancements to the LlamaBot CLI, including new features for cache management and bot refinement, as well as improvements to the SimpleBot functionality. Additionally, it simplifies the notebook bot implementation and removes outdated resources.</p>"},{"location":"releases/v0.8.2/#new-features","title":"New Features","text":"<ul> <li>Added cache management commands to LlamaBot CLI to handle cache operations more efficiently, including a command to clear the disk cache. (d58f8a) (Eric Ma)</li> <li>Enhanced SimpleBot to better handle o1 models by adding special case handling and removing type hints for increased flexibility. (e3268c) (Eric Ma)</li> <li>Introduced new functions in llamabot/cli/docs to refine documentation generation, including <code>refine_bot</code> and <code>refine_bot_sysprompt</code> for creating structured prompts and configurations. (27a1df) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.2/#refactorings","title":"Refactorings","text":"<ul> <li>Simplified the notebook bot implementation in llamabot/cli by replacing <code>StructuredBot</code> with <code>SimpleBot</code> and streamlining content provision and explanation processes. (a1ae4b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.2/#deprecations","title":"Deprecations","text":"<ul> <li>Removed the explained version of the dummy Jupyter notebook, including all markdown explanations and code cells related to data analysis and visualization. (a9fb7c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.3/","title":"V0.8.3","text":""},{"location":"releases/v0.8.3/#version-083","title":"Version 0.8.3","text":"<p>This release includes refinements to the LlamaBot CLI, enhancing its flexibility and usability by allowing dynamic parameter adjustments and simplifying its internal bot structure.</p>"},{"location":"releases/v0.8.3/#new-features","title":"New Features","text":"<ul> <li>Refactored the documentation bot to use a simpler bot type and added dynamic verbosity control to the writing function. This update makes the bot more adaptable to different verbosity requirements and simplifies its implementation. (fb336b8) (Eric Ma)</li> <li>Enhanced bot functions to accept model names as parameters, allowing for more flexible bot operations across different models. This change improves the modularity and reusability of the bot functions. (9119182) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.8.3/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.8.4/","title":"V0.8.4","text":""},{"location":"releases/v0.8.4/#version-084","title":"Version 0.8.4","text":"<p>This release includes enhancements to the recorder module, improved logging and database integration for the llamabot, and updates to the project's configuration files.</p>"},{"location":"releases/v0.8.4/#new-features","title":"New Features","text":"<ul> <li>Enhanced the recorder module with additional database columns for storing model names and temperature, and improved error handling during database upgrades. (92aef9) (Eric Ma)</li> <li>Enhanced logging and database integration in llamabot, including new model names and refactored methods for better integration. (f4403c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.8.4/#deprecations","title":"Deprecations","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.8.5/","title":"V0.8.5","text":""},{"location":"releases/v0.8.5/#version-085","title":"Version 0.8.5","text":"<p>This release introduces new features to enhance the llamabot project, including a new visualization command for the CLI and updates to dependencies. It also includes several bug fixes and improvements in dependency management.</p>"},{"location":"releases/v0.8.5/#new-features","title":"New Features","text":"<ul> <li>Added a visualization command to the CLI to launch a web app for visualizing prompts and messages. This includes a new module and updates to dependencies to support the feature. (2afba4) (Eric Ma)</li> <li>Introduced a web module for llamabot, setting up the initial structure and necessary configurations for web app development. (6957ea) (Eric Ma)</li> <li>Added 'sentence-transformers' to the project's dependencies to enhance natural language processing capabilities. (64cae0) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Downgraded numpy version to 1.26.4 across various platforms to address compatibility issues. (3c1ce3) (Eric Ma)</li> <li>Updated the sha256 hash for the llamabot package in pixi.lock to ensure integrity and correctness. (69f155) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.5/#dependency-management","title":"Dependency Management","text":"<ul> <li>Removed unused dependencies from the project to streamline the setup and reduce potential conflicts. (5ebe4a) (Eric Ma)</li> <li>Relaxed version constraints on several dependencies to increase compatibility with other packages and reduce potential conflicts during dependency resolution. (4250a9) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.6/","title":"V0.8.6","text":""},{"location":"releases/v0.8.6/#version-086","title":"Version 0.8.6","text":"<p>This release introduces enhancements to the log viewer functionality, improving both the user interface and the interaction capabilities.</p>"},{"location":"releases/v0.8.6/#new-features","title":"New Features","text":"<ul> <li>Enhanced log viewer with message preview and full content display, alongside dynamic client-side filtering. This update makes it easier to navigate and analyze log data. (689d46a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.6/#bug-fixes","title":"Bug Fixes","text":"<p>None in this release.</p>"},{"location":"releases/v0.8.6/#deprecations","title":"Deprecations","text":"<p>None in this release.</p>"},{"location":"releases/v0.8.7/","title":"V0.8.7","text":""},{"location":"releases/v0.8.7/#version-087","title":"Version 0.8.7","text":"<p>This release includes improvements to the stability and reliability of database operations within the llamabot project.</p>"},{"location":"releases/v0.8.7/#new-features","title":"New Features","text":"<ul> <li>Added a retry mechanism with exponential backoff to the SQLite logging function to handle transient database errors more effectively. (4b57a2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.7/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.8.7/#deprecations","title":"Deprecations","text":""},{"location":"releases/v0.8.8/","title":"V0.8.8","text":""},{"location":"releases/v0.8.8/#version-088","title":"Version 0.8.8","text":"<p>This release includes updates to dependencies, enhancements to our GitHub Actions and Dependabot configurations, and a refactor of the <code>create_app</code> function to support an optional database path.</p>"},{"location":"releases/v0.8.8/#new-features","title":"New Features","text":"<ul> <li>Upgraded GitHub Actions checkout step from version 2 to version 4 in the build-devcontainer workflow, and added a new Dependabot configuration file for weekly updates of GitHub Actions. (89b673) (Eric Ma)</li> <li>Refactored the <code>create_app</code> function to accept an optional <code>db_path</code> parameter, improving flexibility in database management. (ae677c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated llamabot to version 0.8.7 and adjusted the SHA256 hash to match the new package version, ensuring compatibility and security. (c916fb) (Eric Ma)</li> </ul>"},{"location":"releases/v0.8.8/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.0/","title":"V0.9.0","text":""},{"location":"releases/v0.9.0/#version-090","title":"Version 0.9.0","text":"<p>This release introduces significant enhancements to the llamabot, including improved message handling, version control for prompts, and updates to the web interface. Additionally, the underlying dependencies have been updated to their latest major versions.</p>"},{"location":"releases/v0.9.0/#new-features","title":"New Features","text":"<ul> <li>Enhanced SimpleBot to accept both string and BaseMessage types, automatically converting string inputs to HumanMessage. (7abade) (Eric Ma)</li> <li>Added version control to prompts, allowing for better management and tracking of changes. (ace03f) (Eric Ma)</li> <li>Enhanced logging and display features in the web interface, including better traceability and dynamic content styling. (2b9bee) (Eric Ma)</li> <li>Added conventional commit message formatting to the commitbot in the CLI tool. (2647e4) (Eric Ma)</li> <li>Enhanced message content display in Llamabot by changing the message content element to preserve formatting and improve readability. (d3f1fa) (Eric Ma)</li> <li>Enhanced the message log UI with prompt template modals and improved link interactions for a more informative user experience. (7105ce) (Eric Ma)</li> <li>Refactored logging and prompt handling in llamabot to enhance functionality and maintain backward compatibility. (a68495) (Eric Ma)</li> <li>Enhanced the version_prompt and store_prompt_version functions to include the function name, improving traceability. (5199ee) (Eric Ma)</li> <li>Added utility function to retrieve the name of an object as defined in the current namespace, enhancing modularity and reusability. (8ff1ce) (Eric Ma)</li> <li>Ensured the database is properly initialized and upgraded upon application start, supporting robust data management. (ee1d9f) (Eric Ma)</li> <li>Enhanced the database schema and logging capabilities, including the introduction of version-controlled prompt templates. (f32282) (Eric Ma)</li> <li>Enhanced the prompt manager with version control and database integration, streamlining prompt management and storage. (e669ef) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.0/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated llamabot package version and corresponding hash in pixi.lock to ensure consistency with dependencies. (f93c43) (Eric Ma)</li> <li>Enhanced HTML escaping in JavaScript to include additional characters, improving security against injection attacks. (ec4a4e) (Eric Ma)</li> <li>Ensured correct content comparison in blog test, fixing issues with inaccurate test validations. (99ee86) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.0/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Bumped actions/checkout from version 2 to 4, ensuring compatibility with latest workflows. (56c9d7) (dependabot[bot])</li> <li>Bumped codecov/codecov-action from version 2 to 4, updating to the latest version for improved code coverage reporting. (891358) (dependabot[bot])</li> <li>Bumped peaceiris/actions-gh-pages from version 3 to 4, aligning with the latest GitHub Pages actions. (955f21) (dependabot[bot])</li> <li>Bumped actions/setup-python from version 3 to 5, supporting the latest Python setups in workflows. (0a8bb8) (dependabot[bot])</li> <li>Bumped pre-commit/action from version 2.0.0 to 3.0.1, enhancing pre-commit checks with the latest features. (ff4535) (dependabot[bot])</li> </ul>"},{"location":"releases/v0.9.1/","title":"V0.9.1","text":""},{"location":"releases/v0.9.1/#version-091","title":"Version 0.9.1","text":"<p>This release includes an update to dependency management, enhancing the software's compatibility and performance.</p>"},{"location":"releases/v0.9.1/#new-features","title":"New Features","text":"<ul> <li>Updated llamabot to version 0.9.0 and adjusted Python version requirements to support up to Python 3.12, ensuring better compatibility and stability with newer technology stacks. (0d1e99d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.1/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.10/","title":"V0.9.10","text":""},{"location":"releases/v0.9.10/#version-0910","title":"Version 0.9.10","text":"<p>This release introduces enhancements to the visualization and server configuration of Llamabot, along with updates to the UI components for better usability and organization of model names.</p>"},{"location":"releases/v0.9.10/#new-features","title":"New Features","text":"<ul> <li>Enhanced visualization and server configuration options in Llamabot, including new model names, host and port configuration, and improved prompt history retrieval and display. (3fa3854) (Eric Ma)</li> <li>Updated the model names list in Llamabot to include new models and improve list organization. (a3ce1c5) (Eric Ma)</li> <li>Refactored UI components to improve usability, implemented Bootstrap for better tab and log display, and enhanced responsiveness and styling of the log viewer and prompt comparison sections. (de49cae) (Eric Ma)</li> <li>Refactored prompt handling and display in the Llamabot web app, including changes to how prompt history loads and how version counts are displayed. (486f04d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.10/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None listed for this release.</li> </ul>"},{"location":"releases/v0.9.10/#deprecations","title":"Deprecations","text":"<ul> <li>None listed for this release.</li> </ul>"},{"location":"releases/v0.9.11/","title":"V0.9.11","text":""},{"location":"releases/v0.9.11/#version-0911","title":"Version 0.9.11","text":"<p>This release includes several enhancements and refinements to the llamabot's database and API handling, improving efficiency and clarity. New features for version tracking and database integration tests have been added, along with a new YAML log export functionality.</p>"},{"location":"releases/v0.9.11/#new-features","title":"New Features","text":"<ul> <li>Enhanced version_prompt to handle version history, allowing for better tracking of changes in prompt templates. (c64dd52) (Eric Ma)</li> <li>Added version tracking to prompts in the database, linking previous versions directly to facilitate better historical data management. (1f3ca10) (Eric Ma)</li> <li>Implemented database integration tests for the version_prompt function to ensure robust handling of prompt creation, updates, and duplicates. (9e3c653) (Eric Ma)</li> <li>Introduced YAML log export functionality, enabling users to export and download log data in YAML format directly from the web interface. (6f0e590) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.11/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated llamabot package to fix dependencies and ensure compatibility with the latest changes. (a1ece6a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.11/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored database and API handling in llamabot for improved efficiency and clarity, including updates to data structures and error handling mechanisms. (1083c05) (Eric Ma)</li> <li>Refactored internal storage variable names in PromptRecorder and related methods to better reflect their usage and simplify the codebase. (fa7ec3c) (Eric Ma)</li> <li>Refactored version_prompt to support an optional database path and enhanced logging, making the function more flexible and easier to debug. (43bc5e4) (Eric Ma)</li> </ul> <p>This version continues to build on the robustness and usability of the llamabot, making it more efficient and user-friendly.</p>"},{"location":"releases/v0.9.12/","title":"V0.9.12","text":""},{"location":"releases/v0.9.12/#version-0912","title":"Version 0.9.12","text":"<p>This release introduces enhancements to the QueryBot, including new integrations and refactoring, along with a critical bug fix to improve the stability and functionality of the bot.</p>"},{"location":"releases/v0.9.12/#new-features","title":"New Features","text":"<ul> <li>Added SQLiteVecDocStore integration and updated QueryBot to support various document stores, enhancing the bot's flexibility and capability in handling different storage solutions. This update also includes new tests to verify the semantic search functionalities with SQLiteVecDocStore. (2f08b63) (Eric Ma)</li> <li>Introduced SQLiteVecDocStore class to provide efficient document storage and retrieval using the sqlite-vec library, which has been added as a new dependency. This feature aims to optimize document handling within the bot framework. (57a354c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.12/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the reset method call for the QueryBot instance to correctly reset the bot's document store, ensuring proper functionality during reinitializations. (6632db8) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.12/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored the handling of collection names in QueryBot by standardizing the slugification process during initialization, which removed redundant slugify calls in document store initializations. This change simplifies the codebase and improves maintainability. (5e41d19) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.13/","title":"V0.9.13","text":""},{"location":"releases/v0.9.13/#version-0913","title":"Version 0.9.13","text":"<p>This release includes several enhancements and new features aimed at improving the functionality and user experience of the llamabot application, particularly in the web interface and log viewing components.</p>"},{"location":"releases/v0.9.13/#new-features","title":"New Features","text":"<ul> <li>Added interactive rating buttons to llamabot response templates for user feedback on log entries. (8bfb74a) (Eric Ma)</li> <li>Enhanced UI with collapsible message headers and system message styling for better readability and interaction. (d2ccff6) (Eric Ma)</li> <li>Introduced modal functionality to display detailed prompt information in the llamabot web interface. (f8b5769) (Eric Ma)</li> <li>Added endpoint to rate log entries, allowing users to provide feedback on the helpfulness of logs. (6ac2013) (Eric Ma)</li> <li>Enhanced log details rendering and data structure for improved clarity and user interaction. (5b3d842) (Eric Ma)</li> <li>Implemented HTMX + FastAPI endpoints for dynamic content updates without full page reloads, enhancing the responsiveness of the web interface. (c8f1a4a) (Eric Ma)</li> <li>Added a new 'rating' column to the MessageLog model to store user ratings, improving data tracking and user interaction. (425ec07) (Eric Ma)</li> <li>Refactored QueryBot to remove support for SQLiteVecDocStore, streamlining the codebase and focusing on more efficient data handling. (a243878) (Eric Ma)</li> <li>Added tests for SQLiteVecDocStore functionalities to ensure robustness and reliability. (f7d789d) (Eric Ma)</li> <li>Refactored log viewing functionality in llamabot, including renaming and reorganizing files for better clarity and maintenance. (c2baa13) (Eric Ma)</li> <li>Added the tuna package to project dependencies to enhance functionality and performance in development environments. (bc9acb2) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.13/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed dev container setup to improve development environment setup and reliability. (09c5c86) (Amir Molavi)</li> </ul>"},{"location":"releases/v0.9.13/#deprecations","title":"Deprecations","text":"<ul> <li>Removed support for SQLiteVecDocStore in QueryBot, marking a shift towards more efficient and scalable data handling solutions. (a243878) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.14/","title":"V0.9.14","text":""},{"location":"releases/v0.9.14/#version-0914","title":"Version 0.9.14","text":"<p>This release includes enhancements to the export functionality and UI interactions in llamabot, updates to VSCode settings, and mandatory API endpoint parameters, along with comprehensive tests for the FastAPI app endpoints.</p>"},{"location":"releases/v0.9.14/#new-features","title":"New Features","text":"<ul> <li>Enhanced export functionality with support for multiple formats and improved UI interactions, including new export options and interactive elements. (d5f64f) (Eric Ma)</li> <li>Added comprehensive tests for FastAPI app endpoints, updated the 'get_log' API endpoint to make the 'expanded' parameter mandatory, and enhanced VSCode settings with new color customizations. (0652d7) (Eric Ma)</li> <li>Enhanced export functionality and improved UI interactions in the llamabot/web module. (c01ebb) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.14/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated llamabot package to version 0.9.13, including an update to the SHA256 hash for the new package version. (039ea1) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.14/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.15/","title":"V0.9.15","text":""},{"location":"releases/v0.9.15/#version-0915","title":"Version 0.9.15","text":"<p>This release introduces a new model to the Llamabot and includes various refinements to the application's functionality and model management.</p>"},{"location":"releases/v0.9.15/#new-features","title":"New Features","text":"<ul> <li>Added a new model named llama3.2-vision to enhance the capabilities of Llamabot. (1f15d1) (Eric Ma)</li> <li>Refactored the handling and naming of models in Llamabot for better organization and efficiency. (ab6be3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.15/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.15/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.16/","title":"V0.9.16","text":""},{"location":"releases/v0.9.16/#version-0916","title":"Version 0.9.16","text":"<p>This release includes updates to the llamabot version across multiple files, ensuring users have the latest features and improvements.</p>"},{"location":"releases/v0.9.16/#new-features","title":"New Features","text":"<ul> <li>Updated llamabot version to 0.9.15 in various configuration and documentation files to reflect the latest release. This includes updates to the installation commands and package version details. (c8fcf7) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.16/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.16/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.16/#version-0915","title":"Version 0.9.15","text":"<p>Release notes for this version were prepared but specific feature details are not provided in the provided commit logs.</p>"},{"location":"releases/v0.9.16/#new-features_1","title":"New Features","text":"<ul> <li>Release notes were added for better tracking of changes and updates. (9b808a) (github-actions)</li> </ul>"},{"location":"releases/v0.9.16/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.16/#deprecations_1","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.17/","title":"V0.9.17","text":""},{"location":"releases/v0.9.17/#version-0917","title":"Version 0.9.17","text":"<p>This release includes several enhancements and updates to the llamabot web application, focusing on improving the functionality of experiment tracking, prompt management, and web routing. Additionally, there are updates to the testing framework and documentation to support these changes.</p>"},{"location":"releases/v0.9.17/#new-features","title":"New Features","text":"<ul> <li>Introduced a new top-level API module for routers in the llamabot project, simplifying the integration and scalability of new routing functionalities. (2420e9) (Eric Ma)</li> <li>Added a new Runs model to efficiently record and manage experiment runs, integrating it with the existing database and endpoint structures for better data handling. (28fc7b) (Eric Ma)</li> <li>Enhanced the web application by adding an experiments module, which includes endpoints for managing machine learning experiment runs and integrating experiment data into the main index page for improved navigation. (c5e810) (Eric Ma)</li> <li>Improved the display of prompt functions in the web interface by showing version counts next to each function, enhancing user experience and information accessibility. (bd7bba) (Eric Ma)</li> <li>Enhanced the prompt functions endpoint to utilize templates, improving the maintainability and scalability of the web application's UI components. (c86b7d) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.17/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue in the test suite to ensure the prompt hash is correctly recognized as a string, enhancing the reliability of prompt tracking tests. (da09ff) (Eric Ma)</li> <li>Addressed a cleanup issue in the Experiment class to prevent dangling references after session cleanup, ensuring cleaner and safer memory management. (ba430a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.17/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored the llamabot's database columns and query methods to standardize data handling and improve compatibility with various database operations. (c16c32) (Eric Ma)</li> <li>Overhauled the web application structure and routing, extracting database operations into a separate module and organizing endpoints more logically. (8f642b) (Eric Ma)</li> <li>Removed the dependency on prompt ID in prompt handling, simplifying the method requirements and enhancing the flexibility of prompt management. (e1e418) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.17/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Updated the documentation to reflect new development setup instructions and dependency information, ensuring users and developers have the latest guidance for setting up their development environment. (d776e3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.17/#dependency-updates","title":"Dependency Updates","text":"<ul> <li>Updated package version constraints and added python-multipart to dependencies to ensure compatibility and extend functionality. (b80b48) (Eric Ma)</li> </ul> <p>This version marks a significant improvement in the llamabot's functionality and user interface, making it more robust, user-friendly, and easier to maintain.</p>"},{"location":"releases/v0.9.18/","title":"V0.9.18","text":""},{"location":"releases/v0.9.18/#version-0918","title":"Version 0.9.18","text":"<p>This release includes several refinements in the documentation deployment process, enhancements in the CI workflows, and significant refactoring in the prompt management and llamabot functionalities to improve performance and maintainability.</p>"},{"location":"releases/v0.9.18/#new-features","title":"New Features","text":"<ul> <li>Added a comprehensive tutorial for using the LlamaBot log viewer, detailing the interface and usage with practical examples. (8216059) (Eric Ma)</li> <li>Introduced a new section in the README to guide users on recording prompt experiments locally, complete with example code and visual aids. (41eca24) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.18/#refactorings","title":"Refactorings","text":"<ul> <li>Refactored prompt hashing and docstring handling in the prompt manager to delay hash computation and enhance performance. (7b28762) (Eric Ma)</li> <li>Transformed the prompt function into a class within llamabot to better encapsulate role handling and improve the structure. (4a914ad) (Eric Ma)</li> <li>Separated the download and execution steps of the Ollama installation in the CI workflow for clarity and maintainability. (592f064) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.18/#continuous-integration-ci-improvements","title":"Continuous Integration (CI) Improvements","text":"<ul> <li>Updated GitHub Actions workflow to only deploy documentation when on the main branch and refined PR preview deployment conditions. (7a9349a, 51958ab, 8f0a43c) (Eric Ma)</li> <li>Enhanced CI workflow for PR tests by ensuring necessary tools like curl are installed before proceeding with further steps. (8eb5016, bcf2d98) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.18/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the llamabot package version in pixi.lock to ensure compatibility and security. (734d352) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.18/#documentation","title":"Documentation","text":"<ul> <li>Updated GitHub Actions workflow configurations for more efficient documentation deployment. (7a9349a, 51958ab, 8f0a43c) (Eric Ma)</li> <li>Added detailed documentation for the Experiment context manager in the README to assist users in managing prompt experiments. (41eca24) (Eric Ma)</li> </ul> <p>This version continues to refine the user experience and developer workflows, ensuring both efficiency and ease of use in managing and deploying the application.</p>"},{"location":"releases/v0.9.19/","title":"V0.9.19","text":""},{"location":"releases/v0.9.19/#version-0919","title":"Version 0.9.19","text":"<p>This release includes significant refactoring improvements to the LlamaBot, enhancing the setup of logging and database paths. These changes streamline the codebase and improve the maintainability of the system.</p>"},{"location":"releases/v0.9.19/#new-features","title":"New Features","text":"<ul> <li>Centralized the database path resolution logic to reduce redundancy and improve maintainability. This update ensures that all modules uniformly manage database paths through a single utility function, enhancing the code's cleanliness and reducing potential errors. (0ece15b) (Eric Ma)</li> <li>Implemented dynamic log level setting based on an environment variable and ensured the creation of necessary directories for database logging. This feature allows for more flexible and environment-specific logging configurations, which can be crucial for debugging and monitoring the application in different deployment scenarios. (2dba66b) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.19/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.19/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.2/","title":"V0.9.2","text":""},{"location":"releases/v0.9.2/#version-092","title":"Version 0.9.2","text":"<p>This release includes enhancements to the documentation generation for llamabot, improving the clarity and usability of the documentation provided to users.</p>"},{"location":"releases/v0.9.2/#new-features","title":"New Features","text":"<ul> <li>Enhanced documentation generation by adding a new 'Instructions' section to guide users and refining the logic to exclude instructional text from processing input. This update ensures clearer and more user-friendly documentation. (99da16e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.2/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.2/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.20/","title":"V0.9.20","text":""},{"location":"releases/v0.9.20/#version-0920","title":"Version 0.9.20","text":"<p>This release includes enhancements to the documentation system, improvements in development container setup, and a refactor in test directory handling.</p>"},{"location":"releases/v0.9.20/#new-features","title":"New Features","text":"<ul> <li>Enhanced the documentation system by integrating the Diataxis framework, which now supports different documentation types and sources, improving the structure and accessibility of project documentation. (687ff8) (Eric Ma)</li> <li>Updated the development container documentation to be more comprehensive and revised the Dockerfile to remove unnecessary whitespace, making the development environment setup clearer and more efficient. (f34e12) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.20/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.20/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.20/#refactors","title":"Refactors","text":"<ul> <li>Refactored the handling of temporary directories in tests to use <code>tempfile.TemporaryDirectory</code> for better management and reliability. (e71b92) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.21/","title":"V0.9.21","text":""},{"location":"releases/v0.9.21/#version-0921","title":"Version 0.9.21","text":"<p>This release introduces significant enhancements to the llamabot, including new functionalities for message handling and API interactions, as well as updates to dependencies and testing procedures.</p>"},{"location":"releases/v0.9.21/#new-features","title":"New Features","text":"<ul> <li>Enhanced llamabot with new message creation functions and bot classes to handle various types of interactions, including text, images, and URLs. This update also includes high-level API functions for creating user and system messages, and improvements to SimpleBot and StructuredBot for better interaction flow. New notebook examples demonstrate the usage of vision models and structured data extraction. (c1d9eba) (Eric Ma)</li> <li>Updated the HTTPX library to version 0.28.0 and added new model names to llamabot, such as llama3.3, snowflake-arctic-embed2, and sailor2. HTTPX has also been added as a dependency in the pyproject.toml and llamabot package. (8f3ba5f) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.21/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed an issue in the test prompt manager to ensure correct handling of base64 encoded image content in tests. (38d1b49) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.21/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.9.3/","title":"V0.9.3","text":""},{"location":"releases/v0.9.3/#version-093","title":"Version 0.9.3","text":"<p>This release introduces enhancements to the BaseMessage class by adding string addition operations, improving the flexibility and usability of message handling in the llamabot components.</p>"},{"location":"releases/v0.9.3/#new-features","title":"New Features","text":"<ul> <li>Added the ability to append strings to the beginning and end of BaseMessage content. This update includes the implementation of <code>__add__</code> for left addition and <code>__radd__</code> for right addition of strings, allowing for more intuitive message construction. (5097fe8) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.3/#deprecations","title":"Deprecations","text":"<ul> <li>None in this release.</li> </ul>"},{"location":"releases/v0.9.4/","title":"V0.9.4","text":""},{"location":"releases/v0.9.4/#version-094","title":"Version 0.9.4","text":"<p>This release includes improvements to the llamabot's message handling capabilities and updates to the package version management.</p>"},{"location":"releases/v0.9.4/#new-features","title":"New Features","text":"<ul> <li>Enhanced llamabot to better handle system prompts and manage responses using the new AIMessage attribute. This update ensures more robust content management within the bot's operations. (e434beb) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.4/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.9.4/#deprecations","title":"Deprecations","text":"<ul> <li>None reported in this release.</li> </ul>"},{"location":"releases/v0.9.5/","title":"V0.9.5","text":""},{"location":"releases/v0.9.5/#version-095","title":"Version 0.9.5","text":"<p>This release introduces a new optional parameter for the database path in the launch command, enhancing flexibility and usability.</p>"},{"location":"releases/v0.9.5/#new-features","title":"New Features","text":"<ul> <li>Added an optional database path parameter to the <code>launch</code> command in Llamabot CLI, allowing users to specify a custom path for the database. If not provided, a default path is used. (d6fb2a) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.5/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.5/#deprecations","title":"Deprecations","text":"<ul> <li>None</li> </ul>"},{"location":"releases/v0.9.6/","title":"V0.9.6","text":""},{"location":"releases/v0.9.6/#version-096","title":"Version 0.9.6","text":"<p>This release includes several refinements and documentation updates, enhancing the overall stability and usability of the software.</p>"},{"location":"releases/v0.9.6/#new-features","title":"New Features","text":"<ul> <li>Updated the documentation to include a new section on general coding rules, emphasizing the importance of adding tests with code changes. (f6be27f) (Eric Ma)</li> <li>Refactored the way template directory paths are resolved to improve reliability using the Path library. (36b47bb) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.6/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Updated the llamabot package version in pixi.lock to ensure compatibility and security with the latest version. (215caa3) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.6/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Updated the model provider prefix in the README documentation to reflect the new naming convention, enhancing clarity in code examples. (c1bdeac) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.6/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Prepared and added release notes for the previous version (0.9.5). (8dec104) (github-actions)</li> <li>Bumped the software version from 0.9.5 to 0.9.6 to mark the new release. (9842e45) (github-actions)</li> </ul>"},{"location":"releases/v0.9.7/","title":"V0.9.7","text":""},{"location":"releases/v0.9.7/#version-097","title":"Version 0.9.7","text":"<p>This release includes improvements in the application's handling of static files, enhancing the flexibility and reliability of resource management.</p>"},{"location":"releases/v0.9.7/#new-features","title":"New Features","text":"<ul> <li>Improved resolution of static file paths to dynamically adapt to different environments, ensuring more robust and flexible file handling. (9be73e) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.7/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.9.7/#deprecations","title":"Deprecations","text":"<p>Please note that this release does not include any bug fixes or deprecations.</p>"},{"location":"releases/v0.9.8/","title":"V0.9.8","text":""},{"location":"releases/v0.9.8/#version-098","title":"Version 0.9.8","text":"<p>This release includes several enhancements and bug fixes to the GitHub Actions workflow for Python package releases, improving automation and build processes.</p>"},{"location":"releases/v0.9.8/#new-features","title":"New Features","text":"<ul> <li>Introduced a new environment variable <code>DEFAULT_VERSION_NAME</code> to specify default versioning, and modified versioning steps to utilize this variable. (cc43046) (Eric Ma)</li> <li>Added a new step to install the llamabot package before writing release notes, and conditionally execute steps for writing and committing release notes based on the trigger event. (23e54ce) (Eric Ma)</li> <li>Enhanced the GitHub Actions workflow by adding a pull request trigger for all branches, introducing the <code>UV_SYSTEM_PYTHON</code> environment variable, and replacing direct Python and pip usage with the uv tool. (0ef659a) (Eric Ma)</li> <li>Updated the publishing step in the CI workflow to use the uv publish command instead of the previous pypa gh-action. (7218b62) (Eric Ma)</li> <li>Added conditional dry run prefix to the job name in the GitHub Actions workflow based on the event type. (312b959) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.8/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Corrected the Python build command in the GitHub Actions workflow by removing an unnecessary prefix. (8cae44d) (Eric Ma)</li> <li>Removed caching from the uv setup in the GitHub Actions workflow to potentially resolve issues with environment caching. (f06a2ef) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.8/#deprecations","title":"Deprecations","text":"<ul> <li>No deprecations in this release.</li> </ul>"},{"location":"releases/v0.9.9/","title":"V0.9.9","text":""},{"location":"releases/v0.9.9/#version-099","title":"Version 0.9.9","text":"<p>This release includes packaging enhancements for the llamabot project, ensuring that static assets and templates are included in the distribution package.</p>"},{"location":"releases/v0.9.9/#new-features","title":"New Features","text":"<ul> <li>Included static assets and templates in the distribution package to ensure they are available when the package is used. (aa593c) (Eric Ma)</li> </ul>"},{"location":"releases/v0.9.9/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.9.9/#deprecations","title":"Deprecations","text":"<p>Please note that the release notes for version 0.9.8 were prepared but not detailed in the provided commit logs. For comprehensive details on what was included in version 0.9.8, please refer to the specific release documentation or the commit tagged <code>e46d7b</code>.</p>"},{"location":"tutorials/agentbot/","title":"AgentBot Tutorial","text":"<p>Welcome to the AgentBot tutorial! In this tutorial, we will guide you through the process of building an agent that uses PocketFlow for graph-based tool orchestration. AgentBot automatically wraps your functions as tools and uses a decision node to orchestrate tool execution through a flow graph.</p>"},{"location":"tutorials/agentbot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Basic knowledge of Python programming</li> <li>Familiarity with the concept of bots and automation</li> <li>Access to a Python environment with the necessary libraries installed</li> </ul>"},{"location":"tutorials/agentbot/#what-is-agentbot","title":"What is AgentBot?","text":"<p>AgentBot is a graph-based agent that uses PocketFlow to orchestrate tool execution. Unlike traditional agents that use loops, AgentBot builds a flow graph where:</p> <ol> <li>A decision node (DecideNode) analyzes the conversation and    selects which tool to execute</li> <li>Tool nodes execute the selected tools</li> <li>Tools can loop back to the decision node (except terminal tools    like <code>respond_to_user</code>)</li> <li>The flow continues until a terminal node is reached</li> </ol> <p>This graph-based approach provides:</p> <ul> <li>Visual flow representation: You can visualize the agent's flow   graph</li> <li>Flexible orchestration: Tools are connected in a graph, not a   linear sequence</li> <li>Automatic tool wrapping: You provide plain callables; AgentBot   handles the rest</li> <li>Default tools: <code>today_date</code>, <code>respond_to_user</code>, and <code>return_object_to_user</code> are always   available</li> </ul>"},{"location":"tutorials/agentbot/#part-1-basic-usage","title":"Part 1: Basic Usage","text":""},{"location":"tutorials/agentbot/#step-1-setting-up-the-environment","title":"Step 1: Setting Up the Environment","text":"<p>First, ensure you have the <code>llamabot</code> library installed:</p> <pre><code>pip install llamabot\n</code></pre>"},{"location":"tutorials/agentbot/#step-2-creating-a-simple-agentbot","title":"Step 2: Creating a Simple AgentBot","text":"<p>The simplest way to create an AgentBot is to provide a list of callable functions:</p> <pre><code>import llamabot as lmb\n\n@lmb.tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the current weather for a city.\n\n    :param city: The name of the city\n    :return: Weather information\n    \"\"\"\n    # In practice, you'd call a real weather API\n    return f\"The weather in {city} is sunny, 72\u00b0F\"\n\n# Create an AgentBot with your function\nagent = lmb.AgentBot(\n    tools=[get_weather],\n    model_name=\"gpt-4o-mini\"\n)\n\n# Use the agent\nresult = agent(\"What's the weather in New York?\")\nprint(result)\n</code></pre> <p>What happens behind the scenes:</p> <ol> <li>The function <code>get_weather</code> should be decorated with <code>@tool</code></li> <li>Default tools (<code>today_date</code>, <code>respond_to_user</code>, <code>return_object_to_user</code>, and <code>inspect_globals</code>) are added    automatically</li> <li>A <code>DecideNode</code> is created to decide which tool to use</li> <li>The flow graph is built connecting the decision node to all tools</li> <li>When you call the agent, it runs the flow with your query</li> </ol>"},{"location":"tutorials/agentbot/#step-3-understanding-default-tools","title":"Step 3: Understanding Default Tools","text":"<p>AgentBot always includes three default tools:</p> <ul> <li><code>today_date</code>: Returns the current date (loops back to decide   node)</li> <li><code>respond_to_user</code>: Sends a text response to the user (terminal node,   no loopback)</li> <li><code>return_object_to_user</code>: Returns an object from the calling context's globals   (terminal node, no loopback). Use this when you want to return actual Python   objects like DataFrames, lists, or dictionaries.</li> </ul> <p>These tools are automatically available, so you don't need to provide them:</p> <pre><code>agent = lmb.AgentBot(tools=[])\n\n# The agent can still use today_date, respond_to_user, and return_object_to_user\nresult = agent(\"What's today's date?\")\n</code></pre> <p>When to use <code>respond_to_user</code> vs <code>return_object_to_user</code>:</p> <ul> <li>Use <code>respond_to_user</code> for text responses, explanations, or conversational replies</li> <li>Use <code>return_object_to_user</code> when you want to return actual Python objects (DataFrames,   lists, dicts, etc.) from your notebook's or script's globals</li> </ul> <p>Using <code>return_object_to_user</code> with globals:</p> <p>To enable <code>return_object_to_user</code> to access variables from your calling context, pass <code>globals_dict</code> when calling the agent:</p> <pre><code>import pandas as pd\n\n# Create some data in your notebook/script\ndf = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n\nagent = lmb.AgentBot(tools=[])\n\n# Pass globals() so the agent can access 'df'\nresult = agent(\"Return the dataframe\", globals_dict=globals())\n\n# result will be the DataFrame object\nprint(result)\n</code></pre> <p>The agent can now use <code>return_object_to_user</code> to return objects from your globals dictionary.</p> <p>Fuzzy Variable Name Matching:</p> <p>The agent can intelligently match partial variable names. For example, if you have a variable named <code>ic50_data_with_confounders</code> in your globals, you can ask for it using a shorter name:</p> <pre><code># Variable in globals\nic50_data_with_confounders = pd.read_csv(\"data.csv\")\n\nagent = lmb.AgentBot(tools=[])\n\n# You can use a partial name - the agent will match it intelligently\nresult = agent(\"show me ic50\", globals_dict=globals())\n\n# The agent will match \"ic50\" to \"ic50_data_with_confounders\" and return it\n</code></pre> <p>The agent sees all available variables in globals and can match partial names based on context and similarity, making it easier to access your data without typing full variable names.</p>"},{"location":"tutorials/agentbot/#part-2-building-a-financial-analysis-agent","title":"Part 2: Building a Financial Analysis Agent","text":"<p>Let's create a more sophisticated agent that can analyze financial data using multiple tools.</p>"},{"location":"tutorials/agentbot/#step-1-defining-custom-tools","title":"Step 1: Defining Custom Tools","text":"<p>You can define tools as plain Python functions - no decorators needed:</p> <pre><code>def get_stock_price(symbol: str) -&gt; float:\n    \"\"\"Get the current stock price for a given symbol.\n\n    :param symbol: The stock symbol (e.g., 'AAPL', 'MSFT', 'GOOGL')\n    :return: The current stock price\n    \"\"\"\n    # This is a simplified example - in practice, you'd use a real API\n    mock_prices = {\n        'AAPL': 150.25,\n        'MSFT': 300.50,\n        'GOOGL': 2800.75,\n        'TSLA': 200.30\n    }\n\n    if symbol.upper() not in mock_prices:\n        raise ValueError(f\"Symbol {symbol} not found\")\n\n    return mock_prices[symbol.upper()]\n\ndef calculate_percentage_change(old_price: float, new_price: float) -&gt; float:\n    \"\"\"Calculate the percentage change between two prices.\n\n    :param old_price: The original price\n    :param new_price: The new price\n    :return: The percentage change (positive for increase, negative for decrease)\n    \"\"\"\n    return ((new_price - old_price) / old_price) * 100\n\ndef analyze_portfolio(prices: list[float]) -&gt; dict:\n    \"\"\"Analyze a portfolio of stock prices.\n\n    :param prices: List of stock prices\n    :return: Analysis results including average, min, max, and trend\n    \"\"\"\n    if not prices:\n        return {\"error\": \"No prices provided\"}\n\n    avg_price = sum(prices) / len(prices)\n    min_price = min(prices)\n    max_price = max(prices)\n\n    # Simple trend analysis\n    if len(prices) &gt;= 2:\n        trend = \"upward\" if prices[-1] &gt; prices[0] else \"downward\"\n    else:\n        trend = \"insufficient data\"\n\n    return {\n        \"average\": avg_price,\n        \"minimum\": min_price,\n        \"maximum\": max_price,\n        \"trend\": trend,\n        \"count\": len(prices)\n    }\n</code></pre>"},{"location":"tutorials/agentbot/#step-2-creating-the-financial-agent","title":"Step 2: Creating the Financial Agent","text":"<pre><code># Create a financial analysis agent\nfinancial_agent = lmb.AgentBot(\n    tools=[get_stock_price, calculate_percentage_change, analyze_portfolio],\n    model_name=\"gpt-4o-mini\"\n)\n</code></pre>"},{"location":"tutorials/agentbot/#step-3-using-the-agent-for-analysis","title":"Step 3: Using the Agent for Analysis","text":"<pre><code># Ask the agent to analyze multiple stocks\nresult = financial_agent(\"\"\"\nPlease analyze the following stocks:\n1. Get the current price of AAPL\n2. Get the current price of MSFT\n3. Calculate the percentage change from yesterday's prices (AAPL: $145, MSFT: $295)\n4. Analyze the portfolio performance\n\"\"\")\n\nprint(result)\n</code></pre> <p>The agent will:</p> <ol> <li>Use the decision node to select <code>get_stock_price</code> for AAPL</li> <li>Loop back to decide, then select <code>get_stock_price</code> for MSFT</li> <li>Loop back to decide, then select <code>calculate_percentage_change</code> for    both stocks</li> <li>Loop back to decide, then select <code>analyze_portfolio</code></li> <li>Finally, use <code>respond_to_user</code> (terminal) to provide the final    answer</li> </ol>"},{"location":"tutorials/agentbot/#part-3-visualizing-the-flow-graph","title":"Part 3: Visualizing the Flow Graph","text":"<p>One of the powerful features of AgentBot is the ability to visualize the flow graph. If you're using Marimo notebooks, you can display the graph:</p> <pre><code>agent = lmb.AgentBot(tools=[get_stock_price, calculate_percentage_change])\n\n# In a Marimo notebook, this will display the flow graph\nagent\n</code></pre> <p>The graph shows:</p> <ul> <li>The decision node (DecideNode)</li> <li>All tool nodes (with their function names)</li> <li>Edges showing how tools connect back to the decision node</li> <li>Terminal nodes (like <code>respond_to_user</code>) that don't loop back</li> </ul>"},{"location":"tutorials/agentbot/#visualization-features","title":"Visualization Features","text":"<p>The flow visualization includes several automatic features:</p> <p>Automatic Graph Direction: The visualization automatically determines whether to use a top-down (<code>graph TD</code>) or left-right (<code>graph LR</code>) layout based on the graph structure. If the graph is wider than it is deep, it uses a left-right layout; otherwise, it uses a top-down layout.</p> <p>Terminal Node Coloring: Terminal nodes (nodes with no successors) are automatically colored green to distinguish them from regular nodes, which are colored blue. This makes it easy to identify where the flow ends.</p> <p>You can also manually generate the Mermaid diagram string:</p> <pre><code>from llamabot.components.pocketflow import flow_to_mermaid\n\nagent = lmb.AgentBot(tools=[get_stock_price, calculate_percentage_change])\n\n# Get the Mermaid diagram string\nmermaid_diagram = flow_to_mermaid(agent.flow)\nprint(mermaid_diagram)\n</code></pre>"},{"location":"tutorials/agentbot/#part-4-configuring-the-decision-node","title":"Part 4: Configuring the Decision Node","text":"<p>By default, AgentBot uses <code>DecideNode</code> which uses ToolBot to decide which tool to execute. You can configure the decision node through AgentBot's interface:</p> <pre><code># Configure system prompt and model directly\nagent = lmb.AgentBot(\n    tools=[get_stock_price],\n    system_prompt=\"Your custom system prompt here\",\n    model_name=\"gpt-4.1\",\n    # Additional completion kwargs (e.g., api_base, api_key)\n    api_base=\"https://your-endpoint.com\"\n)\n</code></pre> <p>For advanced cases where you need custom decision logic beyond what <code>DecideNode</code> provides, you can implement your own PocketFlow <code>Node</code> class and pass it via the <code>decide_node</code> parameter:</p> <pre><code>import json\nfrom pocketflow import Node\n\nclass CustomDecisionNode(Node):\n    \"\"\"Custom decision node with specialized logic.\"\"\"\n\n    def __init__(self, tools, model_name=\"gpt-4.1\", **kwargs):\n        super().__init__()\n        self.tools = tools\n        self.model_name = model_name\n        self.kwargs = kwargs\n\n    def prep(self, shared):\n        \"\"\"Prepare the node for execution.\"\"\"\n        return shared\n\n    def exec(self, prep_res):\n        \"\"\"Execute custom decision logic.\"\"\"\n        # Your custom decision logic here\n        # Must return a tool name (string) to route to\n        from llamabot.bot.toolbot import ToolBot\n\n        bot = ToolBot(\n            tools=self.tools,\n            system_prompt=\"Your custom system prompt\",\n            model_name=self.model_name,\n            **self.kwargs\n        )\n        bot.tool_choice = \"required\"\n\n        tool_calls = bot(prep_res[\"memory\"])\n        if not tool_calls:\n            raise ValueError(\"No tool calls returned\")\n\n        # Extract tool name and arguments\n        tool_name = tool_calls[0].function.name\n        func_args = json.loads(tool_calls[0].function.arguments)\n\n        # Store arguments for the next node\n        prep_res[\"func_call\"] = func_args\n\n        return tool_name\n\n    def post(self, shared, prep_res, exec_res):\n        \"\"\"Post-process the execution result.\"\"\"\n        shared[\"memory\"].append(f\"Chosen Tool: {exec_res}\")\n        return exec_res\n\n# Use the custom decision node\n# AgentBot will automatically set the tools on your custom decide node\ncustom_decide = CustomDecisionNode(\n    tools=[],  # Will be set by AgentBot to include DEFAULT_TOOLS + your tools\n    model_name=\"gpt-4.1\"\n)\n\nagent = lmb.AgentBot(\n    tools=[get_stock_price],\n    decide_node=custom_decide\n)\n</code></pre>"},{"location":"tutorials/agentbot/#part-5-advanced-features","title":"Part 5: Advanced Features","text":""},{"location":"tutorials/agentbot/#terminal-tools","title":"Terminal Tools","text":"<p>By default, all tools loop back to the decision node. However, <code>respond_to_user</code> is a terminal tool that ends the flow. You can create your own terminal tools using <code>@tool</code> with <code>loopback_name=None</code>:</p> <pre><code>from llamabot.components.tools import tool\n\n# Terminal node - no loopback\n@tool(loopback_name=None)\ndef final_answer(message: str) -&gt; str:\n    \"\"\"Provide the final answer to the user.\n\n    :param message: The final answer message\n    :return: The message\n    \"\"\"\n    return message\n</code></pre>"},{"location":"tutorials/agentbot/#using-already-decorated-tools","title":"Using Already-Decorated Tools","text":"<p>If you have functions that are already decorated with <code>@tool</code>, that's fine too:</p> <pre><code>@lmb.tool\ndef my_tool(arg: str) -&gt; str:\n    \"\"\"My tool function.\"\"\"\n    return f\"Result: {arg}\"\n\n# AgentBot accepts tools decorated with @tool\nagent = lmb.AgentBot(tools=[my_tool])\n</code></pre>"},{"location":"tutorials/agentbot/#accessing-the-flow","title":"Accessing the Flow","text":"<p>You can access the underlying PocketFlow flow for advanced use cases:</p> <pre><code>agent = lmb.AgentBot(tools=[get_stock_price])\n\n# Access the flow\nflow = agent.flow\n\n# Access individual nodes\ndecide_node = agent.decide_node\ntools = agent.tools\n</code></pre>"},{"location":"tutorials/agentbot/#part-6-best-practices","title":"Part 6: Best Practices","text":""},{"location":"tutorials/agentbot/#1-write-clear-function-documentation","title":"1. Write Clear Function Documentation","text":"<p>Good docstrings help the decision node choose the right tool:</p> <pre><code>def analyze_sentiment(text: str) -&gt; dict:\n    \"\"\"Analyze the sentiment of text using a simple algorithm.\n\n    This tool performs basic sentiment analysis by counting positive and\n    negative words. It's useful for getting a quick understanding of\n    text sentiment.\n\n    :param text: The text to analyze\n    :return: Dictionary with 'sentiment' (positive/negative/neutral),\n        'score' (0-1), and 'confidence'\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"tutorials/agentbot/#2-design-tools-for-specific-use-cases","title":"2. Design Tools for Specific Use Cases","text":"<p>Keep tools focused and single-purpose:</p> <pre><code>def get_user_profile(user_id: str) -&gt; dict:\n    \"\"\"Get user profile information.\n\n    :param user_id: The user's unique identifier\n    :return: User profile dictionary\n    \"\"\"\n    # Implementation\n    pass\n\ndef update_user_profile(user_id: str, updates: dict) -&gt; dict:\n    \"\"\"Update user profile information.\n\n    :param user_id: The user's unique identifier\n    :param updates: Dictionary of fields to update\n    :return: Updated user profile\n    \"\"\"\n    # Implementation\n    pass\n</code></pre>"},{"location":"tutorials/agentbot/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<p>Tools should handle errors and return meaningful results:</p> <pre><code>def safe_api_call(url: str, timeout: int = 10) -&gt; dict:\n    \"\"\"Safely make an API call with error handling.\n\n    :param url: The URL to call\n    :param timeout: Request timeout in seconds, by default 10\n    :return: API response or error information\n    \"\"\"\n    import requests\n\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n        return {\n            \"status\": \"success\",\n            \"data\": response.json(),\n            \"status_code\": response.status_code\n        }\n    except requests.RequestException as e:\n        return {\n            \"status\": \"error\",\n            \"error\": str(e),\n            \"url\": url\n        }\n</code></pre>"},{"location":"tutorials/agentbot/#part-7-understanding-the-flow","title":"Part 7: Understanding the Flow","text":""},{"location":"tutorials/agentbot/#how-tools-are-wrapped","title":"How Tools Are Wrapped","text":"<p>When you provide a function to AgentBot:</p> <ol> <li>The function is wrapped with <code>@tool</code> to create a tool schema</li> <li>The <code>@tool</code> decorator enables AgentBot integration</li> <li>The node is connected to the decision node</li> <li>If not terminal, the node loops back to the decision node</li> </ol>"},{"location":"tutorials/agentbot/#flow-execution","title":"Flow Execution","text":"<p>When you call the agent:</p> <ol> <li>Your query is added to the shared state's memory</li> <li>The flow starts at the decision node</li> <li>The decision node uses ToolBot to select a tool</li> <li>The selected tool executes with arguments from the decision node</li> <li>The result is added to memory</li> <li>If the tool loops back, the flow returns to the decision node</li> <li>This continues until a terminal tool (like <code>respond_to_user</code>) is    reached</li> </ol>"},{"location":"tutorials/agentbot/#shared-state","title":"Shared State","text":"<p>The flow uses a shared state dictionary that contains:</p> <ul> <li><code>memory</code>: List of conversation messages and tool results</li> <li><code>func_call</code>: Dictionary of function arguments (set by decision node)</li> <li><code>result</code>: Tool execution results</li> </ul>"},{"location":"tutorials/agentbot/#conclusion","title":"Conclusion","text":"<p>Congratulations! You now understand how to use AgentBot with PocketFlow for graph-based tool orchestration. AgentBot provides:</p> <ul> <li>Automatic tool wrapping: Just provide plain callables</li> <li>Graph-based orchestration: Visual flow representation</li> <li>Default tools: <code>today_date</code>, <code>respond_to_user</code>, and <code>return_object_to_user</code> always   available</li> <li>Flexible decision making: Custom decision nodes supported</li> <li>Terminal nodes: Control flow termination with terminal tools</li> </ul> <p>The graph-based approach makes it easy to visualize and understand how your agent works, while the automatic wrapping makes it simple to add new tools. Happy coding!</p>"},{"location":"tutorials/ollama/","title":"How to Run Llamabot with Ollama","text":""},{"location":"tutorials/ollama/#overview","title":"Overview","text":"<p>In this guide, you'll learn how to run a chatbot using <code>llamabot</code> and <code>Ollama</code>. We'll cover how to install Ollama, start its server, and finally, run the chatbot within a Python session.</p>"},{"location":"tutorials/ollama/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"tutorials/ollama/#install-ollama","title":"Install Ollama","text":"<ol> <li>macOS Users: Download here</li> <li>Linux &amp; WSL2 Users: Run <code>curl https://ollama.ai/install.sh | sh</code> in your terminal</li> <li>Windows Users: Support coming soon.</li> </ol> <p>For more detailed instructions, refer to Ollama's official site.</p>"},{"location":"tutorials/ollama/#running-ollama-server","title":"Running Ollama Server","text":"<ol> <li>Open your terminal and start the Ollama server with your chosen model.</li> </ol> <pre><code>ollama run &lt;model_name&gt;\n</code></pre> <p>Example:</p> <pre><code>ollama run vicuna\n</code></pre> <p>For a list of available models, visit Ollama's Model Library.</p> <p>Note: Ensure you have adequate RAM for the model you are running.</p>"},{"location":"tutorials/ollama/#running-llamabot-in-python","title":"Running Llamabot in Python","text":"<ol> <li>Open a Python session and import the <code>SimpleBot</code> class from the <code>llamabot</code> library.</li> </ol> <pre><code>from llamabot import SimpleBot  # you can also use QueryBot or StructuredBot\n\nbot = SimpleBot(\"You are a conversation expert\", model_name=\"ollama_chat/vicuna:7b-16k\")\n</code></pre> <p>Note: <code>vicuna:7b-16k</code> includes tags from the vicuna model page.</p> <p>And there you have it! You're now ready to run your own chatbot with Ollama and Llamabot.</p>"},{"location":"tutorials/prompts/","title":"How to use the @prompt decorator","text":"<p>The <code>@prompt</code> decorator is LlamaBot's powerful tool for creating reusable, version-controlled prompt templates. This guide shows you how to use it effectively in your projects.</p>"},{"location":"tutorials/prompts/#what-is-the-prompt-decorator","title":"What is the @prompt decorator?","text":"<p>The <code>@prompt</code> decorator transforms Python functions into Jinja2-templated prompts with automatic version control. Instead of writing prompts as static strings, you can create dynamic, reusable prompt templates that adapt to different inputs.</p>"},{"location":"tutorials/prompts/#basic-usage","title":"Basic usage","text":""},{"location":"tutorials/prompts/#creating-your-first-prompt","title":"Creating your first prompt","text":"<pre><code>from llamabot import prompt\n\n@prompt(role=\"user\")\ndef explain_concept(concept, audience):\n    \"\"\"Please explain {{ concept }} to {{ audience }} in simple terms.\n\n    Use examples and analogies that {{ audience }} would understand.\n    Break down complex ideas into digestible pieces.\n    \"\"\"\n</code></pre> <p>The decorator uses your function's docstring as a Jinja2 template. Variables in double curly braces <code>{{ variable }}</code> are replaced with function arguments.</p>"},{"location":"tutorials/prompts/#using-the-prompt","title":"Using the prompt","text":"<p>The <code>@prompt</code> decorated function is still a regular Python function that you can call directly. When you call it, it returns a message object with the rendered Jinja2 template:</p> <pre><code># Create a prompt message\nmessage = explain_concept(\"machine learning\", \"high school students\")\n\n# The message object contains:\n# - role: \"user\"\n# - content: The rendered template\n# - prompt_hash: A unique hash for version tracking\nprint(message.content)\n</code></pre> <p>What happens when you call the function:</p> <ol> <li>The function arguments (<code>\"machine learning\"</code> and <code>\"high school students\"</code>) are passed to the Jinja2 template</li> <li>The template variables <code>{{ concept }}</code> and <code>{{ audience }}</code> are replaced with the actual values</li> <li>The docstring is rendered as a complete prompt string</li> <li>A message object is returned containing the rendered content</li> </ol> <p>Example of the rendered output:</p> <pre><code>message = explain_concept(\"machine learning\", \"high school students\")\nprint(message.content)\n# Output:\n# \"Please explain machine learning to high school students in simple terms.\n#\n# Use examples and analogies that high school students would understand.\n# Break down complex ideas into digestible pieces.\"\n</code></pre> <p>The key insight is that the decorated function executes normally - it takes your arguments, interpolates them into the docstring template, and returns the final rendered prompt as a message object.</p>"},{"location":"tutorials/prompts/#understanding-message-roles","title":"Understanding message roles","text":"<p>The <code>@prompt</code> decorator supports three message roles:</p>"},{"location":"tutorials/prompts/#system-prompts","title":"System prompts","text":"<p>Use <code>role=\"system\"</code> for instructions that define the AI's behavior:</p> <pre><code>@prompt(role=\"system\")\ndef helpful_assistant():\n    \"\"\"You are a helpful AI assistant. Always provide accurate,\n    clear answers and ask for clarification when needed.\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#user-prompts","title":"User prompts","text":"<p>Use <code>role=\"user\"</code> for messages that simulate user input:</p> <pre><code>@prompt(role=\"user\")\ndef ask_question(topic):\n    \"\"\"I need help understanding {{ topic }}.\n    Can you explain it step by step?\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#assistant-prompts","title":"Assistant prompts","text":"<p>Use <code>role=\"assistant\"</code> for few-shot examples or assistant responses:</p> <pre><code>@prompt(role=\"assistant\")\ndef example_response(task, solution):\n    \"\"\"For the task \"{{ task }}\", here's how I approach it:\n\n    {{ solution }}\n\n    This demonstrates the structured thinking process.\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#advanced-templating","title":"Advanced templating","text":""},{"location":"tutorials/prompts/#complex-data-structures","title":"Complex data structures","text":"<p>Your prompts can accept complex data types:</p> <pre><code>@prompt(role=\"user\")\ndef code_review(code_snippet, issues):\n    \"\"\"Please review this code:\n\n    ```python\n    {{ code_snippet }}\n    ```\n\n    Focus on these areas:\n    {% for issue in issues %}\n    - {{ issue }}\n    {% endfor %}\n    \"\"\"\n\n# Usage\nissues = [\"performance\", \"readability\", \"security\"]\nmessage = code_review(\"def hello(): print('world')\", issues)\n</code></pre>"},{"location":"tutorials/prompts/#conditional-content","title":"Conditional content","text":"<p>Use Jinja2 conditionals for dynamic content:</p> <pre><code>@prompt(role=\"user\")\ndef write_documentation(function_name, include_examples=True):\n    \"\"\"Write documentation for the {{ function_name }} function.\n\n    {% if include_examples %}\n    Include practical examples showing how to use it.\n    {% endif %}\n\n    Follow standard docstring conventions.\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#template-inheritance-patterns","title":"Template inheritance patterns","text":"<p>Create base prompts and extend them:</p> <pre><code>@prompt(role=\"system\")\ndef base_coding_assistant(language):\n    \"\"\"You are an expert {{ language }} programmer.\n    Always write clean, well-documented code.\"\"\"\n\n@prompt(role=\"user\")\ndef debug_code(code, error_message, language=\"Python\"):\n    \"\"\"I'm having trouble with this {{ language }} code:\n\n    {{ code }}\n\n    Error: {{ error_message }}\n\n    Please help me fix it.\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#integration-with-bots","title":"Integration with bots","text":""},{"location":"tutorials/prompts/#using-prompts-with-simplebot","title":"Using prompts with SimpleBot","text":"<pre><code>from llamabot import SimpleBot, prompt\n\n@prompt(role=\"system\")\ndef python_tutor():\n    \"\"\"You are a patient Python tutor who explains concepts clearly.\"\"\"\n\n@prompt(role=\"user\")\ndef explain_topic(topic, skill_level):\n    \"\"\"Explain {{ topic }} to someone with {{ skill_level }} Python experience.\"\"\"\n\n# Create bot with system prompt\nbot = SimpleBot(\n    system_prompt=python_tutor(),\n    model_name=\"gpt-3.5-turbo\"\n)\n\n# Use user prompt\nresponse = bot(explain_topic(\"decorators\", \"beginner\"))\n</code></pre>"},{"location":"tutorials/prompts/#building-prompt-libraries","title":"Building prompt libraries","text":"<p>Organize related prompts in modules:</p> <pre><code># prompts/python_help.py\nfrom llamabot import prompt\n\n@prompt(role=\"user\")\ndef explain_error(error_type, error_message):\n    \"\"\"I got this {{ error_type }} error:\n\n    {{ error_message }}\n\n    Can you explain what caused it and how to fix it?\"\"\"\n\n@prompt(role=\"user\")\ndef optimize_code(code, optimization_goal):\n    \"\"\"Please optimize this Python code for {{ optimization_goal }}:\n\n    {{ code }}\n\n    Explain the improvements you made.\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#automatic-version-control","title":"Automatic version control","text":""},{"location":"tutorials/prompts/#how-versioning-works","title":"How versioning works","text":"<p>LlamaBot automatically tracks changes to your prompts:</p> <pre><code>@prompt(role=\"user\")\ndef greeting(name):\n    \"\"\"Hello {{ name }}!\"\"\"  # Version 1\n\n# Later, you modify the prompt:\n@prompt(role=\"user\")\ndef greeting(name):\n    \"\"\"Hello {{ name }}! How are you today?\"\"\"  # Version 2 (new hash)\n</code></pre> <p>Each unique prompt template gets a hash that's stored in a local SQLite database. When you modify a prompt, LlamaBot creates a new version while preserving the history.</p>"},{"location":"tutorials/prompts/#accessing-version-information","title":"Accessing version information","text":"<pre><code>message = greeting(\"Alice\")\nprint(f\"Prompt hash: {message.prompt_hash}\")\n</code></pre> <p>The prompt hash helps you track which version of a prompt generated specific results.</p>"},{"location":"tutorials/prompts/#working-with-experiments","title":"Working with experiments","text":""},{"location":"tutorials/prompts/#tracking-prompt-experiments","title":"Tracking prompt experiments","text":"<pre><code>from llamabot import Experiment, prompt, SimpleBot\n\n@prompt(role=\"system\")\ndef creative_writer(style):\n    \"\"\"You are a creative writer who writes in {{ style }} style.\"\"\"\n\n@prompt(role=\"user\")\ndef write_story(theme, length):\n    \"\"\"Write a {{ length }} story about {{ theme }}.\"\"\"\n\nwith Experiment(name=\"story_generation\") as exp:\n    # Different prompt versions are automatically tracked\n    bot = SimpleBot(creative_writer(\"mystery\"))\n    story = bot(write_story(\"time travel\", \"short\"))\n</code></pre>"},{"location":"tutorials/prompts/#ab-testing-prompts","title":"A/B testing prompts","text":"<pre><code>@prompt(role=\"user\")\ndef prompt_v1(topic):\n    \"\"\"Explain {{ topic }} briefly.\"\"\"\n\n@prompt(role=\"user\")\ndef prompt_v2(topic):\n    \"\"\"Provide a comprehensive explanation of {{ topic }} with examples.\"\"\"\n\n# Test both versions\nwith Experiment(name=\"explanation_styles\") as exp:\n    bot = SimpleBot(\"You are a helpful teacher.\")\n\n    # Version 1\n    response_v1 = bot(prompt_v1(\"photosynthesis\"))\n\n    # Version 2\n    response_v2 = bot(prompt_v2(\"photosynthesis\"))\n\n    # Compare results in log viewer\n</code></pre>"},{"location":"tutorials/prompts/#best-practices","title":"Best practices","text":""},{"location":"tutorials/prompts/#design-effective-prompts","title":"Design effective prompts","text":"<ol> <li>Be specific: Include clear instructions about format, tone, and expectations</li> <li>Use examples: Show the AI what good output looks like</li> <li>Test iterations: Use the version control to experiment with different approaches</li> <li>Validate variables: Ensure all template variables are provided as function parameters</li> </ol>"},{"location":"tutorials/prompts/#template-validation","title":"Template validation","text":"<p>The decorator automatically validates that all template variables have corresponding function parameters:</p> <pre><code>@prompt(role=\"user\")\ndef broken_prompt(name):\n    \"\"\"Hello {{ name }}! Your {{ age }} is showing.\"\"\"  # Error: 'age' not in parameters\n\n# Fix by adding the parameter:\n@prompt(role=\"user\")\ndef fixed_prompt(name, age):\n    \"\"\"Hello {{ name }}! Your {{ age }} is showing.\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#error-handling","title":"Error handling","text":"<pre><code>@prompt(role=\"user\")\ndef safe_prompt(required_param, optional_param=None):\n    \"\"\"Required: {{ required_param }}\n\n    {% if optional_param %}\n    Optional: {{ optional_param }}\n    {% endif %}\n    \"\"\"\n\n# This works\nmessage = safe_prompt(\"value1\")\n\n# This also works\nmessage = safe_prompt(\"value1\", \"value2\")\n</code></pre>"},{"location":"tutorials/prompts/#common-patterns","title":"Common patterns","text":""},{"location":"tutorials/prompts/#few-shot-examples","title":"Few-shot examples","text":"<pre><code>@prompt(role=\"user\")\ndef classification_with_examples(text, categories):\n    \"\"\"Classify the following text into one of these categories:\n    {% for category in categories %}\n    - {{ category }}\n    {% endfor %}\n\n    Examples:\n    Text: \"The weather is sunny today\"\n    Category: Weather\n\n    Text: \"I love this new restaurant\"\n    Category: Food\n\n    Text: \"{{ text }}\"\n    Category:\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#chain-of-thought-prompting","title":"Chain of thought prompting","text":"<pre><code>@prompt(role=\"user\")\ndef solve_step_by_step(problem):\n    \"\"\"Solve this problem step by step:\n\n    {{ problem }}\n\n    Think through this carefully:\n    1. What information do I have?\n    2. What do I need to find?\n    3. What steps should I take?\n    4. What is the final answer?\n    \"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#multi-turn-conversations","title":"Multi-turn conversations","text":"<pre><code>@prompt(role=\"system\")\ndef conversation_system():\n    \"\"\"You are a helpful assistant in a multi-turn conversation.\n    Remember context from previous messages.\"\"\"\n\n@prompt(role=\"user\")\ndef follow_up(previous_topic, new_question):\n    \"\"\"Earlier we discussed {{ previous_topic }}.\n    Now I want to ask: {{ new_question }}\"\"\"\n</code></pre>"},{"location":"tutorials/prompts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/prompts/#common-issues","title":"Common issues","text":"<p>Missing template variables: Ensure all <code>{{ variable }}</code> references have corresponding function parameters.</p> <p>Jinja2 syntax errors: Check your template syntax, especially with loops and conditionals.</p> <p>Import errors: Make sure to import <code>prompt</code> from <code>llamabot</code>:</p> <pre><code>from llamabot import prompt\n</code></pre> <p>Role validation: Use only \"system\", \"user\", or \"assistant\" as role values.</p>"},{"location":"tutorials/prompts/#debugging-prompts","title":"Debugging prompts","text":"<pre><code>@prompt(role=\"user\")\ndef debug_prompt(data):\n    \"\"\"Debug data: {{ data }}\"\"\"\n\n# Check the rendered content\nmessage = debug_prompt(\"test value\")\nprint(message.content)  # See exactly what was generated\nprint(message.prompt_hash)  # Track the version\n</code></pre>"},{"location":"tutorials/prompts/#next-steps","title":"Next steps","text":"<ul> <li>Explore the built-in prompt library in <code>llamabot.prompt_library</code></li> <li>Use the log viewer to analyze your prompt experiments</li> <li>Integrate prompts with QueryBot for document-based interactions</li> <li>Build custom prompt libraries for your specific use cases</li> </ul> <p>The <code>@prompt</code> decorator is your gateway to systematic prompt engineering. Start with simple templates and gradually build more sophisticated prompt libraries as you discover what works best for your applications.</p>"},{"location":"tutorials/querybot/","title":"QueryBot Tutorial","text":"<p>In this tutorial, we will learn how to use the <code>QueryBot</code> class to create a chatbot that can query documents using an LLM. The <code>QueryBot</code> class allows us to index documents and use an LLM to generate responses based on the indexed documents.</p>"},{"location":"tutorials/querybot/#using-querybot-with-a-document-store-and-chat-memory","title":"Using QueryBot with a Document Store and Chat Memory","text":"<p>The recommended way to use QueryBot is to explicitly create and manage your own document store using <code>LanceDBDocStore</code> and chat memory using <code>ChatMemory</code>. This gives you full control over storage, persistence, and memory management. Below is the standard usage pattern (inspired by <code>notebooks/llamabot_docs.py</code>).</p> <pre><code>from llamabot.components.docstore import LanceDBDocStore\nfrom llamabot import QueryBot\nfrom pyprojroot import here\n\n# Create a document store for your knowledge base\ndocstore = LanceDBDocStore(\n    table_name=\"my-documents\",\n    # Optional: Configure embedding model settings\n    embedding_registry=\"sentence-transformers\",  # Default registry\n    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n)\n\ndocstore.reset()  # Optionally clear all documents\n\n# Add documents (e.g., all Markdown files in the docs folder)\ndocs_paths = (here() / \"docs\").rglob(\"*.md\")\ndocs_texts = [p.read_text() for p in docs_paths]\ndocstore.extend(docs_texts)\n\n# Create a separate store for chat memory\n# For simple linear memory (fast, no LLM calls)\nchat_memory = lmb.ChatMemory()\n\n# For intelligent threading (uses LLM for smart connections)\n# chat_memory = lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")\n\nchat_memory.reset()  # Optionally clear previous chat history\n\n# Define a system prompt (optionally using the @prompt decorator)\nsystem_prompt = \"You are a helpful assistant for my project.\"\n\n# Initialize QueryBot with both docstore and chat memory\nbot = QueryBot(\n    system_prompt=system_prompt,\n    docstore=docstore,\n    memory=chat_memory,\n)\n\n# Use the bot in a conversational loop\nwhile True:\n    user_input = input(\"Ask a question: \")\n    if user_input.lower() in {\"exit\", \"quit\"}:\n        break\n    response = bot(user_input)\n    print(response.content)\n</code></pre>"},{"location":"tutorials/querybot/#organizing-documents-with-partitions","title":"Organizing Documents with Partitions","text":"<p>When working with large document collections, you may want to organize documents into logical groups called partitions. Partitions are useful for:</p> <ul> <li>Organizing documents by source (e.g., \"tutorials\", \"reference\", \"api_docs\")</li> <li>Categorizing by topic or domain</li> <li>Separating documents by date or version</li> <li>Any other logical grouping that helps you filter and search more effectively</li> </ul>"},{"location":"tutorials/querybot/#enabling-partitioning","title":"Enabling Partitioning","text":"<p>To use partitioning, enable it when creating your document store:</p> <pre><code>from llamabot.components.docstore import LanceDBDocStore\n\n# Create a document store with partitioning enabled\ndocstore = LanceDBDocStore(\n    table_name=\"my-documents\",\n    enable_partitioning=True,\n    default_partition=\"general\",  # Optional: default partition name\n)\n</code></pre>"},{"location":"tutorials/querybot/#adding-documents-to-partitions","title":"Adding Documents to Partitions","text":"<p>You can assign documents to partitions in several ways:</p> <p>Using <code>append()</code> with a partition:</p> <pre><code># Add a single document to a specific partition\ndocstore.append(\"Python tutorial content\", partition=\"tutorials\")\ndocstore.append(\"API reference documentation\", partition=\"reference\")\n</code></pre> <p>Using <code>extend()</code> with a single partition (all documents go to the same partition):</p> <pre><code># Add multiple documents to the same partition\ntutorial_docs = [\n    \"Python tutorial part 1\",\n    \"Python tutorial part 2\",\n    \"Python tutorial part 3\",\n]\ndocstore.extend(tutorial_docs, partition=\"tutorials\")\n</code></pre> <p>Using <code>extend()</code> with multiple partitions (one partition per document):</p> <pre><code># Add documents to different partitions in one call\ndocuments = [\n    \"Python tutorial\",\n    \"Python reference\",\n    \"Python API docs\",\n]\npartitions = [\"tutorials\", \"reference\", \"api_docs\"]\ndocstore.extend(documents, partitions=partitions)\n</code></pre>"},{"location":"tutorials/querybot/#querying-specific-partitions","title":"Querying Specific Partitions","text":"<p>You can query documents from specific partitions using the <code>partitions</code> parameter:</p> <pre><code># Query only the tutorials partition\nresults = docstore.retrieve(\"python programming\", partitions=[\"tutorials\"])\n\n# Query multiple partitions\nresults = docstore.retrieve(\n    \"python\", partitions=[\"tutorials\", \"reference\"]\n)\n\n# Query all partitions (default behavior)\nresults = docstore.retrieve(\"python\")  # Searches all partitions\n</code></pre> <p>Note: Currently, <code>QueryBot</code> doesn't support partition filtering directly. If you need to query specific partitions with QueryBot, you have a few options:</p> <ol> <li>Access the docstore directly before creating the bot:</li> </ol> <pre><code># Get partition-filtered results\nrelevant_docs = docstore.retrieve(\"python\", partitions=[\"tutorials\"])\n\n# Then use these results with your bot\n# (You'd need to manually construct the context)\n</code></pre> <ol> <li>Create separate QueryBot instances for different partitions:</li> </ol> <pre><code># Create separate docstores or filter results per partition\ntutorial_bot = QueryBot(\n    system_prompt=\"You are a Python tutorial assistant.\",\n    docstore=docstore,  # Same docstore, but filter in your queries\n)\n</code></pre> <ol> <li>Use the docstore's <code>retrieve()</code> method and manually pass results to your LLM.</li> </ol>"},{"location":"tutorials/querybot/#helper-methods-for-partition-management","title":"Helper Methods for Partition Management","text":"<p>The <code>LanceDBDocStore</code> provides several helper methods for working with partitions:</p> <p>List all available partitions:</p> <pre><code>partitions = docstore.list_partitions()\nprint(partitions)  # ['tutorials', 'reference', 'api_docs']\n</code></pre> <p>Get the count of documents in a partition:</p> <pre><code>count = docstore.get_partition_count(\"tutorials\")\nprint(f\"Tutorials partition has {count} documents\")\n</code></pre> <p>Reset (clear) a specific partition:</p> <pre><code># Delete all documents in the tutorials partition\ndocstore.reset_partition(\"tutorials\")\n</code></pre>"},{"location":"tutorials/querybot/#complete-example-with-partitions","title":"Complete Example with Partitions","text":"<p>Here's a complete example showing partitioning in action:</p> <pre><code>from llamabot.components.docstore import LanceDBDocStore\nfrom llamabot import QueryBot\nfrom pathlib import Path\n\n# Create partitioned document store\ndocstore = LanceDBDocStore(\n    table_name=\"project-docs\",\n    enable_partitioning=True,\n    default_partition=\"general\",\n)\n\ndocstore.reset()\n\n# Organize documents by category\ntutorial_files = list(Path(\"docs/tutorials\").glob(\"*.md\"))\nreference_files = list(Path(\"docs/reference\").glob(\"*.md\"))\n\n# Add tutorials\nfor file in tutorial_files:\n    docstore.append(file.read_text(), partition=\"tutorials\")\n\n# Add reference docs\nfor file in reference_files:\n    docstore.append(file.read_text(), partition=\"reference\")\n\n# Query specific partition directly\ntutorial_results = docstore.retrieve(\n    \"how do I get started?\",\n    partitions=[\"tutorials\"]  # Only search tutorials partition\n)\nprint(f\"Found {len(tutorial_results)} results in tutorials partition\")\n\n# Query multiple partitions\ntutorial_and_ref_results = docstore.retrieve(\n    \"python syntax\",\n    partitions=[\"tutorials\", \"reference\"]  # Search both partitions\n)\n\n# See what partitions exist\nprint(f\"Available partitions: {docstore.list_partitions()}\")\nprint(f\"Tutorials count: {docstore.get_partition_count('tutorials')}\")\n\n# Create QueryBot (searches all partitions by default)\nbot = QueryBot(\n    system_prompt=\"You are a helpful documentation assistant.\",\n    docstore=docstore,\n)\n\n# QueryBot will search across all partitions\nresponse = bot(\"How do I use the API?\")\n\n# To query only a specific partition, use docstore.retrieve() directly\n# and then manually construct your prompt with those results\ntutorial_only_results = docstore.retrieve(\n    \"getting started\",\n    partitions=[\"tutorials\"]  # Only tutorials partition\n)\n# You can then use these results with your LLM of choice\n</code></pre> <p>Tips:</p> <ul> <li>You can use <code>.reset()</code> on either store to clear its contents.</li> <li>The <code>LanceDBDocStore</code> uses the following default settings:</li> <li><code>embedding_registry</code>: \"sentence-transformers\"</li> <li><code>embedding_model</code>: \"minishlab/potion-base-8M\"   You can customize these settings when initializing the store to use different embedding models.</li> <li>For more details, see the source code in <code>llamabot/bot/querybot.py</code> and <code>llamabot/components/docstore.py</code>.</li> <li>This pattern is ideal for interactive apps, notebooks, or production bots where you want persistent memory and document storage.</li> </ul>"},{"location":"tutorials/recording_prompts/","title":"Automatically Record QueryBot Calls with PromptRecorder","text":"<p>In this tutorial, we will learn how to use the <code>PromptRecorder</code> class to automatically record calls made to the <code>QueryBot</code>. The <code>PromptRecorder</code> class is designed to record prompts and responses, making it a perfect fit for logging interactions with the <code>QueryBot</code>.</p>"},{"location":"tutorials/recording_prompts/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have the following Python libraries installed:</p> <ul> <li>pandas</li> <li>panel</li> </ul> <p>You can install them using pip:</p> <pre><code>pip install pandas panel\n</code></pre>"},{"location":"tutorials/recording_prompts/#step-1-import-the-necessary-classes","title":"Step 1: Import the necessary classes","text":"<p>First, we need to import the <code>PromptRecorder</code> and <code>QueryBot</code> classes from their respective source files. You can do this by adding the following lines at the beginning of your script:</p> <pre><code>from llamabot.recorder import PromptRecorder, autorecord\nfrom llamabot.bot.querybot import QueryBot\n</code></pre>"},{"location":"tutorials/recording_prompts/#step-2-initialize-the-querybot","title":"Step 2: Initialize the QueryBot","text":"<p>Next, we need to create an instance of the <code>QueryBot</code> class. You can do this by providing the necessary parameters, such as the system message, model name, and document paths. For example:</p> <pre><code>system_message = \"You are a helpful assistant that can answer questions based on the provided documents.\"\nmodel_name = \"gpt-4\"\ndoc_paths = [\"document1.txt\", \"document2.txt\"]\n\nquery_bot = QueryBot(system_message, model_name=model_name, document_paths=doc_paths)\n</code></pre>"},{"location":"tutorials/recording_prompts/#step-3-use-the-promptrecorder-context-manager","title":"Step 3: Use the PromptRecorder context manager","text":"<p>Now that we have an instance of the <code>QueryBot</code>, we can use the <code>PromptRecorder</code> context manager to automatically record the prompts and responses. To do this, simply wrap your interactions with the <code>QueryBot</code> inside a <code>with</code> statement, like this:</p> <pre><code>with PromptRecorder() as recorder:\n    # Interact with the QueryBot here\n</code></pre>"},{"location":"tutorials/recording_prompts/#step-4-interact-with-the-querybot","title":"Step 4: Interact with the QueryBot","text":"<p>Inside the <code>with</code> statement, you can now interact with the <code>QueryBot</code> by calling it with your queries. For example:</p> <pre><code>with PromptRecorder() as recorder:\n    query = \"What is the main idea of document1?\"\n    response = query_bot(query)\n    print(response.content)\n\n    query = \"How does document2 support the main idea?\"\n    response = query_bot(query)\n    print(response.content)\n</code></pre> <p>The <code>PromptRecorder</code> will automatically record the prompts and responses for each interaction with the <code>QueryBot</code>.</p>"},{"location":"tutorials/recording_prompts/#step-5-access-the-recorded-data","title":"Step 5: Access the recorded data","text":"<p>After you have finished interacting with the <code>QueryBot</code>, you can access the recorded data using the <code>PromptRecorder</code> instance. For example, you can print the recorded data as a pandas DataFrame:</p> <pre><code>print(recorder.dataframe())\n</code></pre> <p>Or, you can display the recorded data as an interactive panel:</p> <pre><code>recorder.panel().show()\n</code></pre>"},{"location":"tutorials/recording_prompts/#complete-example","title":"Complete Example","text":"<p>Here's the complete example that demonstrates how to use the <code>PromptRecorder</code> to automatically record <code>QueryBot</code> calls:</p> <pre><code>from llamabot.recorder import PromptRecorder, autorecord\nfrom llamabot.bot.querybot import QueryBot\n\nsystem_message = \"You are a helpful assistant that can answer questions based on the provided documents.\"\nmodel_name = \"gpt-4\"\ndoc_paths = [\"document1.txt\", \"document2.txt\"]\n\nquery_bot = QueryBot(system_message, model_name=model_name, document_paths=doc_paths)\n\nwith PromptRecorder() as recorder:\n    query = \"What is the main idea of document1?\"\n    response = query_bot(query)\n    print(response.content)\n\n    query = \"How does document2 support the main idea?\"\n    response = query_bot(query)\n    print(response.content)\n\nprint(recorder.dataframe())\nrecorder.panel().show()\n</code></pre> <p>That's it! You now know how to use the <code>PromptRecorder</code> class to automatically record calls made to the <code>QueryBot</code>. This can be a useful tool for logging and analyzing interactions with your chatbot.</p>"},{"location":"tutorials/simplebot/","title":"SimpleBot Tutorial","text":"<p>Note</p> <p>This tutorial was written by GPT4 and edited by a human.</p> <p>In this tutorial, we will learn how to use the <code>SimpleBot</code> class, a Python implementation of a chatbot that interacts with OpenAI's GPT-4 model. The <code>SimpleBot</code> class is designed to be simple and easy to use, allowing you to create a chatbot that can respond to human messages based on a given system prompt.</p>"},{"location":"tutorials/simplebot/#getting-started","title":"Getting Started","text":"<p>First, let's import the <code>SimpleBot</code> class:</p> <pre><code>from llamabot.bot.simplebot import SimpleBot\n</code></pre>"},{"location":"tutorials/simplebot/#initializing-the-simplebot","title":"Initializing the SimpleBot","text":"<p>To create a new instance of <code>SimpleBot</code>, you need to provide a system prompt. The system prompt is used to prime the GPT-4 model, giving it context for generating responses. You can also optionally set the <code>temperature</code> and <code>model_name</code> parameters.</p> <pre><code>system_prompt = \"You are an AI assistant that helps users with their questions.\"\nbot = SimpleBot(system_prompt)\n</code></pre>"},{"location":"tutorials/simplebot/#interacting-with-the-simplebot","title":"Interacting with the SimpleBot","text":"<p>To interact with the <code>SimpleBot</code>, simply call the instance with a human message as a parameter. The bot will return an <code>AIMessage</code> object containing the generated response.</p> <pre><code>human_message = \"What is the capital of France?\"\nresponse = bot(human_message)\nprint(response.content)\n</code></pre>"},{"location":"tutorials/simplebot/#aimessage","title":"AIMessage","text":"<p>When interacting with the <code>SimpleBot</code>, it's important to note that the response returned is not a simple string, but an <code>AIMessage</code> object. This object contains the generated response and additional metadata. The structure of an <code>AIMessage</code> is as follows:</p> <pre><code>from llamabot.components.messages import AIMessage\n\n# Example AIMessage structure\n{\n\"content\": \"Generated response content\",\n\"role\": \"assistant\"\n}\n</code></pre>"},{"location":"tutorials/simplebot/#example","title":"Example","text":"<p>Here's a complete example of how to create and interact with a <code>SimpleBot</code>:</p> <pre><code>from llamabot.bot.simplebot import SimpleBot\n\n# Initialize the SimpleBot\nsystem_prompt = \"You are an AI assistant that helps users with their questions.\"\nbot = SimpleBot(system_prompt)\n\n# Interact with the SimpleBot\nhuman_message = \"What is the capital of France?\"\nresponse = bot(human_message)\nprint(response.content)\n</code></pre>"},{"location":"tutorials/simplebot/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to use the <code>SimpleBot</code> class to create a simple chatbot that interacts with OpenAI's GPT-4 model. With this knowledge, you can now create your own chatbots and experiment with different system prompts and settings.</p>"},{"location":"tutorials/simplebot/#additional-information","title":"Additional Information","text":"<p>For more detailed information on the <code>SimpleBot</code> class and its methods, please refer to the source code and documentation provided in the <code>llamabot</code> package.</p>"},{"location":"tutorials/structuredbot/","title":"StructuredBot Tutorial","text":"<p>Welcome to the StructuredBot tutorial! In this tutorial, we will learn how to use the <code>StructuredBot</code> class to get validated, structured outputs from LLMs using Pydantic models.</p>"},{"location":"tutorials/structuredbot/#what-is-structuredbot","title":"What is StructuredBot?","text":"<p>StructuredBot is designed for scenarios where you need guaranteed structured outputs from LLMs. Unlike SimpleBot, StructuredBot:</p> <ul> <li>Enforces Pydantic schema validation on all responses</li> <li>Automatically retries when the LLM produces invalid output</li> <li>Returns validated Pydantic objects instead of raw text</li> <li>Provides clear error messages when validation fails</li> </ul> <p>This makes StructuredBot perfect for data extraction, API responses, form processing, and any scenario where you need reliable structured data.</p>"},{"location":"tutorials/structuredbot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Basic knowledge of Python programming</li> <li>Familiarity with Pydantic models</li> <li>Access to a Python environment with the necessary libraries installed</li> </ul>"},{"location":"tutorials/structuredbot/#installation","title":"Installation","text":"<p>First, ensure you have the <code>llamabot</code> library installed:</p> <pre><code>pip install llamabot\n</code></pre>"},{"location":"tutorials/structuredbot/#basic-usage","title":"Basic Usage","text":""},{"location":"tutorials/structuredbot/#step-1-define-your-pydantic-model","title":"Step 1: Define Your Pydantic Model","text":"<p>Start by creating a Pydantic model that defines the structure you want:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: Optional[str] = None\n    hobbies: List[str] = []\n    created_at: datetime\n</code></pre>"},{"location":"tutorials/structuredbot/#step-2-create-a-structuredbot","title":"Step 2: Create a StructuredBot","text":"<pre><code>import llamabot as lmb\nfrom datetime import datetime\n\n# Create a StructuredBot with your Pydantic model\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract person information from the given text. Always include a created_at timestamp.\",\n    pydantic_model=Person,\n    model_name=\"gpt-4o\"\n)\n</code></pre>"},{"location":"tutorials/structuredbot/#step-3-use-the-bot","title":"Step 3: Use the Bot","text":"<pre><code># The bot will return a validated Person object\nperson = bot(\"John Smith is 25 years old and enjoys hiking, photography, and cooking. His email is john@example.com.\")\n\nprint(person.name)        # \"John Smith\"\nprint(person.age)         # 25\nprint(person.email)       # \"john@example.com\"\nprint(person.hobbies)     # [\"hiking\", \"photography\", \"cooking\"]\nprint(person.created_at)  # datetime object\n</code></pre>"},{"location":"tutorials/structuredbot/#advanced-features","title":"Advanced Features","text":""},{"location":"tutorials/structuredbot/#validation-and-retry-logic","title":"Validation and Retry Logic","text":"<p>StructuredBot automatically handles validation failures by retrying with the LLM:</p> <pre><code># If the LLM produces invalid output, StructuredBot will:\n# 1. Show the validation error to the LLM\n# 2. Ask it to fix the output\n# 3. Retry up to the maximum number of attempts\n# 4. Raise an error if all attempts fail\n\ntry:\n    person = bot(\"Invalid input that might confuse the model\")\nexcept ValidationError as e:\n    print(f\"Validation failed after retries: {e}\")\n</code></pre>"},{"location":"tutorials/structuredbot/#custom-validation-rules","title":"Custom Validation Rules","text":"<p>You can add custom validation to your Pydantic models:</p> <pre><code>from pydantic import BaseModel, validator\nfrom typing import List\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    category: str\n    tags: List[str]\n\n    @validator('price')\n    def price_must_be_positive(cls, v):\n        if v &lt;= 0:\n            raise ValueError('Price must be positive')\n        return v\n\n    @validator('category')\n    def category_must_be_valid(cls, v):\n        valid_categories = ['electronics', 'clothing', 'books', 'home']\n        if v.lower() not in valid_categories:\n            raise ValueError(f'Category must be one of: {valid_categories}')\n        return v.lower()\n\n# Create bot with custom validation\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract product information from text.\",\n    pydantic_model=Product,\n    model_name=\"gpt-4o\"\n)\n</code></pre>"},{"location":"tutorials/structuredbot/#complex-nested-models","title":"Complex Nested Models","text":"<p>StructuredBot works with complex nested structures:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str\n\nclass Company(BaseModel):\n    name: str\n    industry: str\n    address: Address\n    employees: int\n\nclass Employee(BaseModel):\n    name: str\n    position: str\n    salary: float\n    company: Company\n    skills: List[str]\n\n# Create bot for complex nested data\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract employee information including company details.\",\n    pydantic_model=Employee,\n    model_name=\"gpt-4o\"\n)\n\nemployee = bot(\"\"\"\nSarah Johnson works as a Senior Software Engineer at TechCorp,\na technology company in the software industry.\nShe earns $95,000 per year and has skills in Python, JavaScript, and React.\nThe company is located at 123 Tech Street, San Francisco, CA 94105\nand has 150 employees.\n\"\"\")\n</code></pre>"},{"location":"tutorials/structuredbot/#configuration-options","title":"Configuration Options","text":""},{"location":"tutorials/structuredbot/#retry-behavior","title":"Retry Behavior","text":"<pre><code>bot = lmb.StructuredBot(\n    system_prompt=\"Extract data from text.\",\n    pydantic_model=YourModel,\n    model_name=\"gpt-4o\",\n    allow_failed_validation=False,  # Default: False (retry on validation failure)\n    max_retries=3,                  # Default: 3 retries\n    temperature=0.1                 # Lower temperature for more consistent outputs\n)\n</code></pre>"},{"location":"tutorials/structuredbot/#streaming","title":"Streaming","text":"<p>StructuredBot supports streaming for real-time feedback:</p> <pre><code>bot = lmb.StructuredBot(\n    system_prompt=\"Extract data from text.\",\n    pydantic_model=YourModel,\n    stream_target=\"stdout\"  # Stream to console\n)\n</code></pre>"},{"location":"tutorials/structuredbot/#common-use-cases","title":"Common Use Cases","text":""},{"location":"tutorials/structuredbot/#1-data-extraction-from-documents","title":"1. Data Extraction from Documents","text":"<pre><code>class Invoice(BaseModel):\n    invoice_number: str\n    date: datetime\n    total_amount: float\n    vendor: str\n    line_items: List[dict]\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract invoice information from the document.\",\n    pydantic_model=Invoice,\n    model_name=\"gpt-4o\"\n)\n\ninvoice = bot(invoice_document_text)\n</code></pre>"},{"location":"tutorials/structuredbot/#2-api-response-processing","title":"2. API Response Processing","text":"<pre><code>class APIResponse(BaseModel):\n    status: str\n    data: dict\n    error_message: Optional[str] = None\n    timestamp: datetime\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Parse API response and extract structured data.\",\n    pydantic_model=APIResponse,\n    model_name=\"gpt-4o\"\n)\n\nresponse = bot(api_response_text)\n</code></pre>"},{"location":"tutorials/structuredbot/#3-form-data-validation","title":"3. Form Data Validation","text":"<pre><code>class ContactForm(BaseModel):\n    name: str\n    email: str\n    phone: Optional[str] = None\n    message: str\n    urgency: str  # \"low\", \"medium\", \"high\"\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract and validate contact form information.\",\n    pydantic_model=ContactForm,\n    model_name=\"gpt-4o\"\n)\n\nform_data = bot(user_submitted_text)\n</code></pre>"},{"location":"tutorials/structuredbot/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/structuredbot/#1-design-clear-schemas","title":"1. Design Clear Schemas","text":"<pre><code># Good: Clear, specific fields\nclass UserProfile(BaseModel):\n    full_name: str\n    email: str\n    age: int\n    interests: List[str]\n\n# Avoid: Vague or overly complex schemas\nclass BadProfile(BaseModel):\n    info: dict  # Too vague\n    data: Any   # Too flexible\n</code></pre>"},{"location":"tutorials/structuredbot/#2-use-appropriate-types","title":"2. Use Appropriate Types","text":"<pre><code>from typing import Optional, List, Union\nfrom datetime import datetime\n\nclass Event(BaseModel):\n    title: str\n    start_time: datetime\n    duration_minutes: int\n    attendees: List[str]\n    is_online: bool\n    location: Optional[str] = None\n</code></pre>"},{"location":"tutorials/structuredbot/#3-add-helpful-validation","title":"3. Add Helpful Validation","text":"<pre><code>from pydantic import BaseModel, validator\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    category: str\n\n    @validator('price')\n    def price_must_be_positive(cls, v):\n        if v &lt;= 0:\n            raise ValueError('Price must be positive')\n        return v\n</code></pre>"},{"location":"tutorials/structuredbot/#4-handle-edge-cases","title":"4. Handle Edge Cases","text":"<pre><code># Use Optional fields for data that might not be present\nclass Article(BaseModel):\n    title: str\n    content: str\n    author: Optional[str] = None\n    publish_date: Optional[datetime] = None\n    tags: List[str] = []\n</code></pre>"},{"location":"tutorials/structuredbot/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/structuredbot/#common-issues","title":"Common Issues","text":"<ol> <li>Validation Errors: Check your Pydantic model for type mismatches</li> <li>Retry Failures: Ensure your system prompt is clear about the expected format</li> <li>Complex Nested Data: Start with simpler models and gradually add complexity</li> </ol>"},{"location":"tutorials/structuredbot/#debug-mode","title":"Debug Mode","text":"<pre><code>import llamabot as lmb\n\n# Enable debug mode to see validation attempts\nlmb.set_debug_mode(True)\n\nbot = lmb.StructuredBot(\n    system_prompt=\"Extract data from text.\",\n    pydantic_model=YourModel,\n    model_name=\"gpt-4o\"\n)\n</code></pre>"},{"location":"tutorials/structuredbot/#comparison-with-simplebot","title":"Comparison with SimpleBot","text":"Feature SimpleBot StructuredBot Output Type Raw text Validated Pydantic objects Validation None Automatic Pydantic validation Retry Logic None Automatic retry on validation failure Type Safety No Yes (Pydantic models) Use Case General conversation Structured data extraction"},{"location":"tutorials/structuredbot/#conclusion","title":"Conclusion","text":"<p>StructuredBot provides a powerful way to get reliable, validated structured outputs from LLMs. By combining Pydantic models with automatic validation and retry logic, StructuredBot ensures that your applications receive data in the exact format you expect.</p> <p>Key takeaways:</p> <ul> <li>Use StructuredBot when you need guaranteed structured outputs</li> <li>Design clear, well-validated Pydantic models</li> <li>Leverage automatic retry logic for robust data extraction</li> <li>Combine with appropriate system prompts for best results</li> </ul> <p>For more advanced usage patterns and examples, check out the other bot tutorials in the LlamaBot documentation.</p>"},{"location":"tutorials/toolbot/","title":"ToolBot Tutorial","text":"<p>Welcome to the ToolBot tutorial! In this tutorial, we will learn how to use the <code>ToolBot</code> class to create a single-turn bot that can execute tools and functions. ToolBot is designed to analyze user requests and determine the most appropriate tool to execute, making it perfect for automation tasks and function calling scenarios.</p>"},{"location":"tutorials/toolbot/#what-is-toolbot","title":"What is ToolBot?","text":"<p>ToolBot is a specialized bot that focuses on tool selection and execution rather than multi-turn conversation. It's designed to:</p> <ul> <li>Analyze user requests to understand what they want to accomplish</li> <li>Select the most appropriate tool from its available function toolkit</li> <li>Extract or infer the necessary arguments for the selected function</li> <li>Return a single function call with the proper arguments to execute</li> </ul> <p>This makes ToolBot ideal for:</p> <ul> <li>Automation workflows where you need to execute specific functions</li> <li>Data analysis tasks that require custom code execution</li> <li>API integrations that need to call external services</li> <li>Single-turn function calling scenarios</li> </ul>"},{"location":"tutorials/toolbot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Basic knowledge of Python programming</li> <li>Familiarity with function calling and tool execution concepts</li> <li>Access to a Python environment with the necessary libraries installed</li> </ul>"},{"location":"tutorials/toolbot/#installation","title":"Installation","text":"<p>First, ensure you have the <code>llamabot</code> library installed:</p> <pre><code>pip install llamabot\n</code></pre>"},{"location":"tutorials/toolbot/#basic-usage","title":"Basic Usage","text":""},{"location":"tutorials/toolbot/#step-1-import-toolbot","title":"Step 1: Import ToolBot","text":"<pre><code>from llamabot import ToolBot\nfrom llamabot.components.tools import write_and_execute_code\n</code></pre>"},{"location":"tutorials/toolbot/#step-2-create-a-simple-toolbot","title":"Step 2: Create a Simple ToolBot","text":"<p>Let's start with a basic ToolBot that can execute code:</p> <pre><code># Create a ToolBot with code execution capabilities\nbot = ToolBot(\n    system_prompt=\"You are a helpful assistant that can execute Python code.\",\n    model_name=\"gpt-4.1\",\n    tools=[write_and_execute_code(globals_dict=globals())],\n    # Optional: Add chat memory for conversation context\n    # chat_memory=lmb.ChatMemory(),\n)\n</code></pre>"},{"location":"tutorials/toolbot/#step-3-use-the-toolbot","title":"Step 3: Use the ToolBot","text":"<pre><code># Ask the bot to perform a calculation\nresponse = bot(\"Calculate the sum of numbers from 1 to 100\")\nprint(response)\n</code></pre>"},{"location":"tutorials/toolbot/#advanced-usage-with-custom-tools","title":"Advanced Usage with Custom Tools","text":""},{"location":"tutorials/toolbot/#creating-custom-tools","title":"Creating Custom Tools","text":"<p>ToolBot works with any function decorated with <code>@lmb.tool</code>. Here's how to create custom tools:</p> <pre><code>import llamabot as lmb\n\n@lmb.tool\ndef calculate_fibonacci(n: int) -&gt; int:\n    \"\"\"Calculate the nth Fibonacci number.\n\n    This function calculates the Fibonacci sequence up to the nth term.\n    Use this when you need to compute Fibonacci numbers for mathematical\n    calculations or sequence analysis.\n\n    Parameters\n    ----------\n    n : int\n        The position in the Fibonacci sequence (must be non-negative)\n\n    Returns\n    -------\n    int\n        The nth Fibonacci number\n\n    Examples\n    --------\n    &gt;&gt;&gt; calculate_fibonacci(10)\n    55\n    \"\"\"\n    if n &lt; 0:\n        raise ValueError(\"Input must be non-negative\")\n    if n &lt;= 1:\n        return n\n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    return b\n\n@lmb.tool\ndef analyze_dataframe(df_name: str, operation: str) -&gt; str:\n    \"\"\"Analyze a pandas DataFrame with common operations.\n\n    This function performs basic analysis on DataFrames including\n    shape, columns, data types, and summary statistics.\n\n    Parameters\n    ----------\n    df_name : str\n        The name of the DataFrame variable in the global scope\n    operation : str\n        The analysis operation to perform ('shape', 'columns', 'dtypes', 'describe')\n\n    Returns\n    -------\n    str\n        The analysis results as a formatted string\n    \"\"\"\n    import pandas as pd\n\n    # Get the DataFrame from globals\n    if df_name not in globals():\n        return f\"DataFrame '{df_name}' not found in global scope\"\n\n    df = globals()[df_name]\n\n    if operation == \"shape\":\n        return f\"DataFrame shape: {df.shape}\"\n    elif operation == \"columns\":\n        return f\"Columns: {list(df.columns)}\"\n    elif operation == \"dtypes\":\n        return f\"Data types:\\n{df.dtypes}\"\n    elif operation == \"describe\":\n        return f\"Summary statistics:\\n{df.describe()}\"\n    else:\n        return f\"Unknown operation: {operation}\"\n</code></pre>"},{"location":"tutorials/toolbot/#using-custom-tools-with-toolbot","title":"Using Custom Tools with ToolBot","text":"<pre><code># Create a ToolBot with custom tools\nbot = ToolBot(\n    system_prompt=\"You are a data analysis assistant that can perform calculations and analyze DataFrames.\",\n    model_name=\"gpt-4.1\",\n    tools=[\n        write_and_execute_code(globals_dict=globals()),\n        calculate_fibonacci,\n        analyze_dataframe\n    ],\n)\n\n# Create some sample data\nimport pandas as pd\nsample_df = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': ['a', 'b', 'c', 'd', 'e'],\n    'C': [1.1, 2.2, 3.3, 4.4, 5.5]\n})\n\n# Use the bot to analyze the data\nresponse = bot(\"Analyze the shape of sample_df\")\nprint(response)\n\n# Use the bot to calculate Fibonacci numbers\nresponse = bot(\"Calculate the 15th Fibonacci number\")\nprint(response)\n</code></pre>"},{"location":"tutorials/toolbot/#working-with-global-variables","title":"Working with Global Variables","text":"<p>One of ToolBot's key features is its ability to access and work with global variables in your Python session. This is particularly useful for data analysis workflows:</p> <pre><code># Create some global variables\nimport pandas as pd\nimport numpy as np\n\n# Sample data\nsales_data = pd.DataFrame({\n    'date': pd.date_range('2024-01-01', periods=100),\n    'sales': np.random.randint(100, 1000, 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100)\n})\n\n# Create a ToolBot that can access these variables\nbot = ToolBot(\n    system_prompt=\"You are a data analyst. You have access to sales_data DataFrame.\",\n    model_name=\"gpt-4.1\",\n    tools=[write_and_execute_code(globals_dict=globals())],\n)\n\n# Ask the bot to analyze the sales data\nresponse = bot(\"Calculate the total sales and average sales by region\")\nprint(response)\n</code></pre>"},{"location":"tutorials/toolbot/#using-chat-memory-with-toolbot","title":"Using Chat Memory with ToolBot","text":"<p>ToolBot supports chat memory for maintaining conversation context across multiple interactions:</p> <pre><code>import llamabot as lmb\n\n# Create a ToolBot with chat memory\nbot = ToolBot(\n    system_prompt=\"You are a data analysis assistant.\",\n    model_name=\"gpt-4.1\",\n    tools=[write_and_execute_code(globals_dict=globals())],\n    chat_memory=lmb.ChatMemory(),  # Enable conversation memory\n)\n\n# The bot will remember previous interactions\nresponse1 = bot(\"Create a DataFrame with sample data\")\nresponse2 = bot(\"Now analyze the data you just created\")  # Bot remembers the DataFrame\n</code></pre> <p>Memory Options:</p> <ul> <li><code>lmb.ChatMemory()</code> - Linear memory (fast, no LLM calls)</li> <li><code>lmb.ChatMemory.threaded(model=\"gpt-4o-mini\")</code> - Intelligent threading (uses LLM for smart connections)</li> </ul>"},{"location":"tutorials/toolbot/#toolbot-vs-other-bots","title":"ToolBot vs Other Bots","text":""},{"location":"tutorials/toolbot/#toolbot-vs-simplebot","title":"ToolBot vs SimpleBot","text":"<ul> <li>SimpleBot: Focuses on conversation and text generation</li> <li>ToolBot: Focuses on tool execution and function calling</li> </ul>"},{"location":"tutorials/toolbot/#toolbot-vs-agentbot","title":"ToolBot vs AgentBot","text":"<ul> <li>AgentBot: Multi-turn planning and execution with multiple tools</li> <li>ToolBot: Single-turn tool selection and execution</li> </ul>"},{"location":"tutorials/toolbot/#toolbot-vs-querybot","title":"ToolBot vs QueryBot","text":"<ul> <li>QueryBot: Document retrieval and question answering</li> <li>ToolBot: Function execution and automation</li> </ul>"},{"location":"tutorials/toolbot/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/toolbot/#1-tool-documentation","title":"1. Tool Documentation","text":"<p>Always provide comprehensive docstrings for your tools:</p> <pre><code>@lmb.tool\ndef my_tool(param1: str, param2: int) -&gt; str:\n    \"\"\"Clear description of what the tool does.\n\n    Detailed explanation of when and how to use this tool.\n    Include examples and edge cases.\n\n    Parameters\n    ----------\n    param1 : str\n        Description of the first parameter\n    param2 : int\n        Description of the second parameter\n\n    Returns\n    -------\n    str\n        Description of what the tool returns\n\n    Examples\n    --------\n    &gt;&gt;&gt; my_tool(\"example\", 42)\n    \"expected output\"\n    \"\"\"\n    # Tool implementation\n    pass\n</code></pre>"},{"location":"tutorials/toolbot/#2-global-variable-management","title":"2. Global Variable Management","text":"<ul> <li>Always pass <code>globals_dict=globals()</code> when using <code>write_and_execute_code</code></li> <li>Keep your global namespace clean and well-organized</li> <li>Use descriptive variable names</li> </ul>"},{"location":"tutorials/toolbot/#3-error-handling","title":"3. Error Handling","text":"<p>ToolBot will handle tool execution errors gracefully, but it's good practice to include error handling in your custom tools:</p> <pre><code>@lmb.tool\ndef robust_tool(input_data: str) -&gt; str:\n    \"\"\"A tool with proper error handling.\"\"\"\n    try:\n        # Tool logic here\n        result = process_data(input_data)\n        return result\n    except Exception as e:\n        return f\"Error processing data: {str(e)}\"\n</code></pre>"},{"location":"tutorials/toolbot/#4-system-prompt-design","title":"4. System Prompt Design","text":"<p>Design your system prompt to be specific about the tools available:</p> <pre><code>system_prompt = \"\"\"\nYou are a data analysis assistant with access to the following tools:\n- write_and_execute_code: Execute Python code with access to global variables\n- calculate_fibonacci: Calculate Fibonacci numbers\n- analyze_dataframe: Analyze pandas DataFrames\n\nUse the most appropriate tool for each request.\n\"\"\"\n</code></pre>"},{"location":"tutorials/toolbot/#real-world-example-data-analysis-workflow","title":"Real-World Example: Data Analysis Workflow","text":"<p>Here's a complete example of using ToolBot for a data analysis workflow:</p> <pre><code>import llamabot as lmb\nimport pandas as pd\nimport numpy as np\nfrom llamabot.components.tools import write_and_execute_code\n\n# Create sample data\ncustomer_data = pd.DataFrame({\n    'customer_id': range(1, 101),\n    'age': np.random.randint(18, 80, 100),\n    'income': np.random.randint(20000, 150000, 100),\n    'purchase_amount': np.random.randint(10, 1000, 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100)\n})\n\n# Create a ToolBot for data analysis\nbot = ToolBot(\n    system_prompt=\"\"\"\n    You are a data analyst assistant. You have access to customer_data DataFrame\n    and can execute Python code to perform analysis. Focus on providing insights\n    about customer demographics, purchasing patterns, and regional differences.\n    \"\"\",\n    model_name=\"gpt-4.1\",\n    tools=[write_and_execute_code(globals_dict=globals())],\n)\n\n# Perform various analyses\nanalyses = [\n    \"Calculate the average age and income by region\",\n    \"Find the top 10 customers by purchase amount\",\n    \"Create a correlation matrix between age, income, and purchase amount\",\n    \"Generate summary statistics for all numeric columns\"\n]\n\nfor analysis in analyses:\n    print(f\"\\n--- {analysis} ---\")\n    response = bot(analysis)\n    print(response)\n</code></pre>"},{"location":"tutorials/toolbot/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/toolbot/#common-issues","title":"Common Issues","text":"<ol> <li>Tool not found: Ensure your tool is properly decorated with <code>@lmb.tool</code></li> <li>Global variables not accessible: Always pass <code>globals_dict=globals()</code> to <code>write_and_execute_code</code></li> <li>Import errors: Make sure all required libraries are installed</li> <li>Type errors: Ensure your tool parameters match the expected types</li> </ol>"},{"location":"tutorials/toolbot/#debug-mode","title":"Debug Mode","text":"<p>Enable debug mode to see detailed information about tool execution:</p> <pre><code>import llamabot as lmb\nlmb.set_debug_mode(True)\n\n# Your ToolBot code here\n</code></pre>"},{"location":"tutorials/toolbot/#conclusion","title":"Conclusion","text":"<p>ToolBot provides a powerful way to create single-turn bots that can execute tools and functions. It's particularly useful for:</p> <ul> <li>Data analysis workflows where you need to execute custom code</li> <li>Automation tasks that require specific function calls</li> <li>API integrations that need to call external services</li> <li>Single-turn function calling scenarios</li> </ul> <p>By following the best practices outlined in this tutorial, you can create robust and efficient ToolBot instances that handle complex automation tasks with ease.</p> <p>For more advanced usage patterns and examples, check out the other bot tutorials in the LlamaBot documentation.</p>"},{"location":"tutorials/chat-ui/querybot/","title":"Building a QueryBot Chat Interface with File Upload in Panel","text":"<ul> <li>Building a QueryBot Chat Interface with File Upload in Panel</li> <li>Prerequisites</li> <li>Code Breakdown<ul> <li>Import Necessary Libraries</li> <li>Initialize Panel Extension</li> <li>Set Up Widgets and Global Variables</li> <li>Define the File Upload Function</li> <li>Interact with the Bot and Update Chat Interface</li> <li>Monitor File Uploads</li> <li>Define the Callback Function for Chat Interface</li> <li>Set Up the Chat Interface</li> <li>Combine Widgets and Chat Interface into a Single App</li> </ul> </li> <li>All the code together<ul> <li>The script</li> <li>The terminal command</li> </ul> </li> <li>Conclusion</li> </ul> <p>In this tutorial, we will walk through how to create a chat interface that allows users to upload a PDF file, which the <code>QueryBot</code> from the <code>llamabot</code> library will then summarize. This is all presented in a neat web app using the <code>Panel</code> library.</p>"},{"location":"tutorials/chat-ui/querybot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Familiarity with Python programming.</li> <li>The <code>llamabot</code> and <code>panel</code> libraries installed.</li> </ul>"},{"location":"tutorials/chat-ui/querybot/#code-breakdown","title":"Code Breakdown","text":""},{"location":"tutorials/chat-ui/querybot/#import-necessary-libraries","title":"Import Necessary Libraries","text":"<pre><code>from llamabot import QueryBot\nimport tempfile\nimport panel as pn\nfrom pathlib import Path\n</code></pre> <ul> <li><code>QueryBot</code>: A class from the <code>llamabot</code> library designed to query and extract information from a given document.</li> <li><code>tempfile</code>: A module to generate temporary files and directories.</li> <li><code>panel</code> (aliased as <code>pn</code>): A Python library for creating web-based interactive apps and dashboards.</li> <li><code>Path</code>: A class from the <code>pathlib</code> library for manipulating filesystem paths.</li> </ul>"},{"location":"tutorials/chat-ui/querybot/#initialize-panel-extension","title":"Initialize Panel Extension","text":"<pre><code>pn.extension()\n</code></pre> <p>This initializes Panel's extension, preparing the environment to work with Panel components.</p>"},{"location":"tutorials/chat-ui/querybot/#set-up-widgets-and-global-variables","title":"Set Up Widgets and Global Variables","text":"<pre><code>file_input = pn.widgets.FileInput(mime_type=[\"application/pdf\"])\nspinner = pn.indicators.LoadingSpinner(value=False, width=30, height=30)\nglobal bot\nbot = None\n</code></pre> <ul> <li><code>file_input</code>: A widget that allows users to upload PDF files.</li> <li><code>spinner</code>: A loading spinner indicator to show when the bot is processing.</li> <li><code>bot</code>: A global variable to store the <code>QueryBot</code> instance.</li> </ul>"},{"location":"tutorials/chat-ui/querybot/#define-the-file-upload-function","title":"Define the File Upload Function","text":"<pre><code>def upload_file(event):\n    spinner.value = True\n    raw_contents = event.new\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=\".pdf\", mode=\"wb\"\n    ) as temp_file:\n        temp_file.write(raw_contents)\n        global bot\n        bot = QueryBot(\"You are Richard Feynman\", doc_paths=[Path(temp_file.name)])\n    ...\n</code></pre> <p>This function is triggered when a file is uploaded:</p> <ul> <li>It sets the spinner to active.</li> <li>Retrieves the raw contents of the uploaded file.</li> <li>Creates a temporary PDF file and writes the uploaded content to it.</li> <li>Initializes the <code>QueryBot</code> instance with the context \"You are Richard Feynman\" and the path to the temporary file.</li> </ul>"},{"location":"tutorials/chat-ui/querybot/#interact-with-the-bot-and-update-chat-interface","title":"Interact with the Bot and Update Chat Interface","text":"<pre><code>    chat_interface.send(\n        \"Please allow me to summarize the paper for you. One moment...\",\n        user=\"System\",\n        respond=False,\n    )\n    response = bot(\"Please summarize this paper for me.\")\n    chat_interface.send(response.content, user=\"System\", respond=False)\n    spinner.value = False\n</code></pre> <p>After initializing the bot:</p> <ul> <li>A system message is sent to inform the user that the bot is working on summarizing.</li> <li>The bot is then asked to summarize the uploaded paper.</li> <li>The bot's response is sent to the chat interface.</li> <li>The spinner is deactivated.</li> </ul>"},{"location":"tutorials/chat-ui/querybot/#monitor-file-uploads","title":"Monitor File Uploads","text":"<pre><code>file_input.param.watch(upload_file, \"value\")\n</code></pre> <p>This line sets up an event listener on the <code>file_input</code> widget. When a file is uploaded (i.e., its value changes), the <code>upload_file</code> function is triggered.</p>"},{"location":"tutorials/chat-ui/querybot/#define-the-callback-function-for-chat-interface","title":"Define the Callback Function for Chat Interface","text":"<pre><code>async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n    spinner.value = True\n    global bot\n    response = bot(contents)\n    spinner.value = False\n    yield response.content\n</code></pre> <p>This function is called whenever a user sends a message:</p> <ul> <li>Activates the spinner.</li> <li>Queries the <code>bot</code> with the user's message.</li> <li>Deactivates the spinner.</li> <li>Yields the bot's response.</li> </ul>"},{"location":"tutorials/chat-ui/querybot/#set-up-the-chat-interface","title":"Set Up the Chat Interface","text":"<pre><code>chat_interface = pn.chat.ChatInterface(\n    callback=callback,\n    callback_user=\"QueryBot\",\n    show_clear=False,\n)\nchat_interface.send(\n    \"Send a message to get a reply from the bot!\",\n    user=\"System\",\n    respond=False,\n)\n</code></pre> <p>This sets up the chat interface and sends an initial message prompting the user to interact.</p>"},{"location":"tutorials/chat-ui/querybot/#combine-widgets-and-chat-interface-into-a-single-app","title":"Combine Widgets and Chat Interface into a Single App","text":"<pre><code>app = pn.Column(pn.Row(file_input, spinner), chat_interface)\napp.servable()\n</code></pre> <p>The file upload widget, spinner, and chat interface are arranged in a layout. The <code>app</code> is made servable, marking it as the main component when the app runs.</p>"},{"location":"tutorials/chat-ui/querybot/#all-the-code-together","title":"All the code together","text":""},{"location":"tutorials/chat-ui/querybot/#the-script","title":"The script","text":"<pre><code># chat_interface.py\nfrom llamabot import QueryBot\nimport tempfile\nimport panel as pn\nfrom pathlib import Path\n\npn.extension()\n\nfile_input = pn.widgets.FileInput(mime_type=[\"application/pdf\"])\nspinner = pn.indicators.LoadingSpinner(value=False, width=30, height=30)\nglobal bot\nbot = None\n\n\ndef upload_file(event):\n    spinner.value = True\n    raw_contents = event.new\n\n    with tempfile.NamedTemporaryFile(\n        delete=False, suffix=\".pdf\", mode=\"wb\"\n    ) as temp_file:\n        temp_file.write(raw_contents)\n        global bot\n        bot = QueryBot(\"You are Richard Feynman\", doc_paths=[Path(temp_file.name)])\n\n    chat_interface.send(\n        \"Please allow me to summarize the paper for you. One moment...\",\n        user=\"System\",\n        respond=False,\n    )\n    response = bot(\"Please summarize this paper for me.\")\n    chat_interface.send(response.content, user=\"System\", respond=False)\n    spinner.value = False\n\n\nfile_input.param.watch(upload_file, \"value\")\n\n\nasync def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n    spinner.value = True\n    global bot\n    response = bot(contents)\n    spinner.value = False\n    yield response.content\n\n\nchat_interface = pn.chat.ChatInterface(\n    callback=callback,\n    callback_user=\"QueryBot\",\n    show_clear=False,\n)\nchat_interface.send(\n    \"Send a message to get a reply from the bot!\",\n    user=\"System\",\n    respond=False,\n)\n\napp = pn.Column(pn.Row(file_input, spinner), chat_interface)\napp.show()\n</code></pre>"},{"location":"tutorials/chat-ui/querybot/#the-terminal-command","title":"The terminal command","text":"<pre><code>panel serve chat_interface.py\n</code></pre>"},{"location":"tutorials/chat-ui/querybot/#conclusion","title":"Conclusion","text":"<p>By integrating the <code>QueryBot</code> and <code>Panel</code> libraries, we've built a dynamic chat interface that can summarize uploaded PDF files. This tutorial serves as a foundation to develop more sophisticated chatbot applications with file processing capabilities. Happy coding!</p>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/","title":"Create a Chat Interface with SimpleBot and Panel","text":"<ul> <li>Create a Chat Interface with SimpleBot and Panel</li> <li>Prerequisites</li> <li>Code Breakdown<ul> <li>Import Necessary Libraries</li> <li>Initialize Panel Extension</li> <li>Create a SimpleBot Instance</li> <li>Define the Callback Function</li> <li>Create the Chat Interface</li> <li>Send an Initial Message</li> <li>Make the Chat Interface Servable</li> <li>Serve up the Panel app</li> </ul> </li> <li>All the code together<ul> <li>The python script</li> <li>The terminal command</li> </ul> </li> <li>Conclusion</li> </ul> <p>In this tutorial, we will explore how to set up a simple chat interface using the <code>SimpleBot</code> class from the <code>llamabot</code> library and the <code>Panel</code> library. By the end of this tutorial, you'll be able to integrate a bot into a chat interface and see how it interacts.</p>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Familiarity with Python programming.</li> <li>The <code>llamabot</code> and <code>panel</code> libraries installed.</li> </ul>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#code-breakdown","title":"Code Breakdown","text":""},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#import-necessary-libraries","title":"Import Necessary Libraries","text":"<pre><code>from llamabot import SimpleBot\nimport panel as pn\n</code></pre> <ul> <li><code>SimpleBot</code>: Class from the <code>llamabot</code> library that allows you to create chatbot instances.</li> <li><code>panel</code> (aliased as <code>pn</code>): A Python library for creating web-based interactive apps and dashboards.</li> </ul>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#initialize-panel-extension","title":"Initialize Panel Extension","text":"<pre><code>pn.extension()\n</code></pre> <p>Before using Panel's functionality, you need to initialize its extension with <code>pn.extension()</code>. This prepares your Python environment to work with Panel components.</p>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#create-a-simplebot-instance","title":"Create a SimpleBot Instance","text":"<pre><code>bot = SimpleBot(\"You are Richard Feynman.\")\n</code></pre> <p>Here, we're creating an instance of the <code>SimpleBot</code> class. The string argument, \"You are Richard Feynman.\", serves as a context or persona for the bot. Essentially, this bot will behave as if it's Richard Feynman.</p>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#define-the-callback-function","title":"Define the Callback Function","text":"<pre><code>async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n    response = bot(contents)\n    yield response.content\n</code></pre> <ul> <li>This asynchronous function will be called whenever a user sends a message to the chat interface.</li> <li>It accepts three parameters:</li> <li><code>contents</code>: The message sent by the user.</li> <li><code>user</code>: The name of the user sending the message.</li> <li><code>instance</code>: The chat interface instance.</li> <li>Inside the function, the message <code>contents</code> is passed to the bot, and the bot's response is yielded.</li> </ul>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#create-the-chat-interface","title":"Create the Chat Interface","text":"<pre><code>chat_interface = pn.chat.ChatInterface(\n    callback=callback, callback_user=\"Feynman Bot\", show_clear=False\n)\n</code></pre> <ul> <li>We're creating an instance of <code>ChatInterface</code> from Panel's chat module.</li> <li>The <code>callback</code> parameter is set to the previously defined <code>callback</code> function. This tells the chat interface to use our function to handle messages.</li> <li><code>callback_user</code> is the name that will be displayed for the bot's messages.</li> <li><code>show_clear=False</code> means the chat interface won't have a clear button to erase the chat history.</li> </ul>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#send-an-initial-message","title":"Send an Initial Message","text":"<pre><code>chat_interface.send(\n    \"Send a message to get a reply from the bot!\",\n    user=\"System\",\n    respond=False,\n)\n</code></pre> <ul> <li>This sends an initial message to the chat interface to prompt users to interact with the bot.</li> <li>The message is sent from the \"System\" user and does not expect a reply (<code>respond=False</code>).</li> </ul>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#make-the-chat-interface-servable","title":"Make the Chat Interface Servable","text":"<pre><code>chat_interface.servable()\n</code></pre> <p>By calling <code>.servable()</code> on the chat interface, you're telling Panel to treat this interface as the main component when you run the app.</p>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#serve-up-the-panel-app","title":"Serve up the Panel app","text":"<p>Now, you can serve up the app typing</p> <pre><code>panel serve chat_interface.py\n</code></pre> <p>in your terminal. This will open up a new browser window with the chat interface.</p>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#all-the-code-together","title":"All the code together","text":""},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#the-python-script","title":"The python script","text":"<pre><code># chat_interface.py\nfrom llamabot import SimpleBot\nimport panel as pn\n\npn.extension()\n\nbot = SimpleBot(\"You are Richard Feynman.\")\n\n\nasync def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n    response = bot(contents)\n    yield response.content\n\n\nchat_interface = pn.chat.ChatInterface(\n    callback=callback, callback_user=\"Feynman Bot\", show_clear=False\n)\nchat_interface.send(\n    \"Send a message to get a reply from the bot!\",\n    user=\"System\",\n    respond=False,\n)\nchat_interface.servable()\n</code></pre>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#the-terminal-command","title":"The terminal command","text":"<pre><code>panel serve chat_interface.py\n</code></pre>"},{"location":"tutorials/chat-ui/simplebot-and-chatbot/#conclusion","title":"Conclusion","text":"<p>With just a few lines of code, you can create a chat interface and integrate it with a bot using the <code>llamabot</code> and <code>Panel</code> libraries. This setup provides a foundational step towards creating more interactive and dynamic chatbot applications. Happy coding!</p>"}]}